diff --git a/llvm/CMakeLists.txt b/llvm/CMakeLists.txt
index 038139a24090..2a08b19b48be 100644
--- a/llvm/CMakeLists.txt
+++ b/llvm/CMakeLists.txt
@@ -309,6 +309,7 @@ set(LLVM_ALL_TARGETS
   BPF
   Hexagon
   Lanai
+  LoongArch
   Mips
   MSP430
   NVPTX
diff --git a/llvm/cmake/config-ix.cmake b/llvm/cmake/config-ix.cmake
index 90e5d327c757..cac6be0fab7c 100644
--- a/llvm/cmake/config-ix.cmake
+++ b/llvm/cmake/config-ix.cmake
@@ -435,6 +435,8 @@ elseif (LLVM_NATIVE_ARCH MATCHES "riscv32")
   set(LLVM_NATIVE_ARCH RISCV)
 elseif (LLVM_NATIVE_ARCH MATCHES "riscv64")
   set(LLVM_NATIVE_ARCH RISCV)
+elseif (LLVM_NATIVE_ARCH MATCHES "loongarch")
+  set(LLVM_NATIVE_ARCH LoongArch)
 else ()
   message(FATAL_ERROR "Unknown architecture ${LLVM_NATIVE_ARCH}")
 endif ()
diff --git a/llvm/cmake/config.guess b/llvm/cmake/config.guess
index 9fdfcce8d033..e2412e0eece0 100644
--- a/llvm/cmake/config.guess
+++ b/llvm/cmake/config.guess
@@ -1018,6 +1018,9 @@ EOF
     x86_64:Linux:*:*)
 	echo x86_64-unknown-linux-gnu
 	exit ;;
+    loongarch64:Linux:*:*)
+	echo loongarch64-unknown-linux-gnu
+	exit ;;
     xtensa*:Linux:*:*)
 	echo ${UNAME_MACHINE}-unknown-linux-gnu
 	exit ;;
diff --git a/llvm/include/llvm/ADT/Triple.h b/llvm/include/llvm/ADT/Triple.h
index 6bad18f19244..fa9ed18b4ca7 100644
--- a/llvm/include/llvm/ADT/Triple.h
+++ b/llvm/include/llvm/ADT/Triple.h
@@ -57,6 +57,8 @@ public:
     bpfel,          // eBPF or extended BPF or 64-bit BPF (little endian)
     bpfeb,          // eBPF or extended BPF or 64-bit BPF (big endian)
     hexagon,        // Hexagon: hexagon
+    loongarch32,    // LoongArch (32-bit): loongarch32
+    loongarch64,    // LoongArch (64-bit): loongarch64
     mips,           // MIPS: mips, mipsallegrex, mipsr6
     mipsel,         // MIPSEL: mipsel, mipsallegrexe, mipsr6el
     mips64,         // MIPS64: mips64, mips64r6, mipsn32, mipsn32r6
@@ -721,6 +723,21 @@ public:
     return isMIPS32() || isMIPS64();
   }
 
+	/// Tests whether the target is LoongArch 32-bit
+	bool isLoongArch32() const {
+		return getArch() == Triple::loongarch32;
+	}
+
+	/// Tests whether the target is LoongArch 64-bit.
+	bool isLoongArch64() const {
+		return getArch() == Triple::loongarch64;
+	}
+
+	/// Tests whether the target is LoongArch (32- or 64-bit).
+	bool isLoongArch() const {
+		return isLoongArch32() || isLoongArch64();
+	}
+
   /// Tests whether the target is 64-bit PowerPC (little and big endian).
   bool isPPC64() const {
     return getArch() == Triple::ppc64 || getArch() == Triple::ppc64le;
diff --git a/llvm/include/llvm/BinaryFormat/ELF.h b/llvm/include/llvm/BinaryFormat/ELF.h
index bdcf10fd1640..df7fb25b2d7d 100644
--- a/llvm/include/llvm/BinaryFormat/ELF.h
+++ b/llvm/include/llvm/BinaryFormat/ELF.h
@@ -312,6 +312,7 @@ enum {
   EM_LANAI = 244,         // Lanai 32-bit processor
   EM_BPF = 247,           // Linux kernel bpf virtual machine
   EM_VE = 251,            // NEC SX-Aurora VE
+  EM_LOONGARCH = 258,     // LoongArch processor
 };
 
 // Object file classes.
@@ -630,6 +631,22 @@ enum {
 #include "ELFRelocs/RISCV.def"
 };
 
+// LoongArch Specific e_flags
+enum : unsigned {
+  // FIXME: Change these when all ABIs definition were finalized.
+  // See current definitions:
+  // https://loongson.github.io/LoongArch-Documentation/LoongArch-ELF-ABI-EN.html#_e_flags_identifies_abi_type_and_version
+  EF_LARCH_ABI = 0x0003,
+  EF_LARCH_ABI_LP32 = 0x0001,
+  EF_LARCH_ABI_XLP32 = 0x0002,
+  EF_LARCH_ABI_LP64D = 0x0003,
+};
+
+// ELF Relocation types for LoongArch
+enum {
+#include "ELFRelocs/LoongArch.def"
+};
+
 // ELF Relocation types for S390/zSeries
 enum {
 #include "ELFRelocs/SystemZ.def"
diff --git a/llvm/include/llvm/BinaryFormat/ELFRelocs/LoongArch.def b/llvm/include/llvm/BinaryFormat/ELFRelocs/LoongArch.def
new file mode 100644
index 000000000000..131d26622e87
--- /dev/null
+++ b/llvm/include/llvm/BinaryFormat/ELFRelocs/LoongArch.def
@@ -0,0 +1,64 @@
+
+#ifndef ELF_RELOC
+#error "ELF_RELOC must be defined"
+#endif
+
+ELF_RELOC(R_LARCH_NONE, 0)
+ELF_RELOC(R_LARCH_32, 1)
+ELF_RELOC(R_LARCH_64, 2)
+ELF_RELOC(R_LARCH_RELATIVE, 3)
+ELF_RELOC(R_LARCH_COPY, 4)
+ELF_RELOC(R_LARCH_JUMP_SLOT, 5)
+ELF_RELOC(R_LARCH_TLS_DTPMOD32, 6)
+ELF_RELOC(R_LARCH_TLS_DTPMOD64, 7)
+ELF_RELOC(R_LARCH_TLS_DTPREL32, 8)
+ELF_RELOC(R_LARCH_TLS_DTPREL64, 9)
+ELF_RELOC(R_LARCH_TLS_TPREL32, 10)
+ELF_RELOC(R_LARCH_TLS_TPREL64, 11)
+ELF_RELOC(R_LARCH_IRELATIVE, 12)
+
+ELF_RELOC(R_LARCH_MARK_LA, 20)
+ELF_RELOC(R_LARCH_MARK_PCREL, 21)
+
+ELF_RELOC(R_LARCH_SOP_PUSH_PCREL, 22)
+
+ELF_RELOC(R_LARCH_SOP_PUSH_ABSOLUTE, 23)
+
+ELF_RELOC(R_LARCH_SOP_PUSH_DUP, 24)
+ELF_RELOC(R_LARCH_SOP_PUSH_GPREL, 25)
+ELF_RELOC(R_LARCH_SOP_PUSH_TLS_TPREL, 26)
+ELF_RELOC(R_LARCH_SOP_PUSH_TLS_GOT, 27)
+ELF_RELOC(R_LARCH_SOP_PUSH_TLS_GD, 28)
+ELF_RELOC(R_LARCH_SOP_PUSH_PLT_PCREL, 29)
+
+ELF_RELOC(R_LARCH_SOP_ASSERT, 30)
+ELF_RELOC(R_LARCH_SOP_NOT, 31)
+ELF_RELOC(R_LARCH_SOP_SUB, 32)
+ELF_RELOC(R_LARCH_SOP_SL, 33)
+ELF_RELOC(R_LARCH_SOP_SR, 34)
+ELF_RELOC(R_LARCH_SOP_ADD, 35)
+ELF_RELOC(R_LARCH_SOP_AND, 36)
+ELF_RELOC(R_LARCH_SOP_IF_ELSE, 37)
+ELF_RELOC(R_LARCH_SOP_POP_32_S_10_5, 38)
+ELF_RELOC(R_LARCH_SOP_POP_32_U_10_12, 39)
+ELF_RELOC(R_LARCH_SOP_POP_32_S_10_12, 40)
+ELF_RELOC(R_LARCH_SOP_POP_32_S_10_16, 41)
+ELF_RELOC(R_LARCH_SOP_POP_32_S_10_16_S2, 42)
+ELF_RELOC(R_LARCH_SOP_POP_32_S_5_20, 43)
+ELF_RELOC(R_LARCH_SOP_POP_32_S_0_5_10_16_S2, 44)
+ELF_RELOC(R_LARCH_SOP_POP_32_S_0_10_10_16_S2, 45)
+ELF_RELOC(R_LARCH_SOP_POP_32_U, 46)
+
+ELF_RELOC(R_LARCH_ADD8, 47)
+ELF_RELOC(R_LARCH_ADD16, 48)
+ELF_RELOC(R_LARCH_ADD24, 49)
+ELF_RELOC(R_LARCH_ADD32, 50)
+ELF_RELOC(R_LARCH_ADD64, 51)
+ELF_RELOC(R_LARCH_SUB8, 52)
+ELF_RELOC(R_LARCH_SUB16, 53)
+ELF_RELOC(R_LARCH_SUB24, 54)
+ELF_RELOC(R_LARCH_SUB32, 55)
+ELF_RELOC(R_LARCH_SUB64, 56)
+
+ELF_RELOC(R_LARCH_GNU_VTINHERIT, 57)
+ELF_RELOC(R_LARCH_GNU_VTENTRY, 58)
diff --git a/llvm/include/llvm/IR/CMakeLists.txt b/llvm/include/llvm/IR/CMakeLists.txt
index c8edc29bd887..5bd8a0faa4f1 100644
--- a/llvm/include/llvm/IR/CMakeLists.txt
+++ b/llvm/include/llvm/IR/CMakeLists.txt
@@ -9,6 +9,7 @@ tablegen(LLVM IntrinsicsAMDGPU.h -gen-intrinsic-enums -intrinsic-prefix=amdgcn)
 tablegen(LLVM IntrinsicsARM.h -gen-intrinsic-enums -intrinsic-prefix=arm)
 tablegen(LLVM IntrinsicsBPF.h -gen-intrinsic-enums -intrinsic-prefix=bpf)
 tablegen(LLVM IntrinsicsHexagon.h -gen-intrinsic-enums -intrinsic-prefix=hexagon)
+tablegen(LLVM IntrinsicsLoongArch.h -gen-intrinsic-enums -intrinsic-prefix=loongarch)
 tablegen(LLVM IntrinsicsMips.h -gen-intrinsic-enums -intrinsic-prefix=mips)
 tablegen(LLVM IntrinsicsNVPTX.h -gen-intrinsic-enums -intrinsic-prefix=nvvm)
 tablegen(LLVM IntrinsicsPowerPC.h -gen-intrinsic-enums -intrinsic-prefix=ppc)
diff --git a/llvm/include/llvm/IR/InlineAsm.h b/llvm/include/llvm/IR/InlineAsm.h
index b6f377093337..427cb3ed4fdc 100644
--- a/llvm/include/llvm/IR/InlineAsm.h
+++ b/llvm/include/llvm/IR/InlineAsm.h
@@ -259,6 +259,7 @@ public:
     Constraint_Uy,
     Constraint_X,
     Constraint_Z,
+    Constraint_ZB,
     Constraint_ZC,
     Constraint_Zy,
     Constraints_Max = Constraint_Zy,
diff --git a/llvm/include/llvm/IR/Intrinsics.td b/llvm/include/llvm/IR/Intrinsics.td
index 4918ea876df6..957257f07db2 100644
--- a/llvm/include/llvm/IR/Intrinsics.td
+++ b/llvm/include/llvm/IR/Intrinsics.td
@@ -1550,3 +1550,4 @@ include "llvm/IR/IntrinsicsBPF.td"
 include "llvm/IR/IntrinsicsSystemZ.td"
 include "llvm/IR/IntrinsicsWebAssembly.td"
 include "llvm/IR/IntrinsicsRISCV.td"
+include "llvm/IR/IntrinsicsLoongArch.td"
diff --git a/llvm/include/llvm/IR/IntrinsicsLoongArch.td b/llvm/include/llvm/IR/IntrinsicsLoongArch.td
new file mode 100644
index 000000000000..e43f34cc5440
--- /dev/null
+++ b/llvm/include/llvm/IR/IntrinsicsLoongArch.td
@@ -0,0 +1,129 @@
+//===- IntrinsicsLoongArch.td - Defines LoongArch intrinsics ---------*- tablegen -*-===//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+//
+// This file defines all of the LoongArch-specific intrinsics.
+//
+//===----------------------------------------------------------------------===//
+
+let TargetPrefix = "loongarch" in {  // All intrinsics start with "llvm.loongarch.".
+
+//===----------------------------------------------------------------------===//
+// LoongArch BASE
+
+def int_loongarch_cpucfg : GCCBuiltin<"__builtin_loongarch_cpucfg">,
+  Intrinsic<[llvm_i32_ty], [llvm_i32_ty], []>;
+
+def int_loongarch_csrrd : GCCBuiltin<"__builtin_loongarch_csrrd">,
+  Intrinsic<[llvm_i32_ty], [llvm_i32_ty], []>;
+
+def int_loongarch_dcsrrd : GCCBuiltin<"__builtin_loongarch_dcsrrd">,
+  Intrinsic<[llvm_i64_ty], [llvm_i64_ty], []>;
+
+def int_loongarch_csrwr : GCCBuiltin<"__builtin_loongarch_csrwr">,
+  Intrinsic<[llvm_i32_ty], [llvm_i32_ty, llvm_i32_ty], []>;
+
+def int_loongarch_dcsrwr : GCCBuiltin<"__builtin_loongarch_dcsrwr">,
+  Intrinsic<[llvm_i64_ty], [llvm_i64_ty, llvm_i64_ty], []>;
+
+def int_loongarch_csrxchg : GCCBuiltin<"__builtin_loongarch_csrxchg">,
+  Intrinsic<[llvm_i32_ty], [llvm_i32_ty, llvm_i32_ty, llvm_i32_ty], []>;
+
+def int_loongarch_dcsrxchg : GCCBuiltin<"__builtin_loongarch_dcsrxchg">,
+  Intrinsic<[llvm_i64_ty], [llvm_i64_ty, llvm_i64_ty, llvm_i64_ty], []>;
+
+def int_loongarch_iocsrrd_b : GCCBuiltin<"__builtin_loongarch_iocsrrd_b">,
+  Intrinsic<[llvm_i32_ty], [llvm_i32_ty], []>;
+
+def int_loongarch_iocsrrd_h : GCCBuiltin<"__builtin_loongarch_iocsrrd_h">,
+  Intrinsic<[llvm_i32_ty], [llvm_i32_ty], []>;
+
+def int_loongarch_iocsrrd_w : GCCBuiltin<"__builtin_loongarch_iocsrrd_w">,
+  Intrinsic<[llvm_i32_ty], [llvm_i32_ty], []>;
+
+def int_loongarch_iocsrrd_d : GCCBuiltin<"__builtin_loongarch_iocsrrd_d">,
+  Intrinsic<[llvm_i64_ty], [llvm_i32_ty], []>;
+
+def int_loongarch_iocsrwr_b : GCCBuiltin<"__builtin_loongarch_iocsrwr_b">,
+  Intrinsic<[], [llvm_i32_ty, llvm_i32_ty], []>;
+
+def int_loongarch_iocsrwr_h : GCCBuiltin<"__builtin_loongarch_iocsrwr_h">,
+  Intrinsic<[], [llvm_i32_ty, llvm_i32_ty], []>;
+
+def int_loongarch_iocsrwr_w : GCCBuiltin<"__builtin_loongarch_iocsrwr_w">,
+  Intrinsic<[], [llvm_i32_ty, llvm_i32_ty], []>;
+
+def int_loongarch_iocsrwr_d : GCCBuiltin<"__builtin_loongarch_iocsrwr_d">,
+  Intrinsic<[], [llvm_i64_ty, llvm_i32_ty], []>;
+
+def int_loongarch_cacop : GCCBuiltin<"__builtin_loongarch_cacop">,
+  Intrinsic<[], [llvm_i32_ty, llvm_i32_ty, llvm_i32_ty], []>;
+
+def int_loongarch_dcacop : GCCBuiltin<"__builtin_loongarch_dcacop">,
+  Intrinsic<[], [llvm_i32_ty, llvm_i64_ty, llvm_i64_ty], []>;
+
+def int_loongarch_crc_w_b_w : GCCBuiltin<"__builtin_loongarch_crc_w_b_w">,
+  Intrinsic<[llvm_i32_ty], [llvm_i32_ty, llvm_i32_ty], []>;
+
+def int_loongarch_crc_w_h_w : GCCBuiltin<"__builtin_loongarch_crc_w_h_w">,
+  Intrinsic<[llvm_i32_ty], [llvm_i32_ty, llvm_i32_ty], []>;
+
+def int_loongarch_crc_w_w_w : GCCBuiltin<"__builtin_loongarch_crc_w_w_w">,
+  Intrinsic<[llvm_i32_ty], [llvm_i32_ty, llvm_i32_ty], []>;
+
+def int_loongarch_crc_w_d_w : GCCBuiltin<"__builtin_loongarch_crc_w_d_w">,
+  Intrinsic<[llvm_i32_ty], [llvm_i64_ty, llvm_i32_ty], []>;
+
+def int_loongarch_crcc_w_b_w : GCCBuiltin<"__builtin_loongarch_crcc_w_b_w">,
+  Intrinsic<[llvm_i32_ty], [llvm_i32_ty, llvm_i32_ty], []>;
+
+def int_loongarch_crcc_w_h_w : GCCBuiltin<"__builtin_loongarch_crcc_w_h_w">,
+  Intrinsic<[llvm_i32_ty], [llvm_i32_ty, llvm_i32_ty], []>;
+
+def int_loongarch_crcc_w_w_w : GCCBuiltin<"__builtin_loongarch_crcc_w_w_w">,
+  Intrinsic<[llvm_i32_ty], [llvm_i32_ty, llvm_i32_ty], []>;
+
+def int_loongarch_crcc_w_d_w : GCCBuiltin<"__builtin_loongarch_crcc_w_d_w">,
+  Intrinsic<[llvm_i32_ty], [llvm_i64_ty, llvm_i32_ty], []>;
+
+def int_loongarch_tlbclr : GCCBuiltin<"__builtin_loongarch_tlbclr">,
+  Intrinsic<[], [], []>;
+
+def int_loongarch_tlbflush : GCCBuiltin<"__builtin_loongarch_tlbflush">,
+  Intrinsic<[], [], []>;
+
+def int_loongarch_tlbfill : GCCBuiltin<"__builtin_loongarch_tlbfill">,
+  Intrinsic<[], [], []>;
+
+def int_loongarch_tlbrd : GCCBuiltin<"__builtin_loongarch_tlbrd">,
+  Intrinsic<[], [], []>;
+
+def int_loongarch_tlbwr : GCCBuiltin<"__builtin_loongarch_tlbwr">,
+  Intrinsic<[], [], []>;
+
+def int_loongarch_tlbsrch : GCCBuiltin<"__builtin_loongarch_tlbsrch">,
+  Intrinsic<[], [], []>;
+
+def int_loongarch_syscall : GCCBuiltin<"__builtin_loongarch_syscall">,
+  Intrinsic<[], [llvm_i64_ty], []>;
+
+def int_loongarch_break : GCCBuiltin<"__builtin_loongarch_break">,
+  Intrinsic<[], [llvm_i64_ty], []>;
+
+def int_loongarch_asrtle_d : GCCBuiltin<"__builtin_loongarch_asrtle_d">,
+  Intrinsic<[], [llvm_i64_ty, llvm_i64_ty], []>;
+
+def int_loongarch_asrtgt_d : GCCBuiltin<"__builtin_loongarch_asrtgt_d">,
+  Intrinsic<[], [llvm_i64_ty, llvm_i64_ty], []>;
+
+def int_loongarch_dbar : GCCBuiltin<"__builtin_loongarch_dbar">,
+  Intrinsic<[], [llvm_i64_ty], []>;
+
+def int_loongarch_ibar : GCCBuiltin<"__builtin_loongarch_ibar">,
+  Intrinsic<[], [llvm_i64_ty], []>;
+
+}
diff --git a/llvm/include/llvm/Object/ELFObjectFile.h b/llvm/include/llvm/Object/ELFObjectFile.h
index 62ecd8b5a7e5..f0e72fe5cf75 100644
--- a/llvm/include/llvm/Object/ELFObjectFile.h
+++ b/llvm/include/llvm/Object/ELFObjectFile.h
@@ -1109,6 +1109,8 @@ StringRef ELFObjectFile<ELFT>::getFileFormatName() const {
       return "elf32-powerpc";
     case ELF::EM_RISCV:
       return "elf32-littleriscv";
+    case ELF::EM_LOONGARCH:
+      return "elf32-loongarch";
     case ELF::EM_SPARC:
     case ELF::EM_SPARC32PLUS:
       return "elf32-sparc";
@@ -1133,6 +1135,8 @@ StringRef ELFObjectFile<ELFT>::getFileFormatName() const {
       return "elf64-s390";
     case ELF::EM_SPARCV9:
       return "elf64-sparc";
+    case ELF::EM_LOONGARCH:
+      return "elf64-loongarch";
     case ELF::EM_MIPS:
       return "elf64-mips";
     case ELF::EM_AMDGPU:
@@ -1192,6 +1196,15 @@ template <class ELFT> Triple::ArchType ELFObjectFile<ELFT>::getArch() const {
     default:
       report_fatal_error("Invalid ELFCLASS!");
     }
+  case ELF::EM_LOONGARCH:
+    switch (EF.getHeader()->e_ident[ELF::EI_CLASS]) {
+    case ELF::ELFCLASS32:
+      return Triple::loongarch32;
+    case ELF::ELFCLASS64:
+      return Triple::loongarch64;
+    default:
+      report_fatal_error("Invalid ELFCLASS!");
+    }
   case ELF::EM_S390:
     return Triple::systemz;
 
diff --git a/llvm/include/llvm/module.modulemap b/llvm/include/llvm/module.modulemap
index 778a17c8aeee..7272300da97c 100644
--- a/llvm/include/llvm/module.modulemap
+++ b/llvm/include/llvm/module.modulemap
@@ -68,6 +68,7 @@ module LLVM_BinaryFormat {
     textual header "BinaryFormat/ELFRelocs/Hexagon.def"
     textual header "BinaryFormat/ELFRelocs/i386.def"
     textual header "BinaryFormat/ELFRelocs/Lanai.def"
+    textual header "BinaryFormat/ELFRelocs/LoongArch.def"
     textual header "BinaryFormat/ELFRelocs/Mips.def"
     textual header "BinaryFormat/ELFRelocs/MSP430.def"
     textual header "BinaryFormat/ELFRelocs/PowerPC64.def"
diff --git a/llvm/lib/CodeGen/RegisterScavenging.cpp b/llvm/lib/CodeGen/RegisterScavenging.cpp
index 41b6de1441d7..5fd31f5a91b6 100644
--- a/llvm/lib/CodeGen/RegisterScavenging.cpp
+++ b/llvm/lib/CodeGen/RegisterScavenging.cpp
@@ -440,6 +440,10 @@ findSurvivorBackwards(const MachineRegisterInfo &MRI,
         InstrCountDown = InstrLimit;
         Pos = I;
       }
+      if (I->getFlag(MachineInstr::FrameSetup)) {
+        Pos = To;
+        break;
+      }
       if (I == MBB.begin())
         break;
     }
diff --git a/llvm/lib/CodeGen/TargetLoweringObjectFileImpl.cpp b/llvm/lib/CodeGen/TargetLoweringObjectFileImpl.cpp
index 27bebe503ce6..df6526f04c16 100644
--- a/llvm/lib/CodeGen/TargetLoweringObjectFileImpl.cpp
+++ b/llvm/lib/CodeGen/TargetLoweringObjectFileImpl.cpp
@@ -190,6 +190,16 @@ void TargetLoweringObjectFileELF::Initialize(MCContext &Ctx,
     PersonalityEncoding = dwarf::DW_EH_PE_absptr;
     TTypeEncoding = dwarf::DW_EH_PE_absptr;
     break;
+  case Triple::loongarch32:
+  case Triple::loongarch64:
+    PersonalityEncoding = dwarf::DW_EH_PE_indirect;
+
+    // Note: gas does not support pc-relative LSDA references.
+    LSDAEncoding = dwarf::DW_EH_PE_absptr;
+
+    TTypeEncoding = dwarf::DW_EH_PE_indirect | dwarf::DW_EH_PE_pcrel |
+                    dwarf::DW_EH_PE_sdata4;
+    break;
   case Triple::mips:
   case Triple::mipsel:
   case Triple::mips64:
diff --git a/llvm/lib/CodeGen/XRayInstrumentation.cpp b/llvm/lib/CodeGen/XRayInstrumentation.cpp
index ab9c0e81ebdc..261271a5e8a2 100644
--- a/llvm/lib/CodeGen/XRayInstrumentation.cpp
+++ b/llvm/lib/CodeGen/XRayInstrumentation.cpp
@@ -224,6 +224,8 @@ bool XRayInstrumentation::runOnMachineFunction(MachineFunction &MF) {
     case Triple::ArchType::arm:
     case Triple::ArchType::thumb:
     case Triple::ArchType::aarch64:
+    case Triple::ArchType::loongarch32:
+    case Triple::ArchType::loongarch64:
     case Triple::ArchType::mips:
     case Triple::ArchType::mipsel:
     case Triple::ArchType::mips64:
diff --git a/llvm/lib/IR/Function.cpp b/llvm/lib/IR/Function.cpp
index 10d535e3ab11..2be1a5b5879d 100644
--- a/llvm/lib/IR/Function.cpp
+++ b/llvm/lib/IR/Function.cpp
@@ -37,6 +37,7 @@
 #include "llvm/IR/IntrinsicsARM.h"
 #include "llvm/IR/IntrinsicsBPF.h"
 #include "llvm/IR/IntrinsicsHexagon.h"
+#include "llvm/IR/IntrinsicsLoongArch.h"
 #include "llvm/IR/IntrinsicsMips.h"
 #include "llvm/IR/IntrinsicsNVPTX.h"
 #include "llvm/IR/IntrinsicsPowerPC.h"
diff --git a/llvm/lib/MC/MCObjectFileInfo.cpp b/llvm/lib/MC/MCObjectFileInfo.cpp
index b77a9635f64c..9db80429f57a 100644
--- a/llvm/lib/MC/MCObjectFileInfo.cpp
+++ b/llvm/lib/MC/MCObjectFileInfo.cpp
@@ -302,6 +302,12 @@ void MCObjectFileInfo::initMachOMCObjectFileInfo(const Triple &T) {
 
 void MCObjectFileInfo::initELFMCObjectFileInfo(const Triple &T, bool Large) {
   switch (T.getArch()) {
+  case Triple::loongarch32:
+  case Triple::loongarch64:
+    FDECFIEncoding = Ctx->getAsmInfo()->getCodePointerSize() == 4
+                         ? dwarf::DW_EH_PE_sdata4
+                         : dwarf::DW_EH_PE_sdata8;
+    break;
   case Triple::mips:
   case Triple::mipsel:
   case Triple::mips64:
diff --git a/llvm/lib/Object/ELF.cpp b/llvm/lib/Object/ELF.cpp
index 2515695095a1..5a1b0221ac84 100644
--- a/llvm/lib/Object/ELF.cpp
+++ b/llvm/lib/Object/ELF.cpp
@@ -94,6 +94,13 @@ StringRef llvm::object::getELFRelocationTypeName(uint32_t Machine,
       break;
     }
     break;
+  case ELF::EM_LOONGARCH:
+    switch (Type) {
+#include "llvm/BinaryFormat/ELFRelocs/LoongArch.def"
+    default:
+      break;
+    }
+    break;
   case ELF::EM_PPC64:
     switch (Type) {
 #include "llvm/BinaryFormat/ELFRelocs/PowerPC64.def"
diff --git a/llvm/lib/Support/Host.cpp b/llvm/lib/Support/Host.cpp
index 658c1ee74cfe..d2b1dfbbabf0 100644
--- a/llvm/lib/Support/Host.cpp
+++ b/llvm/lib/Support/Host.cpp
@@ -1192,6 +1192,45 @@ StringRef sys::getHostCPUName() {
   StringRef Content = P ? P->getBuffer() : "";
   return detail::getHostCPUNameForS390x(Content);
 }
+#elif defined(__linux__) && defined(__loongarch__)
+// loongarch prid register
+// +----------------+----------------+----------------+----------------+
+// | Company Options| Company ID     | Processor ID   | Revision       |
+// +----------------+----------------+----------------+----------------+
+//  31            24 23            16 15             8 7              0
+
+#define PRID_OPT_MASK                  0xff000000
+#define PRID_COMP_MASK                 0xff0000
+#define PRID_COMP_LOONGSON             0x140000
+#define PRID_IMP_MASK                  0xff00
+
+#define PRID_IMP_LOONGSON_32  0x4200  /* Loongson 32bit */
+#define PRID_IMP_LOONGSON_64R 0x6100  /* Reduced Loongson 64bit */
+#define PRID_IMP_LOONGSON_64C 0x6300  /* Classic Loongson 64bit */
+#define PRID_IMP_LOONGSON_64G 0xc000  /* Generic Loongson 64bit */
+
+StringRef sys::getHostCPUName() {
+  // use prid to detect cpu name
+  unsigned CPUCFG_NUM = 0; // prid
+  unsigned prid;
+
+  __asm__("cpucfg %[prid], %[CPUCFG_NUM]\n\t"
+      :[prid]"=r"(prid)
+      :[CPUCFG_NUM]"r"(CPUCFG_NUM));
+
+  if ((prid & PRID_COMP_MASK) == PRID_COMP_LOONGSON) {// for Loongson
+    switch (prid & PRID_IMP_MASK) {
+      case PRID_IMP_LOONGSON_32: // not support
+        return "loongarch32";
+      case PRID_IMP_LOONGSON_64R:
+      case PRID_IMP_LOONGSON_64C:
+      case PRID_IMP_LOONGSON_64G:
+        return "la464";
+    }
+  }
+
+  return "generic";
+}
 #elif defined(__APPLE__) && defined(__aarch64__)
 StringRef sys::getHostCPUName() {
   return "cyclone";
@@ -1538,6 +1577,36 @@ bool sys::getHostCPUFeatures(StringMap<bool> &Features) {
 
   return true;
 }
+#elif defined(__linux__) && defined(__loongarch__)
+bool sys::getHostCPUFeatures(StringMap<bool> &Features) {
+  std::unique_ptr<llvm::MemoryBuffer> P = getProcCpuinfoContent();
+  if (!P)
+    return false;
+
+  SmallVector<StringRef, 32> Lines;
+  P->getBuffer().split(Lines, "\n");
+
+  SmallVector<StringRef, 32> CPUFeatures;
+
+  // Look for the CPU features.
+  for (unsigned I = 0, E = Lines.size(); I != E; ++I)
+    if (Lines[I].startswith("features")) {
+      Lines[I].split(CPUFeatures, ' ');
+      break;
+    }
+
+  for (unsigned I = 0, E = CPUFeatures.size(); I != E; ++I) {
+    StringRef LLVMFeatureStr = StringSwitch<StringRef>(CPUFeatures[I])
+                                   .Case("lsx", "lsx")
+                                   .Case("lasx", "lasx")
+                                   .Default("");
+
+    if (LLVMFeatureStr != "")
+      Features[LLVMFeatureStr] = true;
+  }
+
+  return true;
+}
 #else
 bool sys::getHostCPUFeatures(StringMap<bool> &Features) { return false; }
 #endif
diff --git a/llvm/lib/Support/Triple.cpp b/llvm/lib/Support/Triple.cpp
index fec1985ccaca..e120c067ca2b 100644
--- a/llvm/lib/Support/Triple.cpp
+++ b/llvm/lib/Support/Triple.cpp
@@ -43,6 +43,8 @@ StringRef Triple::getArchTypeName(ArchType Kind) {
   case lanai:          return "lanai";
   case le32:           return "le32";
   case le64:           return "le64";
+  case loongarch32:    return "loongarch32";
+  case loongarch64:    return "loongarch64";
   case mips64:         return "mips64";
   case mips64el:       return "mips64el";
   case mips:           return "mips";
@@ -151,6 +153,8 @@ StringRef Triple::getArchTypePrefix(ArchType Kind) {
   case riscv64:     return "riscv";
 
   case ve:          return "ve";
+  case loongarch32:
+  case loongarch64: return "loongarch";
   }
 }
 
@@ -321,6 +325,8 @@ Triple::ArchType Triple::getArchTypeForLLVMName(StringRef Name) {
     .Case("renderscript32", renderscript32)
     .Case("renderscript64", renderscript64)
     .Case("ve", ve)
+    .Case("loongarch32", loongarch32)
+    .Case("loongarch64", loongarch64)
     .Default(UnknownArch);
 }
 
@@ -450,6 +456,8 @@ static Triple::ArchType parseArch(StringRef ArchName) {
     .Case("ve", Triple::ve)
     .Case("wasm32", Triple::wasm32)
     .Case("wasm64", Triple::wasm64)
+    .Case("loongarch32", Triple::loongarch32)
+    .Case("loongarch64", Triple::loongarch64)
     .Default(Triple::UnknownArch);
 
   // Some architectures require special parsing logic just to compute the
@@ -687,6 +695,8 @@ static Triple::ObjectFormatType getDefaultFormat(const Triple &T) {
   case Triple::lanai:
   case Triple::le32:
   case Triple::le64:
+  case Triple::loongarch32:
+  case Triple::loongarch64:
   case Triple::mips64:
   case Triple::mips64el:
   case Triple::mips:
@@ -758,6 +768,7 @@ Triple::Triple(const Twine &Str)
               .StartsWith("mipsisa64", Triple::GNUABI64)
               .StartsWith("mipsisa32", Triple::GNU)
               .Cases("mips", "mipsel", "mipsr6", "mipsr6el", Triple::GNU)
+              .Cases("loongarch32", "loongarch64", Triple::GNU)
               .Default(UnknownEnvironment);
     }
   }
@@ -1254,6 +1265,7 @@ static unsigned getArchPointerBitWidth(llvm::Triple::ArchType Arch) {
   case llvm::Triple::kalimba:
   case llvm::Triple::lanai:
   case llvm::Triple::le32:
+  case llvm::Triple::loongarch32:
   case llvm::Triple::mips:
   case llvm::Triple::mipsel:
   case llvm::Triple::nvptx:
@@ -1282,6 +1294,7 @@ static unsigned getArchPointerBitWidth(llvm::Triple::ArchType Arch) {
   case llvm::Triple::bpfel:
   case llvm::Triple::hsail64:
   case llvm::Triple::le64:
+  case llvm::Triple::loongarch64:
   case llvm::Triple::mips64:
   case llvm::Triple::mips64el:
   case llvm::Triple::nvptx64:
@@ -1337,6 +1350,7 @@ Triple Triple::get32BitArchVariant() const {
   case Triple::kalimba:
   case Triple::lanai:
   case Triple::le32:
+  case Triple::loongarch32:
   case Triple::mips:
   case Triple::mipsel:
   case Triple::nvptx:
@@ -1363,6 +1377,7 @@ Triple Triple::get32BitArchVariant() const {
   case Triple::amdil64:        T.setArch(Triple::amdil);   break;
   case Triple::hsail64:        T.setArch(Triple::hsail);   break;
   case Triple::le64:           T.setArch(Triple::le32);    break;
+  case Triple::loongarch64:    T.setArch(Triple::loongarch32);   break;
   case Triple::mips64:         T.setArch(Triple::mips);    break;
   case Triple::mips64el:       T.setArch(Triple::mipsel);  break;
   case Triple::nvptx64:        T.setArch(Triple::nvptx);   break;
@@ -1404,6 +1419,7 @@ Triple Triple::get64BitArchVariant() const {
   case Triple::bpfel:
   case Triple::hsail64:
   case Triple::le64:
+  case Triple::loongarch64:
   case Triple::mips64:
   case Triple::mips64el:
   case Triple::nvptx64:
@@ -1426,6 +1442,7 @@ Triple Triple::get64BitArchVariant() const {
   case Triple::armeb:           T.setArch(Triple::aarch64_be); break;
   case Triple::hsail:           T.setArch(Triple::hsail64);    break;
   case Triple::le32:            T.setArch(Triple::le64);       break;
+  case Triple::loongarch32:     T.setArch(Triple::loongarch64);        break;
   case Triple::mips:            T.setArch(Triple::mips64);     break;
   case Triple::mipsel:          T.setArch(Triple::mips64el);   break;
   case Triple::nvptx:           T.setArch(Triple::nvptx64);    break;
@@ -1459,6 +1476,8 @@ Triple Triple::getBigEndianArchVariant() const {
   case Triple::kalimba:
   case Triple::le32:
   case Triple::le64:
+  case Triple::loongarch32:
+  case Triple::loongarch64:
   case Triple::msp430:
   case Triple::nvptx64:
   case Triple::nvptx:
@@ -1545,6 +1564,8 @@ bool Triple::isLittleEndian() const {
   case Triple::kalimba:
   case Triple::le32:
   case Triple::le64:
+  case Triple::loongarch32:
+  case Triple::loongarch64:
   case Triple::mips64el:
   case Triple::mipsel:
   case Triple::msp430:
diff --git a/llvm/lib/Target/LLVMBuild.txt b/llvm/lib/Target/LLVMBuild.txt
index e5a9d787e7fa..bb217aa41e6e 100644
--- a/llvm/lib/Target/LLVMBuild.txt
+++ b/llvm/lib/Target/LLVMBuild.txt
@@ -27,6 +27,7 @@ subdirectories =
  Hexagon
  Lanai
  MSP430
+ LoongArch
  Mips
  NVPTX
  PowerPC
diff --git a/llvm/lib/Target/LoongArch/AsmParser/CMakeLists.txt b/llvm/lib/Target/LoongArch/AsmParser/CMakeLists.txt
new file mode 100644
index 000000000000..c000e3d96bed
--- /dev/null
+++ b/llvm/lib/Target/LoongArch/AsmParser/CMakeLists.txt
@@ -0,0 +1,3 @@
+add_llvm_component_library(LLVMLoongArchAsmParser
+  LoongArchAsmParser.cpp
+  )
diff --git a/llvm/lib/Target/LoongArch/AsmParser/LLVMBuild.txt b/llvm/lib/Target/LoongArch/AsmParser/LLVMBuild.txt
new file mode 100644
index 000000000000..8fbecde89dd8
--- /dev/null
+++ b/llvm/lib/Target/LoongArch/AsmParser/LLVMBuild.txt
@@ -0,0 +1,22 @@
+;===- ./lib/Target/LoongArch/AsmParser/LLVMBuild.txt ----------------*- Conf -*--===;
+;
+; Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+; See https://llvm.org/LICENSE.txt for license information.
+; SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+;
+;===------------------------------------------------------------------------===;
+;
+; This is an LLVMBuild description file for the components in this subdirectory.
+;
+; For more information on the LLVMBuild system, please see:
+;
+;   http://llvm.org/docs/LLVMBuild.html
+;
+;===------------------------------------------------------------------------===;
+
+[component_0]
+type = Library
+name = LoongArchAsmParser
+parent = LoongArch
+required_libraries = MC MCParser LoongArchDesc LoongArchInfo Support
+add_to_library_groups = LoongArch
diff --git a/llvm/lib/Target/LoongArch/AsmParser/LoongArchAsmParser.cpp b/llvm/lib/Target/LoongArch/AsmParser/LoongArchAsmParser.cpp
new file mode 100644
index 000000000000..6d00201bdfed
--- /dev/null
+++ b/llvm/lib/Target/LoongArch/AsmParser/LoongArchAsmParser.cpp
@@ -0,0 +1,2332 @@
+//===-- LoongArchAsmParser.cpp - Parse LoongArch assembly to MCInst instructions ----===//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+
+#include "MCTargetDesc/LoongArchFPABIInfo.h"
+#include "MCTargetDesc/LoongArchABIInfo.h"
+#include "MCTargetDesc/LoongArchBaseInfo.h"
+#include "MCTargetDesc/LoongArchMCExpr.h"
+#include "MCTargetDesc/LoongArchMCTargetDesc.h"
+#include "LoongArchTargetStreamer.h"
+#include "TargetInfo/LoongArchTargetInfo.h"
+#include "llvm/ADT/APFloat.h"
+#include "llvm/ADT/STLExtras.h"
+#include "llvm/ADT/SmallVector.h"
+#include "llvm/ADT/StringRef.h"
+#include "llvm/ADT/StringSwitch.h"
+#include "llvm/ADT/Triple.h"
+#include "llvm/ADT/Twine.h"
+#include "llvm/BinaryFormat/ELF.h"
+#include "llvm/MC/MCContext.h"
+#include "llvm/MC/MCExpr.h"
+#include "llvm/MC/MCInst.h"
+#include "llvm/MC/MCInstrDesc.h"
+#include "llvm/MC/MCObjectFileInfo.h"
+#include "llvm/MC/MCParser/MCAsmLexer.h"
+#include "llvm/MC/MCParser/MCAsmParser.h"
+#include "llvm/MC/MCParser/MCAsmParserExtension.h"
+#include "llvm/MC/MCParser/MCParsedAsmOperand.h"
+#include "llvm/MC/MCParser/MCTargetAsmParser.h"
+#include "llvm/MC/MCSectionELF.h"
+#include "llvm/MC/MCStreamer.h"
+#include "llvm/MC/MCSubtargetInfo.h"
+#include "llvm/MC/MCSymbol.h"
+#include "llvm/MC/MCSymbolELF.h"
+#include "llvm/MC/MCValue.h"
+#include "llvm/MC/SubtargetFeature.h"
+#include "llvm/Support/Casting.h"
+#include "llvm/Support/CommandLine.h"
+#include "llvm/Support/Compiler.h"
+#include "llvm/Support/Debug.h"
+#include "llvm/Support/ErrorHandling.h"
+#include "llvm/Support/MathExtras.h"
+#include "llvm/Support/SMLoc.h"
+#include "llvm/Support/SourceMgr.h"
+#include "llvm/Support/TargetRegistry.h"
+#include "llvm/Support/raw_ostream.h"
+#include <algorithm>
+#include <cassert>
+#include <cstdint>
+#include <memory>
+#include <string>
+#include <utility>
+
+using namespace llvm;
+
+#define DEBUG_TYPE "loongarch-asm-parser"
+
+namespace llvm {
+
+class MCInstrInfo;
+
+} // end namespace llvm
+
+namespace {
+
+class LoongArchAssemblerOptions {
+public:
+  LoongArchAssemblerOptions(const FeatureBitset &Features_) : Features(Features_) {}
+
+  LoongArchAssemblerOptions(const LoongArchAssemblerOptions *Opts) {
+    Features = Opts->getFeatures();
+  }
+
+  const FeatureBitset &getFeatures() const { return Features; }
+  void setFeatures(const FeatureBitset &Features_) { Features = Features_; }
+
+private:
+  FeatureBitset Features;
+};
+
+} // end anonymous namespace
+
+namespace {
+
+class LoongArchAsmParser : public MCTargetAsmParser {
+  LoongArchTargetStreamer &getTargetStreamer() {
+    MCTargetStreamer &TS = *getParser().getStreamer().getTargetStreamer();
+    return static_cast<LoongArchTargetStreamer &>(TS);
+  }
+
+  LoongArchABIInfo ABI;
+  SmallVector<std::unique_ptr<LoongArchAssemblerOptions>, 2> AssemblerOptions;
+  MCSymbol *CurrentFn; // Pointer to the function being parsed. It may be a
+                       // nullptr, which indicates that no function is currently
+                       // selected. This usually happens after an '.end'
+                       // directive.
+  bool IsPicEnabled;
+
+  // Map of register aliases created via the .set directive.
+  StringMap<AsmToken> RegisterSets;
+
+#define GET_ASSEMBLER_HEADER
+#include "LoongArchGenAsmMatcher.inc"
+
+  unsigned checkTargetMatchPredicate(MCInst &Inst) override;
+
+  bool MatchAndEmitInstruction(SMLoc IDLoc, unsigned &Opcode,
+                               OperandVector &Operands, MCStreamer &Out,
+                               uint64_t &ErrorInfo,
+                               bool MatchingInlineAsm) override;
+
+  /// Parse a register as used in CFI directives
+  bool ParseRegister(unsigned &RegNo, SMLoc &StartLoc, SMLoc &EndLoc) override;
+  OperandMatchResultTy tryParseRegister(unsigned &RegNo, SMLoc &StartLoc,
+                                        SMLoc &EndLoc) override;
+
+  bool mnemonicIsValid(StringRef Mnemonic);
+
+  bool ParseInstruction(ParseInstructionInfo &Info, StringRef Name,
+                        SMLoc NameLoc, OperandVector &Operands) override;
+
+  bool ParseDirective(AsmToken DirectiveID) override;
+
+  OperandMatchResultTy parseMemOperand(OperandVector &Operands);
+  OperandMatchResultTy
+  matchAnyRegisterNameWithoutDollar(OperandVector &Operands,
+                                    StringRef Identifier, SMLoc S);
+  OperandMatchResultTy matchAnyRegisterWithoutDollar(OperandVector &Operands,
+                                                     const AsmToken &Token,
+                                                     SMLoc S);
+  OperandMatchResultTy matchAnyRegisterWithoutDollar(OperandVector &Operands,
+                                                     SMLoc S);
+  OperandMatchResultTy parseAnyRegister(OperandVector &Operands);
+  OperandMatchResultTy parseJumpTarget(OperandVector &Operands);
+
+  bool searchSymbolAlias(OperandVector &Operands);
+
+  bool parseOperand(OperandVector &, StringRef Mnemonic);
+
+  enum MacroExpanderResultTy {
+    MER_NotAMacro,
+    MER_Success,
+    MER_Fail,
+  };
+
+  // Expands assembly pseudo instructions.
+  MacroExpanderResultTy tryExpandInstruction(MCInst &Inst, SMLoc IDLoc,
+                                             MCStreamer &Out,
+                                             const MCSubtargetInfo *STI);
+
+  bool loadImmediate(int64_t ImmValue, unsigned DstReg, unsigned SrcReg,
+                     bool Is32BitImm, bool IsAddress, SMLoc IDLoc,
+                     MCStreamer &Out, const MCSubtargetInfo *STI);
+
+  bool expandLoadImm(MCInst &Inst, bool Is32BitImm, SMLoc IDLoc,
+                     MCStreamer &Out, const MCSubtargetInfo *STI);
+
+  bool expandLoadAddress(unsigned DstReg, const MCOperand &Offset,
+                         bool IsLocal, SMLoc IDLoc, MCStreamer &Out,
+                         const MCSubtargetInfo *STI);
+
+  bool reportParseError(Twine ErrorMsg);
+
+  bool parseMemOffset(const MCExpr *&Res);
+
+  bool isEvaluated(const MCExpr *Expr);
+  bool parseDirectiveSet();
+
+  bool parseSetFpDirective();
+  bool parseSetSoftFloatDirective();
+  bool parseSetHardFloatDirective();
+
+  bool parseSetAssignment();
+
+  bool parseDirectiveModule();
+  bool parseDirectiveModuleFP();
+  bool parseFpABIValue(LoongArchFPABIInfo::FpABIKind &FpABI,
+                       StringRef Directive);
+
+  bool parseInternalDirectiveReallowModule();
+
+  int matchCPURegisterName(StringRef Symbol);
+
+  int matchFPURegisterName(StringRef Name);
+
+  int matchFCFRRegisterName(StringRef Name);
+  int matchFCSRRegisterName(StringRef Name);
+
+  bool processInstruction(MCInst &Inst, SMLoc IDLoc, MCStreamer &Out,
+                          const MCSubtargetInfo *STI);
+
+  void setFeatureBits(uint64_t Feature, StringRef FeatureString) {
+    if (!(getSTI().getFeatureBits()[Feature])) {
+      MCSubtargetInfo &STI = copySTI();
+      setAvailableFeatures(
+          ComputeAvailableFeatures(STI.ToggleFeature(FeatureString)));
+      AssemblerOptions.back()->setFeatures(STI.getFeatureBits());
+    }
+  }
+
+  void clearFeatureBits(uint64_t Feature, StringRef FeatureString) {
+    if (getSTI().getFeatureBits()[Feature]) {
+      MCSubtargetInfo &STI = copySTI();
+      setAvailableFeatures(
+          ComputeAvailableFeatures(STI.ToggleFeature(FeatureString)));
+      AssemblerOptions.back()->setFeatures(STI.getFeatureBits());
+    }
+  }
+
+  void setModuleFeatureBits(uint64_t Feature, StringRef FeatureString) {
+    setFeatureBits(Feature, FeatureString);
+    AssemblerOptions.front()->setFeatures(getSTI().getFeatureBits());
+  }
+
+  void clearModuleFeatureBits(uint64_t Feature, StringRef FeatureString) {
+    clearFeatureBits(Feature, FeatureString);
+    AssemblerOptions.front()->setFeatures(getSTI().getFeatureBits());
+  }
+
+public:
+  enum LoongArchMatchResultTy {
+    Match_RequiresNoZeroRegister = FIRST_TARGET_MATCH_RESULT_TY,
+    Match_RequiresNoRaRegister,
+    Match_RequiresSameSrcAndDst,
+    Match_RequiresRange0_31,
+    Match_RequiresRange0_63,
+    Match_MsbHigherThanLsb,
+    Match_RequiresPosSizeUImm6,
+#define GET_OPERAND_DIAGNOSTIC_TYPES
+#include "LoongArchGenAsmMatcher.inc"
+#undef GET_OPERAND_DIAGNOSTIC_TYPES
+  };
+
+  LoongArchAsmParser(const MCSubtargetInfo &sti, MCAsmParser &parser,
+                     const MCInstrInfo &MII, const MCTargetOptions &Options)
+    : MCTargetAsmParser(Options, sti, MII),
+        ABI(LoongArchABIInfo::computeTargetABI(Triple(sti.getTargetTriple()),
+                                               sti.getCPU(), Options)) {
+    MCAsmParserExtension::Initialize(parser);
+
+    parser.addAliasForDirective(".asciiz", ".asciz");
+    parser.addAliasForDirective(".hword", ".2byte");
+    parser.addAliasForDirective(".word", ".4byte");
+    parser.addAliasForDirective(".dword", ".8byte");
+
+    // Initialize the set of available features.
+    setAvailableFeatures(ComputeAvailableFeatures(getSTI().getFeatureBits()));
+
+    // Remember the initial assembler options. The user can not modify these.
+    AssemblerOptions.push_back(
+        std::make_unique<LoongArchAssemblerOptions>(getSTI().getFeatureBits()));
+
+    // Create an assembler options environment for the user to modify.
+    AssemblerOptions.push_back(
+        std::make_unique<LoongArchAssemblerOptions>(getSTI().getFeatureBits()));
+
+    getTargetStreamer().updateABIInfo(*this);
+
+    CurrentFn = nullptr;
+
+    IsPicEnabled = getContext().getObjectFileInfo()->isPositionIndependent();
+  }
+
+  bool is64Bit() const {
+    return getSTI().getFeatureBits()[LoongArch::Feature64Bit];
+  }
+
+  bool isFP64bit() const {
+    return getSTI().getFeatureBits()[LoongArch::FeatureFP64Bit];
+  }
+
+  const LoongArchABIInfo &getABI() const { return ABI; }
+  bool isABI_LPX32() const { return ABI.IsLPX32(); }
+  bool isABI_LP64D() const { return ABI.IsLP64D(); }
+  bool isABI_LP32() const { return ABI.IsLP32(); }
+
+  bool inPicMode() {
+    return IsPicEnabled;
+  }
+
+  bool useSoftFloat() const {
+    return getSTI().getFeatureBits()[LoongArch::FeatureSoftFloat];
+  }
+
+  const MCExpr *createTargetUnaryExpr(const MCExpr *E,
+                                      AsmToken::TokenKind OperatorToken,
+                                      MCContext &Ctx) override {
+    switch(OperatorToken) {
+    default:
+      llvm_unreachable("Unknown token");
+      return nullptr;
+#if 0
+    case AsmToken::PercentPlt:
+      return LoongArchMCExpr::create(LoongArchMCExpr::MEK_PLT, E, Ctx);
+#endif
+    }
+  }
+};
+
+/// LoongArchOperand - Instances of this class represent a parsed LoongArch machine
+/// instruction.
+class LoongArchOperand : public MCParsedAsmOperand {
+public:
+  /// Broad categories of register classes
+  /// The exact class is finalized by the render method.
+  enum RegKind {
+    RegKind_GPR = 1,      /// GPR32 and GPR64 (depending on is64Bit())
+    RegKind_FGR = 2,      /// FGR32, FGR64 (depending on isFP64bit())
+    RegKind_FCFR = 4,     /// FCFR
+    RegKind_FCSR = 8,     /// FCSR
+    RegKind_Numeric = RegKind_GPR | RegKind_FGR | RegKind_FCFR | RegKind_FCSR
+  };
+
+private:
+  enum KindTy {
+    k_Immediate,     /// An immediate (possibly involving symbol references)
+    k_Memory,        /// Base + Offset Memory Address
+    k_RegisterIndex, /// A register index in one or more RegKind.
+    k_Token,         /// A simple token
+    k_RegList,       /// A physical register list
+  } Kind;
+
+public:
+  LoongArchOperand(KindTy K, LoongArchAsmParser &Parser)
+      : MCParsedAsmOperand(), Kind(K), AsmParser(Parser) {}
+
+  ~LoongArchOperand() override {
+    switch (Kind) {
+    case k_Memory:
+      delete Mem.Base;
+      break;
+    case k_RegList:
+      delete RegList.List;
+      break;
+    case k_Immediate:
+    case k_RegisterIndex:
+    case k_Token:
+      break;
+    }
+  }
+
+private:
+  /// For diagnostics, and checking the assembler temporary
+  LoongArchAsmParser &AsmParser;
+
+  struct Token {
+    const char *Data;
+    unsigned Length;
+  };
+
+  struct RegIdxOp {
+    unsigned Index; /// Index into the register class
+    RegKind Kind;   /// Bitfield of the kinds it could possibly be
+    struct Token Tok; /// The input token this operand originated from.
+    const MCRegisterInfo *RegInfo;
+  };
+
+  struct ImmOp {
+    const MCExpr *Val;
+  };
+
+  struct MemOp {
+    LoongArchOperand *Base;
+    const MCExpr *Off;
+  };
+
+  struct RegListOp {
+    SmallVector<unsigned, 10> *List;
+  };
+
+  union {
+    struct Token Tok;
+    struct RegIdxOp RegIdx;
+    struct ImmOp Imm;
+    struct MemOp Mem;
+    struct RegListOp RegList;
+  };
+
+  SMLoc StartLoc, EndLoc;
+
+  /// Internal constructor for register kinds
+  static std::unique_ptr<LoongArchOperand> CreateReg(unsigned Index, StringRef Str,
+                                                     RegKind RegKind,
+                                                     const MCRegisterInfo *RegInfo,
+                                                     SMLoc S, SMLoc E,
+                                                     LoongArchAsmParser &Parser) {
+    auto Op = std::make_unique<LoongArchOperand>(k_RegisterIndex, Parser);
+    Op->RegIdx.Index = Index;
+    Op->RegIdx.RegInfo = RegInfo;
+    Op->RegIdx.Kind = RegKind;
+    Op->RegIdx.Tok.Data = Str.data();
+    Op->RegIdx.Tok.Length = Str.size();
+    Op->StartLoc = S;
+    Op->EndLoc = E;
+    return Op;
+  }
+
+public:
+  /// Coerce the register to GPR32 and return the real register for the current
+  /// target.
+  unsigned getGPR32Reg() const {
+    assert(isRegIdx() && (RegIdx.Kind & RegKind_GPR) && "Invalid access!");
+    unsigned ClassID = LoongArch::GPR32RegClassID;
+    return RegIdx.RegInfo->getRegClass(ClassID).getRegister(RegIdx.Index);
+  }
+
+  /// Coerce the register to GPR32 and return the real register for the current
+  /// target.
+  unsigned getGPRMM16Reg() const {
+    assert(isRegIdx() && (RegIdx.Kind & RegKind_GPR) && "Invalid access!");
+    unsigned ClassID = LoongArch::GPR32RegClassID;
+    return RegIdx.RegInfo->getRegClass(ClassID).getRegister(RegIdx.Index);
+  }
+
+  /// Coerce the register to GPR64 and return the real register for the current
+  /// target.
+  unsigned getGPR64Reg() const {
+    assert(isRegIdx() && (RegIdx.Kind & RegKind_GPR) && "Invalid access!");
+    unsigned ClassID = LoongArch::GPR64RegClassID;
+    return RegIdx.RegInfo->getRegClass(ClassID).getRegister(RegIdx.Index);
+  }
+
+private:
+  /// Coerce the register to FGR64 and return the real register for the current
+  /// target.
+  unsigned getFGR64Reg() const {
+    assert(isRegIdx() && (RegIdx.Kind & RegKind_FGR) && "Invalid access!");
+    return RegIdx.RegInfo->getRegClass(LoongArch::FGR64RegClassID)
+        .getRegister(RegIdx.Index);
+  }
+
+  /// Coerce the register to FGR32 and return the real register for the current
+  /// target.
+  unsigned getFGR32Reg() const {
+    assert(isRegIdx() && (RegIdx.Kind & RegKind_FGR) && "Invalid access!");
+    return RegIdx.RegInfo->getRegClass(LoongArch::FGR32RegClassID)
+        .getRegister(RegIdx.Index);
+  }
+
+  /// Coerce the register to FCFR and return the real register for the current
+  /// target.
+  unsigned getFCFRReg() const {
+    assert(isRegIdx() && (RegIdx.Kind & RegKind_FCFR) && "Invalid access!");
+    return RegIdx.RegInfo->getRegClass(LoongArch::FCFRRegClassID)
+        .getRegister(RegIdx.Index);
+  }
+
+  /// Coerce the register to CCR and return the real register for the
+  /// current target.
+  unsigned getFCSRReg() const {
+    assert(isRegIdx() && (RegIdx.Kind & RegKind_FCSR) && "Invalid access!");
+    unsigned ClassID = LoongArch::FCSRRegClassID;
+    return RegIdx.RegInfo->getRegClass(ClassID).getRegister(RegIdx.Index);
+  }
+
+public:
+  void addExpr(MCInst &Inst, const MCExpr *Expr) const {
+    // Add as immediate when possible.  Null MCExpr = 0.
+    if (!Expr)
+      Inst.addOperand(MCOperand::createImm(0));
+    else if (const MCConstantExpr *CE = dyn_cast<MCConstantExpr>(Expr))
+      Inst.addOperand(MCOperand::createImm(CE->getValue()));
+    else
+      Inst.addOperand(MCOperand::createExpr(Expr));
+  }
+
+  void addRegOperands(MCInst &Inst, unsigned N) const {
+    llvm_unreachable("Use a custom parser instead");
+  }
+
+  /// Render the operand to an MCInst as a GPR32
+  /// Asserts if the wrong number of operands are requested, or the operand
+  /// is not a k_RegisterIndex compatible with RegKind_GPR
+  void addGPR32ZeroAsmRegOperands(MCInst &Inst, unsigned N) const {
+    assert(N == 1 && "Invalid number of operands!");
+    Inst.addOperand(MCOperand::createReg(getGPR32Reg()));
+  }
+
+  void addGPR32NonZeroAsmRegOperands(MCInst &Inst, unsigned N) const {
+    assert(N == 1 && "Invalid number of operands!");
+    Inst.addOperand(MCOperand::createReg(getGPR32Reg()));
+  }
+
+  void addGPR32AsmRegOperands(MCInst &Inst, unsigned N) const {
+    assert(N == 1 && "Invalid number of operands!");
+    Inst.addOperand(MCOperand::createReg(getGPR32Reg()));
+  }
+
+  void addGPRMM16AsmRegOperands(MCInst &Inst, unsigned N) const {
+    assert(N == 1 && "Invalid number of operands!");
+    Inst.addOperand(MCOperand::createReg(getGPRMM16Reg()));
+  }
+
+  void addGPRMM16AsmRegZeroOperands(MCInst &Inst, unsigned N) const {
+    assert(N == 1 && "Invalid number of operands!");
+    Inst.addOperand(MCOperand::createReg(getGPRMM16Reg()));
+  }
+
+  void addGPRMM16AsmRegMovePOperands(MCInst &Inst, unsigned N) const {
+    assert(N == 1 && "Invalid number of operands!");
+    Inst.addOperand(MCOperand::createReg(getGPRMM16Reg()));
+  }
+
+  void addGPRMM16AsmRegMovePPairFirstOperands(MCInst &Inst, unsigned N) const {
+    assert(N == 1 && "Invalid number of operands!");
+    Inst.addOperand(MCOperand::createReg(getGPRMM16Reg()));
+  }
+
+  void addGPRMM16AsmRegMovePPairSecondOperands(MCInst &Inst,
+                                               unsigned N) const {
+    assert(N == 1 && "Invalid number of operands!");
+    Inst.addOperand(MCOperand::createReg(getGPRMM16Reg()));
+  }
+
+  /// Render the operand to an MCInst as a GPR64
+  /// Asserts if the wrong number of operands are requested, or the operand
+  /// is not a k_RegisterIndex compatible with RegKind_GPR
+  void addGPR64AsmRegOperands(MCInst &Inst, unsigned N) const {
+    assert(N == 1 && "Invalid number of operands!");
+    Inst.addOperand(MCOperand::createReg(getGPR64Reg()));
+  }
+
+  void addStrictlyFGR64AsmRegOperands(MCInst &Inst, unsigned N) const {
+    assert(N == 1 && "Invalid number of operands!");
+    Inst.addOperand(MCOperand::createReg(getFGR64Reg()));
+  }
+
+  void addFGR64AsmRegOperands(MCInst &Inst, unsigned N) const {
+    assert(N == 1 && "Invalid number of operands!");
+    Inst.addOperand(MCOperand::createReg(getFGR64Reg()));
+  }
+
+  void addFGR32AsmRegOperands(MCInst &Inst, unsigned N) const {
+    assert(N == 1 && "Invalid number of operands!");
+    Inst.addOperand(MCOperand::createReg(getFGR32Reg()));
+  }
+
+  void addStrictlyFGR32AsmRegOperands(MCInst &Inst, unsigned N) const {
+    assert(N == 1 && "Invalid number of operands!");
+    Inst.addOperand(MCOperand::createReg(getFGR32Reg()));
+  }
+
+  void addFCFRAsmRegOperands(MCInst &Inst, unsigned N) const {
+    assert(N == 1 && "Invalid number of operands!");
+    Inst.addOperand(MCOperand::createReg(getFCFRReg()));
+  }
+
+  void addFCSRAsmRegOperands(MCInst &Inst, unsigned N) const {
+    assert(N == 1 && "Invalid number of operands!");
+    Inst.addOperand(MCOperand::createReg(getFCSRReg()));
+  }
+
+  template <unsigned Bits, int Offset = 0, int AdjustOffset = 0>
+  void addConstantUImmOperands(MCInst &Inst, unsigned N) const {
+    assert(N == 1 && "Invalid number of operands!");
+    uint64_t Imm = getConstantImm() - Offset;
+    Imm &= (1ULL << Bits) - 1;
+    Imm += Offset;
+    Imm += AdjustOffset;
+    Inst.addOperand(MCOperand::createImm(Imm));
+  }
+
+  template <unsigned Bits>
+  void addSImmOperands(MCInst &Inst, unsigned N) const {
+    if (isImm() && !isConstantImm()) {
+      addExpr(Inst, getImm());
+      return;
+    }
+    addConstantSImmOperands<Bits, 0, 0>(Inst, N);
+  }
+
+  template <unsigned Bits>
+  void addUImmOperands(MCInst &Inst, unsigned N) const {
+    if (isImm() && !isConstantImm()) {
+      addExpr(Inst, getImm());
+      return;
+    }
+    addConstantUImmOperands<Bits, 0, 0>(Inst, N);
+  }
+
+  template <unsigned Bits, int Offset = 0, int AdjustOffset = 0>
+  void addConstantSImmOperands(MCInst &Inst, unsigned N) const {
+    assert(N == 1 && "Invalid number of operands!");
+    int64_t Imm = getConstantImm() - Offset;
+    Imm = SignExtend64<Bits>(Imm);
+    Imm += Offset;
+    Imm += AdjustOffset;
+    Inst.addOperand(MCOperand::createImm(Imm));
+  }
+
+  void addImmOperands(MCInst &Inst, unsigned N) const {
+    assert(N == 1 && "Invalid number of operands!");
+    const MCExpr *Expr = getImm();
+    addExpr(Inst, Expr);
+  }
+
+  void addMemOperands(MCInst &Inst, unsigned N) const {
+    assert(N == 2 && "Invalid number of operands!");
+
+    Inst.addOperand(MCOperand::createReg(AsmParser.getABI().ArePtrs64bit()
+                                             ? getMemBase()->getGPR64Reg()
+                                             : getMemBase()->getGPR32Reg()));
+
+    const MCExpr *Expr = getMemOff();
+    addExpr(Inst, Expr);
+  }
+
+  void addRegListOperands(MCInst &Inst, unsigned N) const {
+    assert(N == 1 && "Invalid number of operands!");
+
+    for (auto RegNo : getRegList())
+      Inst.addOperand(MCOperand::createReg(RegNo));
+  }
+
+  bool isReg() const override {
+    // As a special case until we sort out the definition of div/divu, accept
+    // $0/$zero here so that MCK_ZERO works correctly.
+    return isGPRAsmReg() && RegIdx.Index == 0;
+  }
+
+  bool isRegIdx() const { return Kind == k_RegisterIndex; }
+  bool isImm() const override { return Kind == k_Immediate; }
+
+  bool isConstantImm() const {
+    int64_t Res;
+    return isImm() && getImm()->evaluateAsAbsolute(Res);
+  }
+
+  bool isConstantImmz() const {
+    return isConstantImm() && getConstantImm() == 0;
+  }
+
+  template <unsigned Bits, int Offset = 0> bool isConstantUImm() const {
+    return isConstantImm() && isUInt<Bits>(getConstantImm() - Offset);
+  }
+
+  template <unsigned Bits> bool isSImm() const {
+    return isConstantImm() ? isInt<Bits>(getConstantImm()) : isImm();
+  }
+
+  template <unsigned Bits> bool isUImm() const {
+    return isConstantImm() ? isUInt<Bits>(getConstantImm()) : isImm();
+  }
+
+  template <unsigned Bits> bool isAnyImm() const {
+    return isConstantImm() ? (isInt<Bits>(getConstantImm()) ||
+                              isUInt<Bits>(getConstantImm()))
+                           : isImm();
+  }
+
+  template <unsigned Bits, int Offset = 0> bool isConstantSImm() const {
+    return isConstantImm() && isInt<Bits>(getConstantImm() - Offset);
+  }
+
+  template <unsigned Bottom, unsigned Top> bool isConstantUImmRange() const {
+    return isConstantImm() && getConstantImm() >= Bottom &&
+           getConstantImm() <= Top;
+  }
+
+  bool isToken() const override {
+    // Note: It's not possible to pretend that other operand kinds are tokens.
+    // The matcher emitter checks tokens first.
+    return Kind == k_Token;
+  }
+
+  bool isMem() const override { return Kind == k_Memory; }
+
+  bool isConstantMemOff() const {
+    return isMem() && isa<MCConstantExpr>(getMemOff());
+  }
+
+  // Allow relocation operators.
+  // FIXME: This predicate and others need to look through binary expressions
+  //        and determine whether a Value is a constant or not.
+  template <unsigned Bits, unsigned ShiftAmount = 0>
+  bool isMemWithSimmOffset() const {
+    if (!isMem())
+      return false;
+    if (!getMemBase()->isGPRAsmReg())
+      return false;
+    if (isa<MCTargetExpr>(getMemOff()) ||
+        (isConstantMemOff() &&
+         isShiftedInt<Bits, ShiftAmount>(getConstantMemOff())))
+      return true;
+    MCValue Res;
+    bool IsReloc = getMemOff()->evaluateAsRelocatable(Res, nullptr, nullptr);
+    return IsReloc && isShiftedInt<Bits, ShiftAmount>(Res.getConstant());
+  }
+
+  bool isMemWithPtrSizeOffset() const {
+    if (!isMem())
+      return false;
+    if (!getMemBase()->isGPRAsmReg())
+      return false;
+    const unsigned PtrBits = AsmParser.getABI().ArePtrs64bit() ? 64 : 32;
+    if (isa<MCTargetExpr>(getMemOff()) ||
+        (isConstantMemOff() && isIntN(PtrBits, getConstantMemOff())))
+      return true;
+    MCValue Res;
+    bool IsReloc = getMemOff()->evaluateAsRelocatable(Res, nullptr, nullptr);
+    return IsReloc && isIntN(PtrBits, Res.getConstant());
+  }
+
+  bool isMemWithGRPMM16Base() const {
+    return isMem() && getMemBase()->isMM16AsmReg();
+  }
+
+  template <unsigned Bits> bool isMemWithUimmOffsetSP() const {
+    return isMem() && isConstantMemOff() && isUInt<Bits>(getConstantMemOff())
+      && getMemBase()->isRegIdx() && (getMemBase()->getGPR32Reg() == LoongArch::SP);
+  }
+
+  template <unsigned Bits> bool isMemWithUimmWordAlignedOffsetSP() const {
+    return isMem() && isConstantMemOff() && isUInt<Bits>(getConstantMemOff())
+      && (getConstantMemOff() % 4 == 0) && getMemBase()->isRegIdx()
+      && (getMemBase()->getGPR32Reg() == LoongArch::SP);
+  }
+
+  template <unsigned Bits, unsigned ShiftLeftAmount>
+  bool isScaledUImm() const {
+    return isConstantImm() &&
+           isShiftedUInt<Bits, ShiftLeftAmount>(getConstantImm());
+  }
+
+  template <unsigned Bits, unsigned ShiftLeftAmount>
+  bool isScaledSImm() const {
+    if (isConstantImm() &&
+        isShiftedInt<Bits, ShiftLeftAmount>(getConstantImm()))
+      return true;
+    // Operand can also be a symbol or symbol plus
+    // offset in case of relocations.
+    if (Kind != k_Immediate)
+      return false;
+    MCValue Res;
+    bool Success = getImm()->evaluateAsRelocatable(Res, nullptr, nullptr);
+    return Success && isShiftedInt<Bits, ShiftLeftAmount>(Res.getConstant());
+  }
+
+  bool isRegList16() const {
+    if (!isRegList())
+      return false;
+
+    int Size = RegList.List->size();
+    if (Size < 2 || Size > 5)
+      return false;
+
+    unsigned R0 = RegList.List->front();
+    unsigned R1 = RegList.List->back();
+    if (!((R0 == LoongArch::S0 && R1 == LoongArch::RA) ||
+          (R0 == LoongArch::S0_64 && R1 == LoongArch::RA_64)))
+      return false;
+
+    int PrevReg = *RegList.List->begin();
+    for (int i = 1; i < Size - 1; i++) {
+      int Reg = (*(RegList.List))[i];
+      if ( Reg != PrevReg + 1)
+        return false;
+      PrevReg = Reg;
+    }
+
+    return true;
+  }
+
+  bool isInvNum() const { return Kind == k_Immediate; }
+
+  bool isLSAImm() const {
+    if (!isConstantImm())
+      return false;
+    int64_t Val = getConstantImm();
+    return 1 <= Val && Val <= 4;
+  }
+
+  bool isRegList() const { return Kind == k_RegList; }
+
+  StringRef getToken() const {
+    assert(Kind == k_Token && "Invalid access!");
+    return StringRef(Tok.Data, Tok.Length);
+  }
+
+  unsigned getReg() const override {
+    // As a special case until we sort out the definition of div/divu, accept
+    // $0/$zero here so that MCK_ZERO works correctly.
+    if (Kind == k_RegisterIndex && RegIdx.Index == 0 &&
+        RegIdx.Kind & RegKind_GPR)
+      return getGPR32Reg(); // FIXME: GPR64 too
+
+    llvm_unreachable("Invalid access!");
+    return 0;
+  }
+
+  const MCExpr *getImm() const {
+    assert((Kind == k_Immediate) && "Invalid access!");
+    return Imm.Val;
+  }
+
+  int64_t getConstantImm() const {
+    const MCExpr *Val = getImm();
+    int64_t Value = 0;
+    (void)Val->evaluateAsAbsolute(Value);
+    return Value;
+  }
+
+  LoongArchOperand *getMemBase() const {
+    assert((Kind == k_Memory) && "Invalid access!");
+    return Mem.Base;
+  }
+
+  const MCExpr *getMemOff() const {
+    assert((Kind == k_Memory) && "Invalid access!");
+    return Mem.Off;
+  }
+
+  int64_t getConstantMemOff() const {
+    return static_cast<const MCConstantExpr *>(getMemOff())->getValue();
+  }
+
+  const SmallVectorImpl<unsigned> &getRegList() const {
+    assert((Kind == k_RegList) && "Invalid access!");
+    return *(RegList.List);
+  }
+
+  static std::unique_ptr<LoongArchOperand> CreateToken(StringRef Str, SMLoc S,
+                                                  LoongArchAsmParser &Parser) {
+    auto Op = std::make_unique<LoongArchOperand>(k_Token, Parser);
+    Op->Tok.Data = Str.data();
+    Op->Tok.Length = Str.size();
+    Op->StartLoc = S;
+    Op->EndLoc = S;
+    return Op;
+  }
+
+  /// Create a numeric register (e.g. $1). The exact register remains
+  /// unresolved until an instruction successfully matches
+  static std::unique_ptr<LoongArchOperand>
+  createNumericReg(unsigned Index, StringRef Str, const MCRegisterInfo *RegInfo,
+                   SMLoc S, SMLoc E, LoongArchAsmParser &Parser) {
+    LLVM_DEBUG(dbgs() << "createNumericReg(" << Index << ", ...)\n");
+    return CreateReg(Index, Str, RegKind_Numeric, RegInfo, S, E, Parser);
+  }
+
+  /// Create a register that is definitely a GPR.
+  /// This is typically only used for named registers such as $gp.
+  static std::unique_ptr<LoongArchOperand>
+  createGPRReg(unsigned Index, StringRef Str, const MCRegisterInfo *RegInfo,
+               SMLoc S, SMLoc E, LoongArchAsmParser &Parser) {
+    return CreateReg(Index, Str, RegKind_GPR, RegInfo, S, E, Parser);
+  }
+
+  /// Create a register that is definitely a FGR.
+  /// This is typically only used for named registers such as $f0.
+  static std::unique_ptr<LoongArchOperand>
+  createFGRReg(unsigned Index, StringRef Str, const MCRegisterInfo *RegInfo,
+               SMLoc S, SMLoc E, LoongArchAsmParser &Parser) {
+    return CreateReg(Index, Str, RegKind_FGR, RegInfo, S, E, Parser);
+  }
+
+  /// Create a register that is definitely an FCFR.
+  /// This is typically only used for named registers such as $fcc0.
+  static std::unique_ptr<LoongArchOperand>
+  createFCFRReg(unsigned Index, StringRef Str, const MCRegisterInfo *RegInfo,
+               SMLoc S, SMLoc E, LoongArchAsmParser &Parser) {
+    return CreateReg(Index, Str, RegKind_FCFR, RegInfo, S, E, Parser);
+  }
+
+  /// Create a register that is definitely an FCSR.
+  /// This is typically only used for named registers such as $fcsr0.
+  static std::unique_ptr<LoongArchOperand>
+  createFCSRReg(unsigned Index, StringRef Str, const MCRegisterInfo *RegInfo,
+                SMLoc S, SMLoc E, LoongArchAsmParser &Parser) {
+    return CreateReg(Index, Str, RegKind_FCSR, RegInfo, S, E, Parser);
+  }
+
+  static std::unique_ptr<LoongArchOperand>
+  CreateImm(const MCExpr *Val, SMLoc S, SMLoc E, LoongArchAsmParser &Parser) {
+    auto Op = std::make_unique<LoongArchOperand>(k_Immediate, Parser);
+    Op->Imm.Val = Val;
+    Op->StartLoc = S;
+    Op->EndLoc = E;
+    return Op;
+  }
+
+  static std::unique_ptr<LoongArchOperand>
+  CreateMem(std::unique_ptr<LoongArchOperand> Base, const MCExpr *Off, SMLoc S,
+            SMLoc E, LoongArchAsmParser &Parser) {
+    auto Op = std::make_unique<LoongArchOperand>(k_Memory, Parser);
+    Op->Mem.Base = Base.release();
+    Op->Mem.Off = Off;
+    Op->StartLoc = S;
+    Op->EndLoc = E;
+    return Op;
+  }
+
+  static std::unique_ptr<LoongArchOperand>
+  CreateRegList(SmallVectorImpl<unsigned> &Regs, SMLoc StartLoc, SMLoc EndLoc,
+                LoongArchAsmParser &Parser) {
+    assert(Regs.size() > 0 && "Empty list not allowed");
+
+    auto Op = std::make_unique<LoongArchOperand>(k_RegList, Parser);
+    Op->RegList.List = new SmallVector<unsigned, 10>(Regs.begin(), Regs.end());
+    Op->StartLoc = StartLoc;
+    Op->EndLoc = EndLoc;
+    return Op;
+  }
+
+ bool isGPRZeroAsmReg() const {
+    return isRegIdx() && RegIdx.Kind & RegKind_GPR && RegIdx.Index == 0;
+  }
+
+ bool isGPRNonZeroAsmReg() const {
+   return isRegIdx() && RegIdx.Kind & RegKind_GPR && RegIdx.Index > 0 &&
+          RegIdx.Index <= 31;
+  }
+
+  bool isGPRAsmReg() const {
+    return isRegIdx() && RegIdx.Kind & RegKind_GPR && RegIdx.Index <= 31;
+  }
+
+  bool isMM16AsmReg() const {
+    if (!(isRegIdx() && RegIdx.Kind))
+      return false;
+    return ((RegIdx.Index >= 2 && RegIdx.Index <= 7)
+            || RegIdx.Index == 16 || RegIdx.Index == 17);
+
+  }
+  bool isMM16AsmRegZero() const {
+    if (!(isRegIdx() && RegIdx.Kind))
+      return false;
+    return (RegIdx.Index == 0 ||
+            (RegIdx.Index >= 2 && RegIdx.Index <= 7) ||
+            RegIdx.Index == 17);
+  }
+
+  bool isMM16AsmRegMoveP() const {
+    if (!(isRegIdx() && RegIdx.Kind))
+      return false;
+    return (RegIdx.Index == 0 || (RegIdx.Index >= 2 && RegIdx.Index <= 3) ||
+      (RegIdx.Index >= 16 && RegIdx.Index <= 20));
+  }
+
+  bool isMM16AsmRegMovePPairFirst() const {
+    if (!(isRegIdx() && RegIdx.Kind))
+      return false;
+    return RegIdx.Index >= 4 && RegIdx.Index <= 6;
+  }
+
+  bool isMM16AsmRegMovePPairSecond() const {
+    if (!(isRegIdx() && RegIdx.Kind))
+      return false;
+    return (RegIdx.Index == 21 || RegIdx.Index == 22 ||
+      (RegIdx.Index >= 5 && RegIdx.Index <= 7));
+  }
+
+  bool isFGRAsmReg() const {
+    return isRegIdx() && RegIdx.Kind & RegKind_FGR && RegIdx.Index <= 31;
+  }
+
+  bool isStrictlyFGRAsmReg() const {
+    return isRegIdx() && RegIdx.Kind == RegKind_FGR && RegIdx.Index <= 31;
+  }
+
+  bool isFCSRAsmReg() const {
+    return isRegIdx() && RegIdx.Kind & RegKind_FCSR && RegIdx.Index <= 3;
+  }
+
+  bool isFCFRAsmReg() const {
+    if (!(isRegIdx() && RegIdx.Kind & RegKind_FCFR))
+      return false;
+    return RegIdx.Index <= 7;
+  }
+
+  /// getStartLoc - Get the location of the first token of this operand.
+  SMLoc getStartLoc() const override { return StartLoc; }
+  /// getEndLoc - Get the location of the last token of this operand.
+  SMLoc getEndLoc() const override { return EndLoc; }
+
+  void print(raw_ostream &OS) const override {
+    switch (Kind) {
+    case k_Immediate:
+      OS << "Imm<";
+      OS << *Imm.Val;
+      OS << ">";
+      break;
+    case k_Memory:
+      OS << "Mem<";
+      Mem.Base->print(OS);
+      OS << ", ";
+      OS << *Mem.Off;
+      OS << ">";
+      break;
+    case k_RegisterIndex:
+      OS << "RegIdx<" << RegIdx.Index << ":" << RegIdx.Kind << ", "
+         << StringRef(RegIdx.Tok.Data, RegIdx.Tok.Length) << ">";
+      break;
+    case k_Token:
+      OS << getToken();
+      break;
+    case k_RegList:
+      OS << "RegList< ";
+      for (auto Reg : (*RegList.List))
+        OS << Reg << " ";
+      OS <<  ">";
+      break;
+    }
+  }
+
+  bool isValidForTie(const LoongArchOperand &Other) const {
+    if (Kind != Other.Kind)
+      return false;
+
+    switch (Kind) {
+    default:
+      llvm_unreachable("Unexpected kind");
+      return false;
+    case k_RegisterIndex: {
+      StringRef Token(RegIdx.Tok.Data, RegIdx.Tok.Length);
+      StringRef OtherToken(Other.RegIdx.Tok.Data, Other.RegIdx.Tok.Length);
+      return Token == OtherToken;
+    }
+    }
+  }
+}; // class LoongArchOperand
+
+} // end anonymous namespace
+
+namespace llvm {
+
+extern const MCInstrDesc LoongArchInsts[];
+
+} // end namespace llvm
+
+static const MCInstrDesc &getInstDesc(unsigned Opcode) {
+  return LoongArchInsts[Opcode];
+}
+
+static const MCSymbol *getSingleMCSymbol(const MCExpr *Expr) {
+  if (const MCSymbolRefExpr *SRExpr = dyn_cast<MCSymbolRefExpr>(Expr)) {
+    return &SRExpr->getSymbol();
+  }
+
+  if (const MCBinaryExpr *BExpr = dyn_cast<MCBinaryExpr>(Expr)) {
+    const MCSymbol *LHSSym = getSingleMCSymbol(BExpr->getLHS());
+    const MCSymbol *RHSSym = getSingleMCSymbol(BExpr->getRHS());
+
+    if (LHSSym)
+      return LHSSym;
+
+    if (RHSSym)
+      return RHSSym;
+
+    return nullptr;
+  }
+
+  if (const MCUnaryExpr *UExpr = dyn_cast<MCUnaryExpr>(Expr))
+    return getSingleMCSymbol(UExpr->getSubExpr());
+
+  return nullptr;
+}
+
+static unsigned countMCSymbolRefExpr(const MCExpr *Expr) {
+  if (isa<MCSymbolRefExpr>(Expr))
+    return 1;
+
+  if (const MCBinaryExpr *BExpr = dyn_cast<MCBinaryExpr>(Expr))
+    return countMCSymbolRefExpr(BExpr->getLHS()) +
+           countMCSymbolRefExpr(BExpr->getRHS());
+
+  if (const MCUnaryExpr *UExpr = dyn_cast<MCUnaryExpr>(Expr))
+    return countMCSymbolRefExpr(UExpr->getSubExpr());
+
+  return 0;
+}
+
+bool LoongArchAsmParser::processInstruction(MCInst &Inst, SMLoc IDLoc,
+                                            MCStreamer &Out,
+                                            const MCSubtargetInfo *STI) {
+  const MCInstrDesc &MCID = getInstDesc(Inst.getOpcode());
+
+  Inst.setLoc(IDLoc);
+
+  if (MCID.isBranch() || MCID.isCall()) {
+    const unsigned Opcode = Inst.getOpcode();
+    MCOperand Offset;
+
+    switch (Opcode) {
+    default:
+      break;
+    case LoongArch::BEQ:
+    case LoongArch::BNE:
+      assert(MCID.getNumOperands() == 3 && "unexpected number of operands");
+      Offset = Inst.getOperand(2);
+      if (!Offset.isImm())
+        break; // We'll deal with this situation later on when applying fixups.
+      if (!isIntN(17, Offset.getImm()))
+        return Error(IDLoc, "branch target out of range");
+      if (offsetToAlignment(Offset.getImm(),
+                            Align(1LL << 2)))
+        return Error(IDLoc, "branch to misaligned address");
+      break;
+    }
+  }
+
+  bool IsPCRelativeLoad = (MCID.TSFlags & LoongArchII::IsPCRelativeLoad) != 0;
+  if ((MCID.mayLoad() || MCID.mayStore()) && !IsPCRelativeLoad) {
+    // Check the offset of memory operand, if it is a symbol
+    // reference or immediate we may have to expand instructions.
+    for (unsigned i = 0; i < MCID.getNumOperands(); i++) {
+      const MCOperandInfo &OpInfo = MCID.OpInfo[i];
+      if ((OpInfo.OperandType == MCOI::OPERAND_MEMORY) ||
+          (OpInfo.OperandType == MCOI::OPERAND_UNKNOWN)) {
+        MCOperand &Op = Inst.getOperand(i);
+        if (Op.isImm()) {
+          int64_t MemOffset = Op.getImm();
+          if (MemOffset < -32768 || MemOffset > 32767) {
+            return getParser().hasPendingError();
+          }
+        } else if (Op.isExpr()) {
+          const MCExpr *Expr = Op.getExpr();
+          if (Expr->getKind() == MCExpr::SymbolRef) {
+            const MCSymbolRefExpr *SR =
+                static_cast<const MCSymbolRefExpr *>(Expr);
+            if (SR->getKind() == MCSymbolRefExpr::VK_None) {
+              return getParser().hasPendingError();
+            }
+          } else if (!isEvaluated(Expr)) {
+            return getParser().hasPendingError();
+          }
+        }
+      }
+    } // for
+  }   // if load/store
+
+  MacroExpanderResultTy ExpandResult =
+      tryExpandInstruction(Inst, IDLoc, Out, STI);
+  switch (ExpandResult) {
+  case MER_NotAMacro:
+    Out.emitInstruction(Inst, *STI);
+    break;
+  case MER_Success:
+    break;
+  case MER_Fail:
+    return true;
+  }
+
+  return false;
+}
+
+LoongArchAsmParser::MacroExpanderResultTy
+LoongArchAsmParser::tryExpandInstruction(MCInst &Inst, SMLoc IDLoc, MCStreamer &Out,
+                                    const MCSubtargetInfo *STI) {
+  switch (Inst.getOpcode()) {
+  default:
+    return MER_NotAMacro;
+  //li.w $r12, $imm32
+  case LoongArch::LoadImm32:
+    return expandLoadImm(Inst, true, IDLoc, Out, STI) ? MER_Fail : MER_Success;
+  //li.d $r12, $imm64
+  case LoongArch::LoadImm64:
+    return expandLoadImm(Inst, false, IDLoc, Out, STI) ? MER_Fail : MER_Success;
+  //la.local $r12, symbol (pcrel)
+  case LoongArch::LoadAddrLocal:
+    assert(Inst.getOperand(0).isReg() && "expected register operand kind");
+    assert(Inst.getOperand(1).isExpr() && "expected immediate operand kind");
+
+    return expandLoadAddress(Inst.getOperand(0).getReg(),
+                                 Inst.getOperand(1), true, IDLoc,
+                                 Out, STI)
+               ? MER_Fail
+               : MER_Success;
+  //la.global $r12, symbol (got)
+  case LoongArch::LoadAddrGlobal:
+  case LoongArch::LoadAddrGlobal_Alias:
+    assert(Inst.getOperand(0).isReg() && "expected register operand kind");
+    assert(Inst.getOperand(1).isExpr() && "expected immediate operand kind");
+    return expandLoadAddress(Inst.getOperand(0).getReg(),
+                                 Inst.getOperand(1), false, IDLoc,
+                                 Out, STI)
+               ? MER_Fail
+               : MER_Success;
+  }
+}
+
+/// Can the value be represented by a unsigned N-bit value and a shift left?
+template <unsigned N> static bool isShiftedUIntAtAnyPosition(uint64_t x) {
+  unsigned BitNum = findFirstSet(x);
+
+  return (x == x >> BitNum << BitNum) && isUInt<N>(x >> BitNum);
+}
+
+bool LoongArchAsmParser::loadImmediate(int64_t ImmValue, unsigned DstReg,
+                                  unsigned SrcReg, bool Is32BitImm,
+                                  bool IsAddress, SMLoc IDLoc, MCStreamer &Out,
+                                  const MCSubtargetInfo *STI) {
+  LoongArchTargetStreamer &TOut = getTargetStreamer();
+  unsigned ZeroReg = LoongArch::ZERO_64;
+  SrcReg = ZeroReg;
+
+  if (!Is32BitImm && !is64Bit()) {
+    Error(IDLoc, "instruction requires a 64-bit architecture");
+    return true;
+  }
+
+  if (Is32BitImm) {
+    if (isInt<32>(ImmValue) || isUInt<32>(ImmValue)) {
+      // Sign extend up to 64-bit so that the predicates match the hardware
+      // behaviour. In particular, isInt<12>(0xfffff000) and similar should be
+      // true.
+      ImmValue = SignExtend64<32>(ImmValue);
+      if (((ImmValue>>12)&0xfffff) != 0) {
+        TOut.emitRI(LoongArch::LU12I_W, DstReg, (ImmValue>>12)&0xfffff, IDLoc, STI);
+        SrcReg = DstReg;
+      }
+      if ((ImmValue&0xfff) != 0)
+        TOut.emitRRI(LoongArch::ORI, DstReg, SrcReg, ImmValue&0xfff, IDLoc, STI);
+      return false;
+    } else {
+      Error(IDLoc, "instruction requires a 32-bit immediate");
+      return true;
+    }
+  }
+
+  if (((ImmValue>>12)&0xfffff) != 0) {
+    TOut.emitRI(LoongArch::LU12I_W, DstReg, (ImmValue>>12)&0xfffff, IDLoc, STI);
+    SrcReg = DstReg;
+  }
+  if ((ImmValue&0xfff) != 0) {
+    TOut.emitRRI(LoongArch::ORI, DstReg, SrcReg, ImmValue&0xfff, IDLoc, STI);
+    SrcReg = DstReg;
+  }
+  if (((ImmValue>>32)&0xfffff) != 0) {
+    TOut.emitRI(LoongArch::LU32I_D, DstReg, (ImmValue>>32)&0xfffff, IDLoc, STI);
+    SrcReg = DstReg;
+  }
+  if (((ImmValue>>52)&0xfff) != 0)
+    TOut.emitRRI(LoongArch::LU52I_D, DstReg, SrcReg, (ImmValue>>52)&0xfff, IDLoc, STI);
+
+  return false;
+}
+
+bool LoongArchAsmParser::expandLoadImm(MCInst &Inst, bool Is32BitImm, SMLoc IDLoc,
+                                  MCStreamer &Out, const MCSubtargetInfo *STI) {
+  const MCOperand &ImmOp = Inst.getOperand(1);
+  assert(ImmOp.isImm() && "expected immediate operand kind");
+  const MCOperand &DstRegOp = Inst.getOperand(0);
+  assert(DstRegOp.isReg() && "expected register operand kind");
+
+  if (loadImmediate(ImmOp.getImm(), DstRegOp.getReg(), LoongArch::NoRegister,
+                    Is32BitImm, false, IDLoc, Out, STI))
+    return true;
+
+  return false;
+}
+
+bool LoongArchAsmParser::expandLoadAddress(unsigned DstReg,
+                                      const MCOperand &Offset,
+                                      bool IsLocal, SMLoc IDLoc,
+                                      MCStreamer &Out,
+                                      const MCSubtargetInfo *STI) {
+  LoongArchTargetStreamer &TOut = getTargetStreamer();
+  const MCExpr *SymExpr = Offset.getExpr();
+  MCValue Res;
+
+
+  if (!SymExpr->evaluateAsRelocatable(Res, nullptr, nullptr)) {
+    Error(IDLoc, "expected relocatable expression");
+    return true;
+  }
+  if (Res.getSymB() != nullptr) {
+    Error(IDLoc, "expected relocatable expression with only one symbol");
+    return true;
+  }
+
+  if (IsLocal) {
+    // pcaddu12i $rd, %hi(sym)
+    // addi.d $rd, $rd, %lo(sym)
+    const LoongArchMCExpr *HiExpr =
+      LoongArchMCExpr::create(LoongArchMCExpr::MEK_PCREL_HI, SymExpr, getContext());
+    const LoongArchMCExpr *LoExpr =
+      LoongArchMCExpr::create(LoongArchMCExpr::MEK_PCREL_LO, SymExpr, getContext());
+
+    TOut.emitRX(LoongArch::PCADDU12I, DstReg,
+        MCOperand::createExpr(HiExpr), IDLoc, STI);
+    TOut.emitRRX(LoongArch::ADDI_D, DstReg, DstReg,
+        MCOperand::createExpr(LoExpr), IDLoc, STI);
+  } else {
+    // registe symbol "_GLOBAL_OFFSET_TABLE_"
+    StringRef SymName("_GLOBAL_OFFSET_TABLE_");
+    MCAssembler &MCA = (static_cast<MCELFStreamer &>(getStreamer())).getAssembler();
+    MCSymbol *Got = MCA.getContext().getOrCreateSymbol(SymName);
+    MCA.registerSymbol(*Got);
+
+    // pcaddu12i $rd, %got_hi(sym)
+    // ld.d $rd, $rd, %got_lo(sym)
+    const LoongArchMCExpr *GotHiExpr =
+      LoongArchMCExpr::create(LoongArchMCExpr::MEK_GOT_HI, SymExpr, getContext());
+    const LoongArchMCExpr *GotLoExpr =
+      LoongArchMCExpr::create(LoongArchMCExpr::MEK_GOT_LO, SymExpr, getContext());
+    const MCExpr *GotSym = MCSymbolRefExpr::create("_GLOBAL_OFFSET_TABLE_",
+                                MCSymbolRefExpr::VK_None, getContext());
+
+    TOut.emitRXX(LoongArch::PCADDU12I_rii, DstReg,
+        MCOperand::createExpr(GotHiExpr),
+        MCOperand::createExpr(GotSym), IDLoc, STI);
+    TOut.emitRRXX(LoongArch::LD_D_rrii, DstReg, DstReg,
+        MCOperand::createExpr(GotLoExpr),
+        MCOperand::createExpr(GotSym), IDLoc, STI);
+  }
+
+  return false;
+}
+
+unsigned LoongArchAsmParser::checkTargetMatchPredicate(MCInst &Inst) {
+  switch (Inst.getOpcode()) {
+  case LoongArch::BSTRINS_W:
+  case LoongArch::BSTRPICK_W: {
+    assert(Inst.getOperand(2).isImm() && Inst.getOperand(3).isImm() &&
+           "Operands must be immediates for bstrins.w/bstrpick.w!");
+    const signed Msbw = Inst.getOperand(2).getImm();
+    const signed Lsbw = Inst.getOperand(3).getImm();
+    if (Msbw < Lsbw)
+      return Match_MsbHigherThanLsb;
+    if ((Lsbw < 0) || (Msbw > 31))
+      return Match_RequiresRange0_31;
+    return Match_Success;
+  }
+  case LoongArch::BSTRINS_D:
+  case LoongArch::BSTRPICK_D: {
+    assert(Inst.getOperand(2).isImm() && Inst.getOperand(3).isImm() &&
+           "Operands must be immediates for bstrins.d/bstrpick.d!");
+    const signed Msbd = Inst.getOperand(2).getImm();
+    const signed Lsbd = Inst.getOperand(3).getImm();
+    if (Msbd < Lsbd)
+      return Match_MsbHigherThanLsb;
+    if ((Lsbd < 0) || (Msbd > 63))
+      return Match_RequiresRange0_63;
+    return Match_Success;
+  }
+  case LoongArch::CSRXCHG32:
+  case LoongArch::CSRXCHG:
+    if (Inst.getOperand(2).getReg() == LoongArch::ZERO ||
+        Inst.getOperand(2).getReg() == LoongArch::ZERO_64)
+      return Match_RequiresNoZeroRegister;
+    if (Inst.getOperand(2).getReg() == LoongArch::RA ||
+        Inst.getOperand(2).getReg() == LoongArch::RA_64)
+      return Match_RequiresNoRaRegister;
+    return Match_Success;
+  }
+
+  return Match_Success;
+}
+
+static SMLoc RefineErrorLoc(const SMLoc Loc, const OperandVector &Operands,
+                            uint64_t ErrorInfo) {
+  if (ErrorInfo != ~0ULL && ErrorInfo < Operands.size()) {
+    SMLoc ErrorLoc = Operands[ErrorInfo]->getStartLoc();
+    if (ErrorLoc == SMLoc())
+      return Loc;
+    return ErrorLoc;
+  }
+  return Loc;
+}
+
+bool LoongArchAsmParser::MatchAndEmitInstruction(SMLoc IDLoc, unsigned &Opcode,
+                                                 OperandVector &Operands,
+                                                 MCStreamer &Out,
+                                                 uint64_t &ErrorInfo,
+                                                 bool MatchingInlineAsm) {
+  MCInst Inst;
+  unsigned MatchResult =
+      MatchInstructionImpl(Operands, Inst, ErrorInfo, MatchingInlineAsm);
+  switch (MatchResult) {
+  case Match_Success:
+    if (processInstruction(Inst, IDLoc, Out, STI))
+      return true;
+    return false;
+  case Match_MissingFeature:
+    Error(IDLoc, "instruction requires a CPU feature not currently enabled");
+    return true;
+  case Match_InvalidOperand: {
+    SMLoc ErrorLoc = IDLoc;
+    if (ErrorInfo != ~0ULL) {
+      if (ErrorInfo >= Operands.size())
+        return Error(IDLoc, "too few operands for instruction");
+
+      ErrorLoc = Operands[ErrorInfo]->getStartLoc();
+      if (ErrorLoc == SMLoc())
+        ErrorLoc = IDLoc;
+    }
+
+    return Error(ErrorLoc, "invalid operand for instruction");
+  }
+  case Match_MnemonicFail:
+    return Error(IDLoc, "invalid instruction");
+  case Match_RequiresNoZeroRegister:
+    return Error(IDLoc, "invalid operand ($zero) for instruction");
+  case Match_RequiresNoRaRegister:
+    return Error(IDLoc, "invalid operand ($r1) for instruction");
+  case Match_RequiresSameSrcAndDst:
+    return Error(IDLoc, "source and destination must match");
+  case Match_InvalidImm0_3:
+    return Error(RefineErrorLoc(IDLoc, Operands, ErrorInfo),
+                 "immediate must be an integer in range [0, 3].");
+  case Match_InvalidImm0_7:
+    return Error(RefineErrorLoc(IDLoc, Operands, ErrorInfo),
+                 "immediate must be an integer in range [0, 7].");
+  case Match_InvalidImm0_31:
+    return Error(RefineErrorLoc(IDLoc, Operands, ErrorInfo),
+                 "immediate must be an integer in range [0, 31].");
+  case Match_InvalidImm0_63:
+    return Error(RefineErrorLoc(IDLoc, Operands, ErrorInfo),
+                 "immediate must be an integer in range [0, 63].");
+  case Match_InvalidImm0_4095:
+  case Match_UImm12_Relaxed:
+    return Error(RefineErrorLoc(IDLoc, Operands, ErrorInfo),
+                 "immediate must be an integer in range [0, 4095].");
+  case Match_InvalidImm0_32767:
+    return Error(RefineErrorLoc(IDLoc, Operands, ErrorInfo),
+                 "immediate must be an integer in range [0, 32767].");
+  case Match_UImm16_Relaxed:
+    return Error(RefineErrorLoc(IDLoc, Operands, ErrorInfo),
+                 "expected 16-bit unsigned immediate");
+  case Match_UImm20_0:
+    return Error(RefineErrorLoc(IDLoc, Operands, ErrorInfo),
+                 "expected 20-bit unsigned immediate");
+  case Match_UImm26_0:
+    return Error(RefineErrorLoc(IDLoc, Operands, ErrorInfo),
+                 "expected 26-bit unsigned immediate");
+  case Match_UImm32_Coerced:
+    return Error(RefineErrorLoc(IDLoc, Operands, ErrorInfo),
+                 "expected 32-bit immediate");
+  case Match_InvalidSImm2:
+    return Error(RefineErrorLoc(IDLoc, Operands, ErrorInfo),
+                 "expected 2-bit signed immediate");
+  case Match_InvalidSImm3:
+    return Error(RefineErrorLoc(IDLoc, Operands, ErrorInfo),
+                 "expected 3-bit signed immediate");
+  case Match_InvalidSImm5:
+    return Error(RefineErrorLoc(IDLoc, Operands, ErrorInfo),
+                 "expected 5-bit signed immediate");
+  case Match_InvalidSImm8:
+    return Error(RefineErrorLoc(IDLoc, Operands, ErrorInfo),
+                 "expected 8-bit signed immediate");
+  case Match_InvalidSImm12:
+  case Match_SImm12_Relaxed:
+    return Error(RefineErrorLoc(IDLoc, Operands, ErrorInfo),
+                 "expected 12-bit signed immediate");
+  case Match_InvalidSImm14:
+    return Error(RefineErrorLoc(IDLoc, Operands, ErrorInfo),
+                 "expected 14-bit signed immediate");
+  case Match_InvalidSImm15:
+    return Error(RefineErrorLoc(IDLoc, Operands, ErrorInfo),
+                 "expected 15-bit signed immediate");
+  case Match_InvalidSImm16:
+    return Error(RefineErrorLoc(IDLoc, Operands, ErrorInfo),
+                 "expected 16-bit signed immediate");
+  case Match_InvalidSImm20:
+    return Error(RefineErrorLoc(IDLoc, Operands, ErrorInfo),
+                 "expected 20-bit signed immediate");
+  case Match_InvalidSImm21:
+    return Error(RefineErrorLoc(IDLoc, Operands, ErrorInfo),
+                 "expected 21-bit signed immediate");
+  case Match_InvalidSImm26:
+    return Error(RefineErrorLoc(IDLoc, Operands, ErrorInfo),
+                 "expected 26-bit signed immediate");
+  case Match_SImm32:
+  case Match_SImm32_Relaxed:
+    return Error(RefineErrorLoc(IDLoc, Operands, ErrorInfo),
+                 "expected 32-bit signed immediate");
+  case Match_MemSImm14:
+    return Error(RefineErrorLoc(IDLoc, Operands, ErrorInfo),
+                 "expected memory with 14-bit signed offset");
+  case Match_MemSImmPtr:
+    return Error(RefineErrorLoc(IDLoc, Operands, ErrorInfo),
+                 "expected memory with 32-bit signed offset");
+  case Match_UImm2_1:
+    return Error(RefineErrorLoc(IDLoc, Operands, ErrorInfo),
+                 "expected immediate in range 1 .. 4");
+  case Match_MemSImm14Lsl2:
+    return Error(RefineErrorLoc(IDLoc, Operands, ErrorInfo),
+                 "expected memory with 16-bit signed offset and multiple of 4");
+  case Match_RequiresRange0_31: {
+    SMLoc ErrorStart = Operands[3]->getStartLoc();
+    SMLoc ErrorEnd = Operands[4]->getEndLoc();
+    return Error(ErrorStart, "from lsbw to msbw are not in the range 0 .. 31",
+                 SMRange(ErrorStart, ErrorEnd));
+    }
+  case Match_RequiresPosSizeUImm6: {
+    SMLoc ErrorStart = Operands[3]->getStartLoc();
+    SMLoc ErrorEnd = Operands[4]->getEndLoc();
+    return Error(ErrorStart, "size plus position are not in the range 1 .. 63",
+                 SMRange(ErrorStart, ErrorEnd));
+    }
+  case Match_RequiresRange0_63: {
+    SMLoc ErrorStart = Operands[3]->getStartLoc();
+    SMLoc ErrorEnd = Operands[4]->getEndLoc();
+    return Error(ErrorStart, "from lsbd to msbd are not in the range 0 .. 63",
+                 SMRange(ErrorStart, ErrorEnd));
+    }
+  case Match_MsbHigherThanLsb: {
+    SMLoc ErrorStart = Operands[3]->getStartLoc();
+    SMLoc ErrorEnd = Operands[4]->getEndLoc();
+    return Error(ErrorStart, "msb are not higher than lsb", SMRange(ErrorStart, ErrorEnd));
+    }
+  }
+
+  llvm_unreachable("Implement any new match types added!");
+}
+
+/*
+ * Note: The implementation of this function must be sync with the definition
+ * of GPR32/GPR64 RegisterClass in LoongArchRegisterInfo.td
+ */
+int LoongArchAsmParser::matchCPURegisterName(StringRef Name) {
+  int CC;
+
+  CC = StringSwitch<unsigned>(Name)
+           .Cases("zero", "r0", 0)
+           .Cases("a0", "v0", "r4", 1)
+           .Cases("a1", "v1", "r5", 2)
+           .Cases("a2", "r6", 3)
+           .Cases("a3", "r7", 4)
+           .Cases("a4", "r8", 5)
+           .Cases("a5", "r9", 6)
+           .Cases("a6", "r10", 7)
+           .Cases("a7", "r11", 8)
+           .Cases("t0", "r12", 9)
+           .Cases("t1", "r13", 10)
+           .Cases("t2", "r14", 11)
+           .Cases("t3", "r15", 12)
+           .Cases("t4", "r16", 13)
+           .Cases("t5", "r17", 14)
+           .Cases("t6", "r18", 15)
+           .Cases("t7", "r19", 16)
+           .Cases("t8", "r20", 17)
+           .Cases("s0", "r23", 18)
+           .Cases("s1", "r24", 19)
+           .Cases("s2", "r25", 20)
+           .Cases("s3", "r26", 21)
+           .Cases("s4", "r27", 22)
+           .Cases("s5", "r28", 23)
+           .Cases("s6", "r29", 24)
+           .Cases("s7", "r30", 25)
+           .Cases("s8", "r31", 26)
+           .Cases("ra", "r1", 27)
+           .Cases("tp", "r2", 28)
+           .Cases("sp", "r3", 29)
+           .Case("r21", 30)
+           .Cases("fp", "r22", 31)
+           .Default(-1);
+
+  return CC;
+}
+
+int LoongArchAsmParser::matchFPURegisterName(StringRef Name) {
+  if (Name[0] == 'f') {
+    int CC;
+
+    CC = StringSwitch<unsigned>(Name)
+             .Cases("f0", "fa0", "fv0", 0)
+             .Cases("f1", "fa1", "fv1", 1)
+             .Cases("f2", "fa2", 2)
+             .Cases("f3", "fa3", 3)
+             .Cases("f4", "fa4", 4)
+             .Cases("f5", "fa5", 5)
+             .Cases("f6", "fa6", 6)
+             .Cases("f7", "fa7", 7)
+             .Cases("f8", "ft0", 8)
+             .Cases("f9", "ft1", 9)
+             .Cases("f10", "ft2", 10)
+             .Cases("f11", "ft3", 11)
+             .Cases("f12", "ft4", 12)
+             .Cases("f13", "ft5", 13)
+             .Cases("f14", "ft6", 14)
+             .Cases("f15", "ft7", 15)
+             .Cases("f16", "ft8", 16)
+             .Cases("f17", "ft9", 17)
+             .Cases("f18", "ft10", 18)
+             .Cases("f19", "ft11", 19)
+             .Cases("f20", "ft12", 20)
+             .Cases("f21", "ft13", 21)
+             .Cases("f22", "ft14", 22)
+             .Cases("f23", "ft15", 23)
+             .Cases("f24", "fs0", 24)
+             .Cases("f25", "fs1", 25)
+             .Cases("f26", "fs2", 26)
+             .Cases("f27", "fs3", 27)
+             .Cases("f28", "fs4", 28)
+             .Cases("f29", "fs5", 29)
+             .Cases("f30", "fs6", 30)
+             .Cases("f31", "fs7", 31)
+             .Default(-1);
+
+    return CC;
+  }
+  return -1;
+}
+
+int LoongArchAsmParser::matchFCFRRegisterName(StringRef Name) {
+  if (Name.startswith("fcc")) {
+    StringRef NumString = Name.substr(3);
+    unsigned IntVal;
+    if (NumString.getAsInteger(10, IntVal))
+      return -1;    // This is not an integer.
+    if (IntVal > 7) // There are only 8 fcc registers.
+      return -1;
+    return IntVal;
+  }
+  return -1;
+}
+
+int LoongArchAsmParser::matchFCSRRegisterName(StringRef Name) {
+  if (Name.startswith("fcsr")) {
+    StringRef NumString = Name.substr(4);
+    unsigned IntVal;
+    if (NumString.getAsInteger(10, IntVal))
+      return -1;    // This is not an integer.
+    if (IntVal > 3) // There are only 4 fcsr registers.
+      return -1;
+    return IntVal;
+  }
+  return -1;
+}
+
+bool LoongArchAsmParser::parseOperand(OperandVector &Operands, StringRef Mnemonic) {
+  MCAsmParser &Parser = getParser();
+  LLVM_DEBUG(dbgs() << "parseOperand\n");
+
+  // Check if the current operand has a custom associated parser, if so, try to
+  // custom parse the operand, or fallback to the general approach.
+  OperandMatchResultTy ResTy = MatchOperandParserImpl(Operands, Mnemonic);
+  if (ResTy == MatchOperand_Success)
+    return false;
+  // If there wasn't a custom match, try the generic matcher below. Otherwise,
+  // there was a match, but an error occurred, in which case, just return that
+  // the operand parsing failed.
+  if (ResTy == MatchOperand_ParseFail)
+    return true;
+
+  LLVM_DEBUG(dbgs() << ".. Generic Parser\n");
+
+  switch (getLexer().getKind()) {
+  case AsmToken::Dollar: {
+    // Parse the register.
+    SMLoc S = Parser.getTok().getLoc();
+
+    // Almost all registers have been parsed by custom parsers. There is only
+    // one exception to this. $zero (and it's alias $0) will reach this point
+    // for div, divu, and similar instructions because it is not an operand
+    // to the instruction definition but an explicit register. Special case
+    // this situation for now.
+    if (parseAnyRegister(Operands) != MatchOperand_NoMatch)
+      return false;
+
+    // Maybe it is a symbol reference.
+    StringRef Identifier;
+    if (Parser.parseIdentifier(Identifier))
+      return true;
+
+    SMLoc E = SMLoc::getFromPointer(Parser.getTok().getLoc().getPointer() - 1);
+    MCSymbol *Sym = getContext().getOrCreateSymbol("$" + Identifier);
+    // Otherwise create a symbol reference.
+    const MCExpr *Res =
+        MCSymbolRefExpr::create(Sym, MCSymbolRefExpr::VK_None, getContext());
+
+    Operands.push_back(LoongArchOperand::CreateImm(Res, S, E, *this));
+    return false;
+  }
+  default: {
+    LLVM_DEBUG(dbgs() << ".. generic integer expression\n");
+
+    const MCExpr *Expr;
+    SMLoc S = Parser.getTok().getLoc(); // Start location of the operand.
+    if (getParser().parseExpression(Expr))
+      return true;
+
+    SMLoc E = SMLoc::getFromPointer(Parser.getTok().getLoc().getPointer() - 1);
+
+    Operands.push_back(LoongArchOperand::CreateImm(Expr, S, E, *this));
+    return false;
+  }
+  } // switch(getLexer().getKind())
+  return true;
+}
+
+bool LoongArchAsmParser::isEvaluated(const MCExpr *Expr) {
+  switch (Expr->getKind()) {
+  case MCExpr::Constant:
+    return true;
+  case MCExpr::SymbolRef:
+    return (cast<MCSymbolRefExpr>(Expr)->getKind() != MCSymbolRefExpr::VK_None);
+  case MCExpr::Binary: {
+    const MCBinaryExpr *BE = cast<MCBinaryExpr>(Expr);
+    if (!isEvaluated(BE->getLHS()))
+      return false;
+    return isEvaluated(BE->getRHS());
+  }
+  case MCExpr::Unary:
+    return isEvaluated(cast<MCUnaryExpr>(Expr)->getSubExpr());
+  case MCExpr::Target:
+    return true;
+  }
+  return false;
+}
+
+bool LoongArchAsmParser::ParseRegister(unsigned &RegNo, SMLoc &StartLoc,
+                                  SMLoc &EndLoc) {
+  return tryParseRegister(RegNo, StartLoc, EndLoc) != MatchOperand_Success;
+}
+
+OperandMatchResultTy LoongArchAsmParser::tryParseRegister(unsigned &RegNo,
+                                                          SMLoc &StartLoc,
+                                                          SMLoc &EndLoc) {
+  SmallVector<std::unique_ptr<MCParsedAsmOperand>, 1> Operands;
+  OperandMatchResultTy ResTy = parseAnyRegister(Operands);
+  if (ResTy == MatchOperand_Success) {
+    assert(Operands.size() == 1);
+    LoongArchOperand &Operand = static_cast<LoongArchOperand &>(*Operands.front());
+    StartLoc = Operand.getStartLoc();
+    EndLoc = Operand.getEndLoc();
+
+    // AFAIK, we only support numeric registers and named GPR's in CFI
+    // directives.
+    // Don't worry about eating tokens before failing. Using an unrecognised
+    // register is a parse error.
+    if (Operand.isGPRAsmReg()) {
+      // Resolve to GPR32 or GPR64 appropriately.
+      RegNo = is64Bit() ? Operand.getGPR64Reg() : Operand.getGPR32Reg();
+    }
+
+    return (RegNo == (unsigned)-1) ? MatchOperand_NoMatch
+                                   : MatchOperand_Success;
+  }
+
+  assert(Operands.size() == 0);
+  return (RegNo == (unsigned)-1) ? MatchOperand_NoMatch : MatchOperand_Success;
+}
+
+bool LoongArchAsmParser::parseMemOffset(const MCExpr *&Res) {
+  return getParser().parseExpression(Res);
+}
+
+OperandMatchResultTy
+LoongArchAsmParser::parseMemOperand(OperandVector &Operands) {
+  MCAsmParser &Parser = getParser();
+  LLVM_DEBUG(dbgs() << "parseMemOperand\n");
+  const MCExpr *IdVal = nullptr;
+  SMLoc S;
+  OperandMatchResultTy Res = MatchOperand_NoMatch;
+  // First operand is the base.
+  S = Parser.getTok().getLoc();
+
+  Res = parseAnyRegister(Operands);
+  if (Res != MatchOperand_Success)
+    return Res;
+
+  if (Parser.getTok().isNot(AsmToken::Comma)) {
+    Error(Parser.getTok().getLoc(), "',' expected");
+    return MatchOperand_ParseFail;
+  }
+
+  Parser.Lex(); // Eat the ',' token.
+
+  if (parseMemOffset(IdVal))
+    return MatchOperand_ParseFail;
+
+  SMLoc E = SMLoc::getFromPointer(Parser.getTok().getLoc().getPointer() - 1);
+
+  // Replace the register operand with the memory operand.
+  std::unique_ptr<LoongArchOperand> op(
+      static_cast<LoongArchOperand *>(Operands.back().release()));
+  // Remove the register from the operands.
+  // "op" will be managed by k_Memory.
+  Operands.pop_back();
+
+  // when symbol not defined, error report.
+  if (dyn_cast<MCSymbolRefExpr>(IdVal)) {
+    return MatchOperand_ParseFail;
+  }
+
+  // Add the memory operand.
+  if (dyn_cast<MCBinaryExpr>(IdVal)) {
+    int64_t Imm;
+    if (IdVal->evaluateAsAbsolute(Imm))
+      IdVal = MCConstantExpr::create(Imm, getContext());
+    else
+      return MatchOperand_ParseFail;
+  }
+
+  Operands.push_back(LoongArchOperand::CreateMem(std::move(op), IdVal, S, E, *this));
+  return MatchOperand_Success;
+}
+
+bool LoongArchAsmParser::searchSymbolAlias(OperandVector &Operands) {
+  MCAsmParser &Parser = getParser();
+  MCSymbol *Sym = getContext().lookupSymbol(Parser.getTok().getIdentifier());
+  if (!Sym)
+    return false;
+
+  SMLoc S = Parser.getTok().getLoc();
+  if (Sym->isVariable()) {
+    const MCExpr *Expr = Sym->getVariableValue();
+    if (Expr->getKind() == MCExpr::SymbolRef) {
+      const MCSymbolRefExpr *Ref = static_cast<const MCSymbolRefExpr *>(Expr);
+      StringRef DefSymbol = Ref->getSymbol().getName();
+      if (DefSymbol.startswith("$")) {
+        OperandMatchResultTy ResTy =
+            matchAnyRegisterNameWithoutDollar(Operands, DefSymbol.substr(1), S);
+        if (ResTy == MatchOperand_Success) {
+          Parser.Lex();
+          return true;
+        }
+        if (ResTy == MatchOperand_ParseFail)
+          llvm_unreachable("Should never ParseFail");
+      }
+    }
+  } else if (Sym->isUnset()) {
+    // If symbol is unset, it might be created in the `parseSetAssignment`
+    // routine as an alias for a numeric register name.
+    // Lookup in the aliases list.
+    auto Entry = RegisterSets.find(Sym->getName());
+    if (Entry != RegisterSets.end()) {
+      OperandMatchResultTy ResTy =
+          matchAnyRegisterWithoutDollar(Operands, Entry->getValue(), S);
+      if (ResTy == MatchOperand_Success) {
+        Parser.Lex();
+        return true;
+      }
+    }
+  }
+
+  return false;
+}
+
+OperandMatchResultTy
+LoongArchAsmParser::matchAnyRegisterNameWithoutDollar(OperandVector &Operands,
+                                                      StringRef Identifier,
+                                                      SMLoc S) {
+  int Index = matchCPURegisterName(Identifier);
+  if (Index != -1) {
+    Operands.push_back(LoongArchOperand::createGPRReg(
+        Index, Identifier, getContext().getRegisterInfo(), S,
+        getLexer().getLoc(), *this));
+    return MatchOperand_Success;
+  }
+
+  Index = matchFPURegisterName(Identifier);
+  if (Index != -1) {
+    Operands.push_back(LoongArchOperand::createFGRReg(
+        Index, Identifier, getContext().getRegisterInfo(), S,
+        getLexer().getLoc(), *this));
+    return MatchOperand_Success;
+  }
+
+  Index = matchFCFRRegisterName(Identifier);
+  if (Index != -1) {
+    Operands.push_back(LoongArchOperand::createFCFRReg(
+        Index, Identifier, getContext().getRegisterInfo(), S,
+        getLexer().getLoc(), *this));
+    return MatchOperand_Success;
+  }
+
+  Index = matchFCSRRegisterName(Identifier);
+  if (Index != -1) {
+    Operands.push_back(LoongArchOperand::createFCSRReg(
+        Index, Identifier, getContext().getRegisterInfo(), S,
+        getLexer().getLoc(), *this));
+    return MatchOperand_Success;
+  }
+
+  return MatchOperand_NoMatch;
+}
+
+OperandMatchResultTy
+LoongArchAsmParser::matchAnyRegisterWithoutDollar(OperandVector &Operands,
+                                                  const AsmToken &Token, SMLoc S) {
+  if (Token.is(AsmToken::Identifier)) {
+    LLVM_DEBUG(dbgs() << ".. identifier\n");
+    StringRef Identifier = Token.getIdentifier();
+    OperandMatchResultTy ResTy =
+        matchAnyRegisterNameWithoutDollar(Operands, Identifier, S);
+    return ResTy;
+  } else if (Token.is(AsmToken::Integer)) {
+    LLVM_DEBUG(dbgs() << ".. integer\n");
+    int64_t RegNum = Token.getIntVal();
+    if (RegNum < 0 || RegNum > 31) {
+      // Show the error, but treat invalid register
+      // number as a normal one to continue parsing
+      // and catch other possible errors.
+      Error(getLexer().getLoc(), "invalid register number");
+    }
+    Operands.push_back(LoongArchOperand::createNumericReg(
+        RegNum, Token.getString(), getContext().getRegisterInfo(), S,
+        Token.getLoc(), *this));
+    return MatchOperand_Success;
+  }
+
+  LLVM_DEBUG(dbgs() << Token.getKind() << "\n");
+
+  return MatchOperand_NoMatch;
+}
+
+OperandMatchResultTy
+LoongArchAsmParser::matchAnyRegisterWithoutDollar(OperandVector &Operands, SMLoc S) {
+  auto Token = getLexer().peekTok(false);
+  return matchAnyRegisterWithoutDollar(Operands, Token, S);
+}
+
+OperandMatchResultTy
+LoongArchAsmParser::parseAnyRegister(OperandVector &Operands) {
+  MCAsmParser &Parser = getParser();
+  LLVM_DEBUG(dbgs() << "parseAnyRegister\n");
+
+  auto Token = Parser.getTok();
+
+  SMLoc S = Token.getLoc();
+
+  if (Token.isNot(AsmToken::Dollar)) {
+    LLVM_DEBUG(dbgs() << ".. !$ -> try sym aliasing\n");
+    if (Token.is(AsmToken::Identifier)) {
+      if (searchSymbolAlias(Operands))
+        return MatchOperand_Success;
+    }
+    LLVM_DEBUG(dbgs() << ".. !symalias -> NoMatch\n");
+    return MatchOperand_NoMatch;
+  }
+  LLVM_DEBUG(dbgs() << ".. $\n");
+
+  OperandMatchResultTy ResTy = matchAnyRegisterWithoutDollar(Operands, S);
+  if (ResTy == MatchOperand_Success) {
+    Parser.Lex(); // $
+    Parser.Lex(); // identifier
+  }
+  return ResTy;
+}
+
+OperandMatchResultTy
+LoongArchAsmParser::parseJumpTarget(OperandVector &Operands) {
+  MCAsmParser &Parser = getParser();
+  LLVM_DEBUG(dbgs() << "parseJumpTarget\n");
+
+  SMLoc S = getLexer().getLoc();
+
+  // Registers are a valid target and have priority over symbols.
+  OperandMatchResultTy ResTy = parseAnyRegister(Operands);
+  if (ResTy != MatchOperand_NoMatch)
+    return ResTy;
+
+  // Integers and expressions are acceptable
+  const MCExpr *Expr = nullptr;
+  if (Parser.parseExpression(Expr)) {
+    // We have no way of knowing if a symbol was consumed so we must ParseFail
+    return MatchOperand_ParseFail;
+  }
+  Operands.push_back(
+      LoongArchOperand::CreateImm(Expr, S, getLexer().getLoc(), *this));
+  return MatchOperand_Success;
+}
+
+static std::string LoongArchMnemonicSpellCheck(StringRef S,
+                                               const FeatureBitset &FBS,
+                                               unsigned VariantID = 0);
+
+bool LoongArchAsmParser::ParseInstruction(ParseInstructionInfo &Info,
+                                          StringRef Name, SMLoc NameLoc,
+                                          OperandVector &Operands) {
+  MCAsmParser &Parser = getParser();
+  LLVM_DEBUG(dbgs() << "ParseInstruction\n");
+
+  // We have reached first instruction, module directive are now forbidden.
+  getTargetStreamer().forbidModuleDirective();
+
+  // Check if we have valid mnemonic
+  if (!mnemonicIsValid(Name)) {
+    FeatureBitset FBS = ComputeAvailableFeatures(getSTI().getFeatureBits());
+    std::string Suggestion = LoongArchMnemonicSpellCheck(Name, FBS);
+    return Error(NameLoc, "unknown instruction" + Suggestion);
+  }
+
+  // First operand in MCInst is instruction mnemonic.
+  Operands.push_back(LoongArchOperand::CreateToken(Name, NameLoc, *this));
+
+  // Read the remaining operands.
+  if (getLexer().isNot(AsmToken::EndOfStatement)) {
+    // Read the first operand.
+    if (parseOperand(Operands, Name)) {
+      SMLoc Loc = getLexer().getLoc();
+      return Error(Loc, "unexpected token in argument list");
+    }
+
+    while (getLexer().is(AsmToken::Comma)) {
+      Parser.Lex(); // Eat the comma.
+      // Parse and remember the operand.
+      if (parseOperand(Operands, Name)) {
+        SMLoc Loc = getLexer().getLoc();
+        return Error(Loc, "unexpected token in argument list");
+      }
+    }
+  }
+  if (getLexer().isNot(AsmToken::EndOfStatement)) {
+    SMLoc Loc = getLexer().getLoc();
+    return Error(Loc, "unexpected token in argument list");
+  }
+  Parser.Lex(); // Consume the EndOfStatement.
+  return false;
+}
+
+// FIXME: Given that these have the same name, these should both be
+// consistent on affecting the Parser.
+bool LoongArchAsmParser::reportParseError(Twine ErrorMsg) {
+  SMLoc Loc = getLexer().getLoc();
+  return Error(Loc, ErrorMsg);
+}
+
+bool LoongArchAsmParser::parseSetFpDirective() {
+  MCAsmParser &Parser = getParser();
+  LoongArchFPABIInfo::FpABIKind FpAbiVal;
+  // Line can be: .set fp=32
+  //              .set fp=64
+  Parser.Lex(); // Eat fp token
+  AsmToken Tok = Parser.getTok();
+  if (Tok.isNot(AsmToken::Equal)) {
+    reportParseError("unexpected token, expected equals sign '='");
+    return false;
+  }
+  Parser.Lex(); // Eat '=' token.
+  Tok = Parser.getTok();
+
+  if (!parseFpABIValue(FpAbiVal, ".set"))
+    return false;
+
+  if (getLexer().isNot(AsmToken::EndOfStatement)) {
+    reportParseError("unexpected token, expected end of statement");
+    return false;
+  }
+  getTargetStreamer().emitDirectiveSetFp(FpAbiVal);
+  Parser.Lex(); // Consume the EndOfStatement.
+  return false;
+}
+
+bool LoongArchAsmParser::parseSetSoftFloatDirective() {
+  MCAsmParser &Parser = getParser();
+  Parser.Lex();
+  if (getLexer().isNot(AsmToken::EndOfStatement))
+    return reportParseError("unexpected token, expected end of statement");
+
+  setFeatureBits(LoongArch::FeatureSoftFloat, "soft-float");
+  getTargetStreamer().emitDirectiveSetSoftFloat();
+  return false;
+}
+
+bool LoongArchAsmParser::parseSetHardFloatDirective() {
+  MCAsmParser &Parser = getParser();
+  Parser.Lex();
+  if (getLexer().isNot(AsmToken::EndOfStatement))
+    return reportParseError("unexpected token, expected end of statement");
+
+  clearFeatureBits(LoongArch::FeatureSoftFloat, "soft-float");
+  getTargetStreamer().emitDirectiveSetHardFloat();
+  return false;
+}
+
+bool LoongArchAsmParser::parseSetAssignment() {
+  StringRef Name;
+  const MCExpr *Value;
+  MCAsmParser &Parser = getParser();
+
+  if (Parser.parseIdentifier(Name))
+    return reportParseError("expected identifier after .set");
+
+  if (getLexer().isNot(AsmToken::Comma))
+    return reportParseError("unexpected token, expected comma");
+  Lex(); // Eat comma
+
+  if (!Parser.parseExpression(Value)) {
+    // Parse assignment of an expression including
+    // symbolic registers:
+    //   .set  $tmp, $BB0-$BB1
+    //   .set  r2, $f2
+    MCSymbol *Sym = getContext().getOrCreateSymbol(Name);
+    Sym->setVariableValue(Value);
+  } else {
+    return reportParseError("expected valid expression after comma");
+  }
+
+  return false;
+}
+
+bool LoongArchAsmParser::parseDirectiveSet() {
+  const AsmToken &Tok = getParser().getTok();
+  StringRef IdVal = Tok.getString();
+  SMLoc Loc = Tok.getLoc();
+
+  if (IdVal == "bopt") {
+    Warning(Loc, "'bopt' feature is unsupported");
+    getParser().Lex();
+    return false;
+  }
+  if (IdVal == "nobopt") {
+    // We're already running in nobopt mode, so nothing to do.
+    getParser().Lex();
+    return false;
+  }
+  if (IdVal == "fp")
+    return parseSetFpDirective();
+  if (IdVal == "softfloat")
+    return parseSetSoftFloatDirective();
+  if (IdVal == "hardfloat")
+    return parseSetHardFloatDirective();
+
+  // It is just an identifier, look for an assignment.
+  return parseSetAssignment();
+}
+
+/// parseDirectiveModule
+///  ::= .module fp=value
+///  ::= .module softfloat
+///  ::= .module hardfloat
+bool LoongArchAsmParser::parseDirectiveModule() {
+  MCAsmParser &Parser = getParser();
+  MCAsmLexer &Lexer = getLexer();
+  SMLoc L = Lexer.getLoc();
+
+  if (!getTargetStreamer().isModuleDirectiveAllowed()) {
+    // TODO : get a better message.
+    reportParseError(".module directive must appear before any code");
+    return false;
+  }
+
+  StringRef Option;
+  if (Parser.parseIdentifier(Option)) {
+    reportParseError("expected .module option identifier");
+    return false;
+  }
+
+  if (Option == "fp") {
+    return parseDirectiveModuleFP();
+  } else if (Option == "softfloat") {
+    setModuleFeatureBits(LoongArch::FeatureSoftFloat, "soft-float");
+
+    // Synchronize the ABI Flags information with the FeatureBits information we
+    // updated above.
+    getTargetStreamer().updateABIInfo(*this);
+
+    // If printing assembly, use the recently updated ABI Flags information.
+    // If generating ELF, don't do anything.
+    getTargetStreamer().emitDirectiveModuleSoftFloat();
+
+    // If this is not the end of the statement, report an error.
+    if (getLexer().isNot(AsmToken::EndOfStatement)) {
+      reportParseError("unexpected token, expected end of statement");
+      return false;
+    }
+
+    return false; // parseDirectiveModule has finished successfully.
+  } else if (Option == "hardfloat") {
+    clearModuleFeatureBits(LoongArch::FeatureSoftFloat, "soft-float");
+
+    // Synchronize the ABI Flags information with the FeatureBits information we
+    // updated above.
+    getTargetStreamer().updateABIInfo(*this);
+
+    // If printing assembly, use the recently updated ABI Flags information.
+    // If generating ELF, don't do anything.
+    getTargetStreamer().emitDirectiveModuleHardFloat();
+
+    // If this is not the end of the statement, report an error.
+    if (getLexer().isNot(AsmToken::EndOfStatement)) {
+      reportParseError("unexpected token, expected end of statement");
+      return false;
+    }
+
+    return false; // parseDirectiveModule has finished successfully.
+  }  else {
+    return Error(L, "'" + Twine(Option) + "' is not a valid .module option.");
+  }
+}
+
+/// parseDirectiveModuleFP
+///  ::= =32
+///  ::= =64
+bool LoongArchAsmParser::parseDirectiveModuleFP() {
+  MCAsmParser &Parser = getParser();
+  MCAsmLexer &Lexer = getLexer();
+
+  if (Lexer.isNot(AsmToken::Equal)) {
+    reportParseError("unexpected token, expected equals sign '='");
+    return false;
+  }
+  Parser.Lex(); // Eat '=' token.
+
+  LoongArchFPABIInfo::FpABIKind FpABI;
+  if (!parseFpABIValue(FpABI, ".module"))
+    return false;
+
+  if (getLexer().isNot(AsmToken::EndOfStatement)) {
+    reportParseError("unexpected token, expected end of statement");
+    return false;
+  }
+
+  // Synchronize the abi information with the FeatureBits information we
+  // changed above.
+  getTargetStreamer().updateABIInfo(*this);
+
+  // If printing assembly, use the recently updated abiflags information.
+  // If generating ELF, don't do anything.
+  getTargetStreamer().emitDirectiveModuleFP();
+
+  Parser.Lex(); // Consume the EndOfStatement.
+  return false;
+}
+
+bool LoongArchAsmParser::parseFpABIValue(LoongArchFPABIInfo::FpABIKind &FpABI,
+                                    StringRef Directive) {
+  MCAsmParser &Parser = getParser();
+  MCAsmLexer &Lexer = getLexer();
+  bool ModuleLevelOptions = Directive == ".module";
+
+  if (Lexer.is(AsmToken::Identifier)) {
+    Parser.Lex();
+
+    if (!isABI_LP32()) {
+      reportParseError("'" + Directive + " fp=32' requires the LP32 ABI");
+      return false;
+    }
+
+    return true;
+  }
+
+  if (Lexer.is(AsmToken::Integer)) {
+    unsigned Value = Parser.getTok().getIntVal();
+    Parser.Lex();
+
+    if (Value != 32 && Value != 64) {
+      reportParseError("unsupported value, expected '32' or '64'");
+      return false;
+    }
+
+    if (Value == 32) {
+      if (!isABI_LP32()) {
+        reportParseError("'" + Directive + " fp=32' requires the LP32 ABI");
+        return false;
+      }
+
+      FpABI = LoongArchFPABIInfo::FpABIKind::S32;
+      if (ModuleLevelOptions) {
+        clearModuleFeatureBits(LoongArch::FeatureFP64Bit, "fp64");
+      } else {
+        clearFeatureBits(LoongArch::FeatureFP64Bit, "fp64");
+      }
+    } else {
+      FpABI = LoongArchFPABIInfo::FpABIKind::S64;
+      if (ModuleLevelOptions) {
+        setModuleFeatureBits(LoongArch::FeatureFP64Bit, "fp64");
+      } else {
+        setFeatureBits(LoongArch::FeatureFP64Bit, "fp64");
+      }
+    }
+
+    return true;
+  }
+
+  return false;
+}
+
+bool LoongArchAsmParser::ParseDirective(AsmToken DirectiveID) {
+  // This returns false if this function recognizes the directive
+  // regardless of whether it is successfully handles or reports an
+  // error. Otherwise it returns true to give the generic parser a
+  // chance at recognizing it.
+
+  MCAsmParser &Parser = getParser();
+  StringRef IDVal = DirectiveID.getString();
+
+  if (IDVal == ".end") {
+      while (getLexer().isNot(AsmToken::Eof))
+        Parser.Lex();
+    return false;
+  }
+
+  if (IDVal == ".set") {
+    parseDirectiveSet();
+    return false;
+  }
+
+  if (IDVal == ".module") {
+    parseDirectiveModule();
+    return false;
+  }
+  if (IDVal == ".llvm_internal_loongarch_reallow_module_directive") {
+    parseInternalDirectiveReallowModule();
+    return false;
+  }
+
+  return true;
+}
+
+bool LoongArchAsmParser::parseInternalDirectiveReallowModule() {
+  // If this is not the end of the statement, report an error.
+  if (getLexer().isNot(AsmToken::EndOfStatement)) {
+    reportParseError("unexpected token, expected end of statement");
+    return false;
+  }
+
+  getTargetStreamer().reallowModuleDirective();
+
+  getParser().Lex(); // Eat EndOfStatement token.
+  return false;
+}
+
+extern "C" LLVM_EXTERNAL_VISIBILITY void LLVMInitializeLoongArchAsmParser() {
+  RegisterMCAsmParser<LoongArchAsmParser> X(getTheLoongArch32Target());
+  RegisterMCAsmParser<LoongArchAsmParser> A(getTheLoongArch64Target());
+}
+
+#define GET_REGISTER_MATCHER
+#define GET_MATCHER_IMPLEMENTATION
+#define GET_MNEMONIC_SPELL_CHECKER
+#include "LoongArchGenAsmMatcher.inc"
+
+bool LoongArchAsmParser::mnemonicIsValid(StringRef Mnemonic) {
+  // Find the appropriate table for this asm variant.
+  const MatchEntry *Start, *End;
+  Start = std::begin(MatchTable0);
+  End = std::end(MatchTable0);
+
+  // Search the table.
+  auto MnemonicRange = std::equal_range(Start, End, Mnemonic, LessOpcode());
+  return MnemonicRange.first != MnemonicRange.second;
+}
diff --git a/llvm/lib/Target/LoongArch/CMakeLists.txt b/llvm/lib/Target/LoongArch/CMakeLists.txt
new file mode 100644
index 000000000000..95e9e80f1bf0
--- /dev/null
+++ b/llvm/lib/Target/LoongArch/CMakeLists.txt
@@ -0,0 +1,37 @@
+set(LLVM_TARGET_DEFINITIONS LoongArch.td)
+
+tablegen(LLVM LoongArchGenAsmMatcher.inc -gen-asm-matcher)
+tablegen(LLVM LoongArchGenAsmWriter.inc -gen-asm-writer)
+tablegen(LLVM LoongArchGenCallingConv.inc -gen-callingconv)
+tablegen(LLVM LoongArchGenDAGISel.inc -gen-dag-isel)
+tablegen(LLVM LoongArchGenDisassemblerTables.inc -gen-disassembler)
+tablegen(LLVM LoongArchGenInstrInfo.inc -gen-instr-info)
+tablegen(LLVM LoongArchGenMCCodeEmitter.inc -gen-emitter)
+tablegen(LLVM LoongArchGenMCPseudoLowering.inc -gen-pseudo-lowering)
+tablegen(LLVM LoongArchGenRegisterInfo.inc -gen-register-info)
+tablegen(LLVM LoongArchGenSubtargetInfo.inc -gen-subtarget)
+
+add_public_tablegen_target(LoongArchCommonTableGen)
+
+add_llvm_target(LoongArchCodeGen
+  LoongArchAnalyzeImmediate.cpp
+  LoongArchAsmPrinter.cpp
+  LoongArchCCState.cpp
+  LoongArchExpandPseudo.cpp
+  LoongArchInstrInfo.cpp
+  LoongArchISelDAGToDAG.cpp
+  LoongArchISelLowering.cpp
+  LoongArchFrameLowering.cpp
+  LoongArchMCInstLower.cpp
+  LoongArchMachineFunction.cpp
+  LoongArchModuleISelDAGToDAG.cpp
+  LoongArchRegisterInfo.cpp
+  LoongArchSubtarget.cpp
+  LoongArchTargetMachine.cpp
+  LoongArchTargetObjectFile.cpp
+  )
+
+add_subdirectory(AsmParser)
+add_subdirectory(Disassembler)
+add_subdirectory(MCTargetDesc)
+add_subdirectory(TargetInfo)
diff --git a/llvm/lib/Target/LoongArch/Disassembler/CMakeLists.txt b/llvm/lib/Target/LoongArch/Disassembler/CMakeLists.txt
new file mode 100644
index 000000000000..305c0abe07ed
--- /dev/null
+++ b/llvm/lib/Target/LoongArch/Disassembler/CMakeLists.txt
@@ -0,0 +1,3 @@
+add_llvm_component_library(LLVMLoongArchDisassembler
+  LoongArchDisassembler.cpp
+  )
diff --git a/llvm/lib/Target/LoongArch/Disassembler/LLVMBuild.txt b/llvm/lib/Target/LoongArch/Disassembler/LLVMBuild.txt
new file mode 100644
index 000000000000..ebdd9686b42d
--- /dev/null
+++ b/llvm/lib/Target/LoongArch/Disassembler/LLVMBuild.txt
@@ -0,0 +1,22 @@
+;===- ./lib/Target/LoongArch/Disassembler/LLVMBuild.txt -------------*- Conf -*--===;
+;
+; Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+; See https://llvm.org/LICENSE.txt for license information.
+; SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+;
+;===------------------------------------------------------------------------===;
+;
+; This is an LLVMBuild description file for the components in this subdirectory.
+;
+; For more information on the LLVMBuild system, please see:
+;
+;   http://llvm.org/docs/LLVMBuild.html
+;
+;===------------------------------------------------------------------------===;
+
+[component_0]
+type = Library
+name = LoongArchDisassembler
+parent = LoongArch
+required_libraries = MCDisassembler LoongArchInfo Support
+add_to_library_groups = LoongArch
diff --git a/llvm/lib/Target/LoongArch/Disassembler/LoongArchDisassembler.cpp b/llvm/lib/Target/LoongArch/Disassembler/LoongArchDisassembler.cpp
new file mode 100644
index 000000000000..690429f4c3b0
--- /dev/null
+++ b/llvm/lib/Target/LoongArch/Disassembler/LoongArchDisassembler.cpp
@@ -0,0 +1,403 @@
+//===- LoongArchDisassembler.cpp - Disassembler for LoongArch -----------------------===//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+//
+// This file is part of the LoongArch Disassembler.
+//
+//===----------------------------------------------------------------------===//
+
+#include "MCTargetDesc/LoongArchMCTargetDesc.h"
+#include "LoongArch.h"
+#include "llvm/ADT/ArrayRef.h"
+#include "llvm/MC/MCContext.h"
+#include "llvm/MC/MCDisassembler/MCDisassembler.h"
+#include "llvm/MC/MCFixedLenDisassembler.h"
+#include "llvm/MC/MCInst.h"
+#include "llvm/MC/MCRegisterInfo.h"
+#include "llvm/MC/MCSubtargetInfo.h"
+#include "llvm/Support/Compiler.h"
+#include "llvm/Support/Debug.h"
+#include "llvm/Support/ErrorHandling.h"
+#include "llvm/Support/MathExtras.h"
+#include "llvm/Support/TargetRegistry.h"
+#include "llvm/Support/raw_ostream.h"
+#include <cassert>
+#include <cstdint>
+
+using namespace llvm;
+
+#define DEBUG_TYPE "loongarch-disassembler"
+
+using DecodeStatus = MCDisassembler::DecodeStatus;
+
+namespace {
+
+class LoongArchDisassembler : public MCDisassembler {
+
+public:
+  LoongArchDisassembler(const MCSubtargetInfo &STI, MCContext &Ctx)
+      : MCDisassembler(STI, Ctx) {}
+
+  bool isFP64() const { return STI.getFeatureBits()[LoongArch::FeatureFP64Bit]; }
+
+  bool is64Bit() const { return STI.getFeatureBits()[LoongArch::Feature64Bit]; }
+
+  DecodeStatus getInstruction(MCInst &Instr, uint64_t &Size,
+                              ArrayRef<uint8_t> Bytes, uint64_t Address,
+                              raw_ostream &CStream) const override;
+};
+
+} // end anonymous namespace
+
+// Forward declare these because the autogenerated code will reference them.
+// Definitions are further down.
+static DecodeStatus DecodeGPR64RegisterClass(MCInst &Inst,
+                                             unsigned RegNo,
+                                             uint64_t Address,
+                                             const void *Decoder);
+
+static DecodeStatus DecodeGPR32RegisterClass(MCInst &Inst,
+                                             unsigned RegNo,
+                                             uint64_t Address,
+                                             const void *Decoder);
+
+static DecodeStatus DecodePtrRegisterClass(MCInst &Inst,
+                                           unsigned Insn,
+                                           uint64_t Address,
+                                           const void *Decoder);
+
+static DecodeStatus DecodeFGR64RegisterClass(MCInst &Inst,
+                                             unsigned RegNo,
+                                             uint64_t Address,
+                                             const void *Decoder);
+
+static DecodeStatus DecodeFGR32RegisterClass(MCInst &Inst,
+                                             unsigned RegNo,
+                                             uint64_t Address,
+                                             const void *Decoder);
+
+static DecodeStatus DecodeFCSRRegisterClass(MCInst &Inst,
+                                            unsigned RegNo,
+                                            uint64_t Address,
+                                            const void *Decoder);
+
+static DecodeStatus DecodeFCFRRegisterClass(MCInst &Inst,
+                                            unsigned RegNo,
+                                            uint64_t Address,
+                                            const void *Decoder);
+
+static DecodeStatus DecodeBranchTarget(MCInst &Inst,
+                                       unsigned Offset,
+                                       uint64_t Address,
+                                       const void *Decoder);
+
+static DecodeStatus DecodeJumpTarget(MCInst &Inst,
+                                     unsigned Insn,
+                                     uint64_t Address,
+                                     const void *Decoder);
+
+static DecodeStatus DecodeMem(MCInst &Inst,
+                              unsigned Insn,
+                              uint64_t Address,
+                              const void *Decoder);
+
+static DecodeStatus DecodeMemSimm14(MCInst &Inst,
+                                    unsigned Insn,
+                                    uint64_t Address,
+                                    const void *Decoder);
+
+static DecodeStatus DecodeFMem(MCInst &Inst, unsigned Insn,
+                               uint64_t Address,
+                               const void *Decoder);
+
+template <unsigned Bits, int Offset, int Scale>
+static DecodeStatus DecodeUImmWithOffsetAndScale(MCInst &Inst, unsigned Value,
+                                                 uint64_t Address,
+                                                 const void *Decoder);
+
+template <unsigned Bits, int Offset>
+static DecodeStatus DecodeUImmWithOffset(MCInst &Inst, unsigned Value,
+                                         uint64_t Address,
+                                         const void *Decoder) {
+  return DecodeUImmWithOffsetAndScale<Bits, Offset, 1>(Inst, Value, Address,
+                                                       Decoder);
+}
+
+template <unsigned Bits, int Offset = 0, int ScaleBy = 1>
+static DecodeStatus DecodeSImmWithOffsetAndScale(MCInst &Inst, unsigned Value,
+                                                 uint64_t Address,
+                                                 const void *Decoder);
+
+namespace llvm {
+
+Target &getTheLoongArch32Target();
+Target &getTheLoongArch64Target();
+
+} // end namespace llvm
+
+static MCDisassembler *createLoongArchDisassembler(
+                       const Target &T,
+                       const MCSubtargetInfo &STI,
+                       MCContext &Ctx) {
+  return new LoongArchDisassembler(STI, Ctx);
+}
+
+extern "C" LLVM_EXTERNAL_VISIBILITY void LLVMInitializeLoongArchDisassembler() {
+  // Register the disassembler.
+  TargetRegistry::RegisterMCDisassembler(getTheLoongArch32Target(),
+                                         createLoongArchDisassembler);
+  TargetRegistry::RegisterMCDisassembler(getTheLoongArch64Target(),
+                                         createLoongArchDisassembler);
+}
+
+#include "LoongArchGenDisassemblerTables.inc"
+
+static unsigned getReg(const void *D, unsigned RC, unsigned RegNo) {
+  const LoongArchDisassembler *Dis = static_cast<const LoongArchDisassembler*>(D);
+  const MCRegisterInfo *RegInfo = Dis->getContext().getRegisterInfo();
+  if (RC == LoongArch::GPR64RegClassID || RC == LoongArch::GPR32RegClassID) {
+    // sync with the GPR32/GPR64 RegisterClass in LoongArchRegisterInfo.td
+    // that just like LoongArchAsmParser.cpp and LoongArchISelLowering.cpp
+    unsigned char indexes[] = { 0, 27, 28, 29, 1, 2, 3, 4,
+                                5, 6, 7, 8, 9, 10, 11, 12,
+                                13, 14, 15, 16, 17, 30, 31, 18,
+                                19, 20, 21, 22, 23, 24, 25, 26
+                              };
+    assert(RegNo < sizeof(indexes));
+    return *(RegInfo->getRegClass(RC).begin() + indexes[RegNo]);
+  }
+  return *(RegInfo->getRegClass(RC).begin() + RegNo);
+}
+
+/// Read four bytes from the ArrayRef and return 32 bit word.
+static DecodeStatus readInstruction32(ArrayRef<uint8_t> Bytes, uint64_t Address,
+                                      uint64_t &Size, uint32_t &Insn) {
+  // We want to read exactly 4 Bytes of data.
+  if (Bytes.size() < 4) {
+    Size = 0;
+    return MCDisassembler::Fail;
+  }
+
+  Insn = (Bytes[0] << 0) | (Bytes[1] << 8) | (Bytes[2] << 16) |
+         (Bytes[3] << 24);
+
+  return MCDisassembler::Success;
+}
+
+DecodeStatus LoongArchDisassembler::getInstruction(MCInst &Instr, uint64_t &Size,
+                                                   ArrayRef<uint8_t> Bytes,
+                                                   uint64_t Address,
+                                                   raw_ostream &CStream) const {
+  uint32_t Insn;
+  DecodeStatus Result;
+  Size = 0;
+
+  // Attempt to read the instruction so that we can attempt to decode it. If
+  // the buffer is not 4 bytes long, let the higher level logic figure out
+  // what to do with a size of zero and MCDisassembler::Fail.
+  Result = readInstruction32(Bytes, Address, Size, Insn);
+  if (Result == MCDisassembler::Fail)
+    return MCDisassembler::Fail;
+
+  // The only instruction size for standard encoded LoongArch.
+  Size = 4;
+
+  if (is64Bit()) {
+    LLVM_DEBUG(dbgs() << "Trying LoongArch (GPR64) table (32-bit opcodes):\n");
+    Result = decodeInstruction(DecoderTableLoongArch32, Instr, Insn,
+                               Address, this, STI);
+    if (Result != MCDisassembler::Fail)
+      return Result;
+  }
+
+  LLVM_DEBUG(dbgs() << "Trying LoongArch32 (GPR32) table (32-bit opcodes):\n");
+  Result = decodeInstruction(DecoderTableLoongArch3232, Instr, Insn,
+                             Address, this, STI);
+  if (Result != MCDisassembler::Fail)
+    return Result;
+
+  return MCDisassembler::Fail;
+}
+
+static DecodeStatus DecodeGPR64RegisterClass(MCInst &Inst,
+                                             unsigned RegNo,
+                                             uint64_t Address,
+                                             const void *Decoder) {
+  if (RegNo > 31)
+    return MCDisassembler::Fail;
+
+  unsigned Reg = getReg(Decoder, LoongArch::GPR64RegClassID, RegNo);
+  Inst.addOperand(MCOperand::createReg(Reg));
+  return MCDisassembler::Success;
+}
+
+static DecodeStatus DecodeGPR32RegisterClass(MCInst &Inst,
+                                             unsigned RegNo,
+                                             uint64_t Address,
+                                             const void *Decoder) {
+  if (RegNo > 31)
+    return MCDisassembler::Fail;
+  unsigned Reg = getReg(Decoder, LoongArch::GPR32RegClassID, RegNo);
+  Inst.addOperand(MCOperand::createReg(Reg));
+  return MCDisassembler::Success;
+}
+
+static DecodeStatus DecodePtrRegisterClass(MCInst &Inst,
+                                           unsigned RegNo,
+                                           uint64_t Address,
+                                           const void *Decoder) {
+  if (static_cast<const LoongArchDisassembler *>(Decoder)->is64Bit())
+    return DecodeGPR64RegisterClass(Inst, RegNo, Address, Decoder);
+
+  return DecodeGPR32RegisterClass(Inst, RegNo, Address, Decoder);
+}
+
+static DecodeStatus DecodeFGR64RegisterClass(MCInst &Inst,
+                                             unsigned RegNo,
+                                             uint64_t Address,
+                                             const void *Decoder) {
+  if (RegNo > 31)
+    return MCDisassembler::Fail;
+  unsigned Reg = getReg(Decoder, LoongArch::FGR64RegClassID, RegNo);
+  Inst.addOperand(MCOperand::createReg(Reg));
+  return MCDisassembler::Success;
+}
+
+static DecodeStatus DecodeFGR32RegisterClass(MCInst &Inst,
+                                             unsigned RegNo,
+                                             uint64_t Address,
+                                             const void *Decoder) {
+  if (RegNo > 31)
+    return MCDisassembler::Fail;
+
+  unsigned Reg = getReg(Decoder, LoongArch::FGR32RegClassID, RegNo);
+  Inst.addOperand(MCOperand::createReg(Reg));
+  return MCDisassembler::Success;
+}
+
+static DecodeStatus DecodeFCSRRegisterClass(MCInst &Inst,
+                                            unsigned RegNo,
+                                            uint64_t Address,
+                                            const void *Decoder) {
+  if (RegNo > 31)
+    return MCDisassembler::Fail;
+
+  unsigned Reg = getReg(Decoder, LoongArch::FCSRRegClassID, RegNo);
+  Inst.addOperand(MCOperand::createReg(Reg));
+  return MCDisassembler::Success;
+}
+
+static DecodeStatus DecodeFCFRRegisterClass(MCInst &Inst,
+                                            unsigned RegNo,
+                                            uint64_t Address,
+                                            const void *Decoder) {
+  if (RegNo > 7)
+    return MCDisassembler::Fail;
+
+  unsigned Reg = getReg(Decoder, LoongArch::FCFRRegClassID, RegNo);
+  Inst.addOperand(MCOperand::createReg(Reg));
+
+  return MCDisassembler::Success;
+}
+
+static DecodeStatus DecodeMem(MCInst &Inst,
+                              unsigned Insn,
+                              uint64_t Address,
+                              const void *Decoder) {
+  int Offset = SignExtend32<12>((Insn >> 10) & 0xfff);
+  unsigned Reg = fieldFromInstruction(Insn, 0, 5);
+  unsigned Base = fieldFromInstruction(Insn, 5, 5);
+
+  Reg = getReg(Decoder, LoongArch::GPR32RegClassID, Reg);
+  Base = getReg(Decoder, LoongArch::GPR32RegClassID, Base);
+
+  if (Inst.getOpcode() == LoongArch::SC_W ||
+      Inst.getOpcode() == LoongArch::SC_D)
+    Inst.addOperand(MCOperand::createReg(Reg));
+
+  Inst.addOperand(MCOperand::createReg(Reg));
+  Inst.addOperand(MCOperand::createReg(Base));
+  Inst.addOperand(MCOperand::createImm(Offset));
+
+  return MCDisassembler::Success;
+}
+
+static DecodeStatus DecodeMemSimm14(MCInst &Inst,
+                                    unsigned Insn,
+                                    uint64_t Address,
+                                    const void *Decoder) {
+  int Offset = SignExtend32<12>((Insn >> 10) & 0x3fff);
+  unsigned Reg = fieldFromInstruction(Insn, 0, 5);
+  unsigned Base = fieldFromInstruction(Insn, 5, 5);
+
+  Reg = getReg(Decoder, LoongArch::GPR32RegClassID, Reg);
+  Base = getReg(Decoder, LoongArch::GPR32RegClassID, Base);
+
+  if (Inst.getOpcode() == LoongArch::SC_W ||
+      Inst.getOpcode() == LoongArch::SC_D)
+    Inst.addOperand(MCOperand::createReg(Reg));
+
+  Inst.addOperand(MCOperand::createReg(Reg));
+  Inst.addOperand(MCOperand::createReg(Base));
+  Inst.addOperand(MCOperand::createImm(Offset));
+
+  return MCDisassembler::Success;
+}
+
+static DecodeStatus DecodeFMem(MCInst &Inst,
+                               unsigned Insn,
+                               uint64_t Address,
+                               const void *Decoder) {
+  int Offset = SignExtend32<12>((Insn >> 10) & 0xffff);
+  unsigned Reg = fieldFromInstruction(Insn, 0, 5);
+  unsigned Base = fieldFromInstruction(Insn, 5, 5);
+  Reg = getReg(Decoder, LoongArch::FGR64RegClassID, Reg);
+  Base = getReg(Decoder, LoongArch::GPR32RegClassID, Base);
+
+  Inst.addOperand(MCOperand::createReg(Reg));
+  Inst.addOperand(MCOperand::createReg(Base));
+  Inst.addOperand(MCOperand::createImm(Offset));
+
+  return MCDisassembler::Success;
+}
+
+static DecodeStatus DecodeBranchTarget(MCInst &Inst,
+                                       unsigned Offset,
+                                       uint64_t Address,
+                                       const void *Decoder) {
+  int32_t BranchOffset = (SignExtend32<16>(Offset) * 4) + 4;
+  Inst.addOperand(MCOperand::createImm(BranchOffset));
+  return MCDisassembler::Success;
+}
+
+static DecodeStatus DecodeJumpTarget(MCInst &Inst,
+                                     unsigned Insn,
+                                     uint64_t Address,
+                                     const void *Decoder) {
+  unsigned JumpOffset = fieldFromInstruction(Insn, 0, 26) << 2;
+  Inst.addOperand(MCOperand::createImm(JumpOffset));
+  return MCDisassembler::Success;
+}
+
+template <unsigned Bits, int Offset, int Scale>
+static DecodeStatus DecodeUImmWithOffsetAndScale(MCInst &Inst, unsigned Value,
+                                                 uint64_t Address,
+                                                 const void *Decoder) {
+  Value &= ((1 << Bits) - 1);
+  Value *= Scale;
+  Inst.addOperand(MCOperand::createImm(Value + Offset));
+  return MCDisassembler::Success;
+}
+
+template <unsigned Bits, int Offset, int ScaleBy>
+static DecodeStatus DecodeSImmWithOffsetAndScale(MCInst &Inst, unsigned Value,
+                                                 uint64_t Address,
+                                                 const void *Decoder) {
+  int32_t Imm = SignExtend32<Bits>(Value) * ScaleBy;
+  Inst.addOperand(MCOperand::createImm(Imm + Offset));
+  return MCDisassembler::Success;
+}
diff --git a/llvm/lib/Target/LoongArch/LLVMBuild.txt b/llvm/lib/Target/LoongArch/LLVMBuild.txt
new file mode 100644
index 000000000000..6157234f45e1
--- /dev/null
+++ b/llvm/lib/Target/LoongArch/LLVMBuild.txt
@@ -0,0 +1,43 @@
+;===- ./lib/Target/LoongArch/LLVMBuild.txt --------------------------*- Conf -*--===;
+;
+; Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+; See https://llvm.org/LICENSE.txt for license information.
+; SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+;
+;===------------------------------------------------------------------------===;
+;
+; This is an LLVMBuild description file for the components in this subdirectory.
+;
+; For more information on the LLVMBuild system, please see:
+;
+;   http://llvm.org/docs/LLVMBuild.html
+;
+;===------------------------------------------------------------------------===;
+
+[common]
+subdirectories = AsmParser Disassembler MCTargetDesc TargetInfo
+
+[component_0]
+type = TargetGroup
+name = LoongArch
+parent = Target
+has_asmparser = 1
+has_asmprinter = 1
+has_disassembler = 1
+
+[component_1]
+type = Library
+name = LoongArchCodeGen
+parent = LoongArch
+required_libraries =
+ Analysis
+ AsmPrinter
+ CodeGen
+ Core
+ MC
+ LoongArchDesc
+ LoongArchInfo
+ SelectionDAG
+ Support
+ Target
+add_to_library_groups = LoongArch
diff --git a/llvm/lib/Target/LoongArch/LoongArch.h b/llvm/lib/Target/LoongArch/LoongArch.h
new file mode 100644
index 000000000000..73fd4a628edb
--- /dev/null
+++ b/llvm/lib/Target/LoongArch/LoongArch.h
@@ -0,0 +1,37 @@
+//===-- LoongArch.h - Top-level interface for LoongArch representation ----*- C++ -*-===//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+//
+// This file contains the entry points for global functions defined in
+// the LLVM LoongArch back-end.
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef LLVM_LIB_TARGET_LOONGARCH_LOONGARCH_H
+#define LLVM_LIB_TARGET_LOONGARCH_LOONGARCH_H
+
+#include "MCTargetDesc/LoongArchMCTargetDesc.h"
+#include "llvm/Target/TargetMachine.h"
+
+namespace llvm {
+  class LoongArchTargetMachine;
+  class ModulePass;
+  class FunctionPass;
+  class LoongArchSubtarget;
+  class LoongArchTargetMachine;
+  class InstructionSelector;
+  class PassRegistry;
+
+  FunctionPass *createLoongArchModuleISelDagPass();
+  FunctionPass *createLoongArchOptimizePICCallPass();
+  FunctionPass *createLoongArchBranchExpansion();
+  FunctionPass *createLoongArchExpandPseudoPass();
+
+  void initializeLoongArchBranchExpansionPass(PassRegistry &);
+} // end namespace llvm;
+
+#endif
diff --git a/llvm/lib/Target/LoongArch/LoongArch.td b/llvm/lib/Target/LoongArch/LoongArch.td
new file mode 100644
index 000000000000..f839264559f5
--- /dev/null
+++ b/llvm/lib/Target/LoongArch/LoongArch.td
@@ -0,0 +1,96 @@
+//===-- LoongArch.td - Describe the LoongArch Target Machine ---------*- tablegen -*-===//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+// This is the top level entry point for the LoongArch target.
+//===----------------------------------------------------------------------===//
+
+//===----------------------------------------------------------------------===//
+// Target-independent interfaces
+//===----------------------------------------------------------------------===//
+
+include "llvm/Target/Target.td"
+
+// The overall idea of the PredicateControl class is to chop the Predicates list
+// into subsets that are usually overridden independently. This allows
+// subclasses to partially override the predicates of their superclasses without
+// having to re-add all the existing predicates.
+class PredicateControl {
+  // Predicates for the encoding scheme in use such as HasStdEnc
+  list<Predicate> EncodingPredicates = [];
+  // Predicates for the GPR size such as is64Bit
+  list<Predicate> GPRPredicates = [];
+  // Predicates for the FGR size and layout such as IsFP64bit
+  list<Predicate> FGRPredicates = [];
+  // Predicates for the instruction group membership such as ISA's.
+  list<Predicate> InsnPredicates = [];
+  // Predicate for the ISA extension that an instruction belongs to.
+  list<Predicate> ExtPredicate = [];
+  // Predicate for marking the instruction as usable in hard-float mode only.
+  list<Predicate> HardFloatPredicate = [];
+  // Predicates for anything else
+  list<Predicate> AdditionalPredicates = [];
+  list<Predicate> Predicates = !listconcat(EncodingPredicates,
+                                           GPRPredicates,
+                                           FGRPredicates,
+                                           InsnPredicates,
+                                           HardFloatPredicate,
+                                           ExtPredicate,
+                                           AdditionalPredicates);
+}
+
+// Like Requires<> but for the AdditionalPredicates list
+class AdditionalRequires<list<Predicate> preds> {
+  list<Predicate> AdditionalPredicates = preds;
+}
+
+//===----------------------------------------------------------------------===//
+// LoongArch Subtarget features                                                    //
+//===----------------------------------------------------------------------===//
+
+def FeatureFP64Bit     : SubtargetFeature<"fp64", "IsFP64bit", "true",
+                                "Support 64-bit FP registers">;
+def FeatureSingleFloat : SubtargetFeature<"single-float", "IsSingleFloat",
+                                "true", "Only supports single precision float">;
+def FeatureSoftFloat   : SubtargetFeature<"soft-float", "IsSoftFloat", "true",
+                                "Does not support floating point instructions">;
+def Feature64Bit       : SubtargetFeature<"64bit", "HasLA64", "true",
+                                "Support LA64 ISA",
+                                [FeatureFP64Bit]>;
+
+//===----------------------------------------------------------------------===//
+// Register File, Calling Conv, Instruction Descriptions
+//===----------------------------------------------------------------------===//
+
+include "LoongArchRegisterInfo.td"
+include "LoongArchInstrInfo.td"
+include "LoongArchCallingConv.td"
+
+def LoongArchInstrInfo : InstrInfo;
+
+//===----------------------------------------------------------------------===//
+// LoongArch processors supported.
+//===----------------------------------------------------------------------===//
+
+def : ProcessorModel<"la464", NoSchedModel, [Feature64Bit]>;
+
+def LoongArchAsmParser : AsmParser {
+  let ShouldEmitMatchRegisterName = 0;
+}
+
+def LoongArchAsmParserVariant : AsmParserVariant {
+  int Variant = 0;
+
+  // Recognize hard coded registers.
+  string RegisterPrefix = "$";
+}
+
+def LoongArch : Target {
+  let InstructionSet = LoongArchInstrInfo;
+  let AssemblyParsers = [LoongArchAsmParser];
+  let AssemblyParserVariants = [LoongArchAsmParserVariant];
+  let AllowRegisterRenaming = 1;
+}
diff --git a/llvm/lib/Target/LoongArch/LoongArch32InstrInfo.td b/llvm/lib/Target/LoongArch/LoongArch32InstrInfo.td
new file mode 100644
index 000000000000..92734c6248ff
--- /dev/null
+++ b/llvm/lib/Target/LoongArch/LoongArch32InstrInfo.td
@@ -0,0 +1,541 @@
+//===- LoongArch32InstrInfo.td - Target Description for LoongArch Target -*- tablegen -*-=//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+//
+// This file describes LoongArch32 instructions.
+//
+//===----------------------------------------------------------------------===//
+
+//===---------------------------------------------------------------------===/
+// Instruction Definitions.
+//===---------------------------------------------------------------------===/
+
+let DecoderNamespace = "LoongArch32" in {
+  ///
+  /// R2
+  ///
+  def CLO_W : Count1<"clo.w", GPR32Opnd, ctlz>, R2I<0b00100>;
+  def CLZ_W : Int_Reg2<"clz.w", GPR32Opnd, ctlz>, R2I<0b00101>;
+  def CTO_W : Count1<"cto.w", GPR32Opnd, cttz>, R2I<0b00110>;
+  def CTZ_W : Int_Reg2<"ctz.w", GPR32Opnd, cttz>, R2I<0b00111>;
+
+  def REVB_2H : Int_Reg2<"revb.2h", GPR32Opnd>, R2I<0b01100>;//see below bswap pattern
+
+  def BITREV_4B : Int_Reg2<"bitrev.4b", GPR32Opnd>, R2I<0b10010>;
+  def BITREV_W  : Int_Reg2<"bitrev.w", GPR32Opnd, bitreverse>, R2I<0b10100>;
+
+  let isCodeGenOnly = 1 in {
+    def EXT_W_H32 : SignExtInReg<"ext.w.h", GPR32Opnd, i16>, R2I<0b10110>;
+    def EXT_W_B32 : SignExtInReg<"ext.w.b", GPR32Opnd, i8>, R2I<0b10111>;
+
+  }
+
+  def CPUCFG    : Int_Reg2<"cpucfg", GPR32Opnd, int_loongarch_cpucfg>, R2I<0b11011>;
+  def RDTIMEL_W32 : Int_Reg2_Rdtime<"rdtimel.w", GPR32Opnd>, R2I<0b11000>;
+  def RDTIMEH_W32 : Int_Reg2_Rdtime<"rdtimeh.w", GPR32Opnd>, R2I<0b11001>;
+
+  ///
+  /// R3
+  ///
+  def ADD_W : Int_Reg3<"add.w", GPR32Opnd, add>, R3I<0b0100000>;
+  def SUB_W : Int_Reg3<"sub.w", GPR32Opnd, sub>, R3I<0b0100010>;
+
+  let isCodeGenOnly = 1 in {
+    def SLT32     : SetCC_R<"slt", GPR32Opnd, setlt>, R3I<0b0100100>;
+    def SLTU32    : SetCC_R<"sltu", GPR32Opnd, setult>, R3I<0b0100101>;
+    def MASKEQZ32 : Int_Reg3<"maskeqz", GPR32Opnd>, R3I<0b0100110>;//see below patterns
+    def MASKNEZ32 : Int_Reg3<"masknez", GPR32Opnd>, R3I<0b0100111>;//see below patterns
+
+    def NOR32   : Nor<"nor", GPR32Opnd>, R3I<0b0101000>;
+    def AND32   : Int_Reg3<"and", GPR32Opnd, and>, R3I<0b0101001>;
+    def OR32    : Int_Reg3<"or", GPR32Opnd, or>, R3I<0b0101010>;
+    def XOR32   : Int_Reg3<"xor", GPR32Opnd, xor>, R3I<0b0101011>;
+  }
+
+  def SLL_W : Shift_Var<"sll.w", GPR32Opnd, shl>, R3I<0b0101110>;
+  def SRL_W : Shift_Var<"srl.w", GPR32Opnd, srl>, R3I<0b0101111>;
+  def SRA_W : Shift_Var<"sra.w", GPR32Opnd, sra>, R3I<0b0110000>;
+  def ROTR_W: Shift_Var<"rotr.w", GPR32Opnd, rotr>, R3I<0b0110110>;
+
+  def MUL_W     : Int_Reg3<"mul.w", GPR32Opnd, mul>, R3I<0b0111000>;
+  def MULH_W    : Int_Reg3<"mulh.w", GPR32Opnd, mulhs>, R3I<0b0111001>;
+  def MULH_WU   : Int_Reg3<"mulh.wu", GPR32Opnd, mulhu>, R3I<0b0111010>;
+
+let usesCustomInserter = 1 in {
+  def DIV_W  : Int_Reg3<"div.w", GPR32Opnd, sdiv>, R3I<0b1000000>;
+  def MOD_W  : Int_Reg3<"mod.w", GPR32Opnd, srem>, R3I<0b1000001>;
+  def DIV_WU : Int_Reg3<"div.wu", GPR32Opnd, udiv>, R3I<0b1000010>;
+  def MOD_WU : Int_Reg3<"mod.wu", GPR32Opnd, urem>, R3I<0b1000011>;
+}
+
+  def CRC_W_B_W  : Int_Reg3<"crc.w.b.w", GPR32Opnd, int_loongarch_crc_w_b_w>, R3I<0b1001000>;
+  def CRC_W_H_W  : Int_Reg3<"crc.w.h.w", GPR32Opnd, int_loongarch_crc_w_h_w>, R3I<0b1001001>;
+  def CRC_W_W_W  : Int_Reg3<"crc.w.w.w", GPR32Opnd, int_loongarch_crc_w_w_w>, R3I<0b1001010>;
+  def CRCC_W_B_W : Int_Reg3<"crcc.w.b.w", GPR32Opnd, int_loongarch_crcc_w_b_w>, R3I<0b1001100>;
+  def CRCC_W_H_W : Int_Reg3<"crcc.w.h.w", GPR32Opnd, int_loongarch_crcc_w_h_w>, R3I<0b1001101>;
+  def CRCC_W_W_W : Int_Reg3<"crcc.w.w.w", GPR32Opnd, int_loongarch_crcc_w_w_w>, R3I<0b1001110>;
+  ///
+  /// SLLI
+  ///
+  def SLLI_W  : Shift_Imm32<"slli.w", GPR32Opnd, shl>, R2_IMM5<0b00>;
+  def SRLI_W  : Shift_Imm32<"srli.w", GPR32Opnd, srl>, R2_IMM5<0b01>;
+  def SRAI_W  : Shift_Imm32<"srai.w", GPR32Opnd, sra>, R2_IMM5<0b10>;
+  def ROTRI_W : Shift_Imm32<"rotri.w", GPR32Opnd, rotr>, R2_IMM5<0b11>;
+  ///
+  /// Misc
+  ///
+  def ALSL_W    : Reg3_Sa<"alsl.w", GPR32Opnd, uimm2_plus1>, R3_SA2<0b00010> {
+    let Pattern = [(set GPR32Opnd:$rd,
+                    (add GPR32Opnd:$rk, (shl GPR32Opnd:$rj, immZExt2Alsl:$sa)))];
+  }
+  def BYTEPICK_W : Reg3_Sa<"bytepick.w", GPR32Opnd, uimm2>, R3_SA2<0b00100>;//pattern:[]
+
+  def BREAK   : Code15<"break", int_loongarch_break>, CODE15<0b1010100>;
+  def SYSCALL : Code15<"syscall", int_loongarch_syscall>, CODE15<0b1010110>;
+  def TRAP    : TrapBase<BREAK>;
+
+  def BSTRINS_W  : InsBase_32<"bstrins.w", GPR32Opnd, uimm5, LoongArchBstrins>,
+                   INSERT_BIT32<0>;
+  def BSTRPICK_W : PickBase_32<"bstrpick.w", GPR32Opnd, uimm5, LoongArchBstrpick>,
+                   INSERT_BIT32<1>;
+
+  ///
+  /// R2_IMM12
+  ///
+  let isCodeGenOnly = 1 in {
+    def SLTI32    : SetCC_I<"slti", GPR32Opnd, simm12_32>, R2_IMM12<0b000>; //PatFrag
+    def SLTUI32   : SetCC_I<"sltui", GPR32Opnd, simm12_32>, R2_IMM12<0b001>; //PatFrag
+  }
+  def ADDI_W  : Int_Reg2_Imm12<"addi.w", GPR32Opnd, simm12_32, add>, R2_IMM12<0b010>;
+
+  let isCodeGenOnly = 1 in {
+    def ANDI32 : Int_Reg2_Imm12<"andi", GPR32Opnd, uimm12_32, and>, R2_IMM12<0b101>;
+    def ORI32  : Int_Reg2_Imm12<"ori", GPR32Opnd, uimm12_32, or>, R2_IMM12<0b110>;
+    def XORI32 : Int_Reg2_Imm12<"xori", GPR32Opnd, uimm12_32, xor>, R2_IMM12<0b111>;
+  }
+
+  ///
+  /// Privilege Instructions
+  ///
+  def CSRRD32 : CSR<"csrrd", GPR32Opnd, uimm14_32, int_loongarch_csrrd>, R1_CSR<0b0000000000100>;
+  def CSRWR32 : CSRW<"csrwr", GPR32Opnd, uimm14_32, int_loongarch_csrwr>, R1_CSR<0b0000100000100>;
+  def CSRXCHG32 : CSRX<"csrxchg", GPR32Opnd, uimm14_32, int_loongarch_csrxchg>, R2_CSR<0b00000100>;
+  def IOCSRRD_B32 : Int_Reg2<"iocsrrd.b", GPR32Opnd, int_loongarch_iocsrrd_b>, R2P<0b000>;
+  def IOCSRRD_H32 : Int_Reg2<"iocsrrd.h", GPR32Opnd, int_loongarch_iocsrrd_h>, R2P<0b001>;
+  def IOCSRRD_W32 : Int_Reg2<"iocsrrd.w", GPR32Opnd, int_loongarch_iocsrrd_w>, R2P<0b010>;
+  def IOCSRWR_B32 : Int_Reg2_Iocsrwr<"iocsrwr.b", GPR32Opnd, GPR32Opnd, int_loongarch_iocsrwr_b>, R2P<0b100>;
+  def IOCSRWR_H32 : Int_Reg2_Iocsrwr<"iocsrwr.h", GPR32Opnd, GPR32Opnd, int_loongarch_iocsrwr_h>, R2P<0b101>;
+  def IOCSRWR_W32 : Int_Reg2_Iocsrwr<"iocsrwr.w", GPR32Opnd, GPR32Opnd, int_loongarch_iocsrwr_w>, R2P<0b110>;
+  def CACOP32 : CAC<"cacop", GPR32Opnd, simm12_32, int_loongarch_cacop>, R1_CACHE;
+  def LDDIR32 : LEVEL<"lddir", GPR32Opnd>, R2_LEVEL<0b00000110010000>;
+  def LDPTE32 : SEQ<"ldpte", GPR32Opnd>, R1_SEQ<0b00000110010001>;
+
+  //def WAIT : Wait<"wait">;
+  //
+  //def IOCSRRD_D : R2P<0b011>, Int_Reg2<"iocsrrd.d", GPR32Opnd>;
+  //def IOCSRWR_D : R2P<0b111>, Int_Reg2<"iocsrwr.d", GPR32Opnd>;
+  //
+  //def TLBINV   : IMM32<0b001000>, OP32<"tlbinv">;
+  //def TLBFLUSH : IMM32<0b001001>, OP32<"tlbflush">;
+  //def TLBP     : IMM32<0b001010>, OP32<"tlbp">;
+  //def TLBR     : IMM32<0b001011>, OP32<"tlbr">;
+  //def TLBWI    : IMM32<0b001100>, OP32<"tlbwi">;
+  //def TLBWR    : IMM32<0b001101>, OP32<"tlbwr">;
+
+  ///
+  /// R1_IMM20
+  ///
+  let isCodeGenOnly = 1 in {
+    def LU12I_W32   : SI20<"lu12i.w", GPR32Opnd, simm20_32>, R1_SI20<0b0001010>;
+    def PCADDI32    : SI20<"pcaddi", GPR32Opnd, simm20_32>, R1_SI20<0b0001100>;
+    def PCALAU12I32 : SI20<"pcalau12i", GPR32Opnd, simm20_32>, R1_SI20<0b0001101>;
+    def PCADDU12I32 : SI20<"pcaddu12i", GPR32Opnd, simm20_32>, R1_SI20<0b0001110>;
+  }
+
+  let isCodeGenOnly = 1 in {
+    def BEQZ32  : Beqz<"beqz", brtarget, seteq, GPR32Opnd>, R1_IMM21BEQZ<0b010000>;
+    def BNEZ32  : Beqz<"bnez", brtarget, setne, GPR32Opnd>, R1_IMM21BEQZ<0b010001>;
+
+    def JIRL32  : FJirl<"jirl", calltarget, GPR32Opnd>, R2_IMM16JIRL;
+
+    def B32     : JumpFB<jmptarget, "b", br, bb>, IMM26B<0b010100>;
+
+    def BEQ32   : Beq<"beq", brtarget, seteq, GPR32Opnd>, R2_IMM16BEQ<0b010110>;
+    def BNE32   : Beq<"bne", brtarget, setne, GPR32Opnd>, R2_IMM16BEQ<0b010111>;
+    def BLT32   : Beq<"blt", brtarget, setlt, GPR32Opnd>, R2_IMM16BEQ<0b011000>;
+    def BGE32   : Beq<"bge", brtarget, setge, GPR32Opnd>, R2_IMM16BEQ<0b011001>;
+    def BLTU32  : Beq<"bltu", brtarget, setult, GPR32Opnd>, R2_IMM16BEQ<0b011010>;
+    def BGEU32  : Beq<"bgeu", brtarget, setuge, GPR32Opnd>, R2_IMM16BEQ<0b011011>;
+  }
+
+  ///
+  /// Mem access
+  ///
+  def LL_W : LLBase<"ll.w", GPR32Opnd, mem_simm14_lsl2>, LL_SC<0b000>;
+  def SC_W : SCBase<"sc.w", GPR32Opnd, mem_simm14_lsl2>, LL_SC<0b001>;
+
+  def PRELD_Raw32  : Preld_Raw<"preld", GPR32Opnd>, PRELD_FM;
+
+  let isCodeGenOnly = 1 in {
+    def LD_B32  : Ld<"ld.b", GPR32Opnd, mem_simmptr, sextloadi8>, LOAD_STORE<0b0000>;
+    def LD_H32  : Ld<"ld.h", GPR32Opnd, mem_simmptr, sextloadi16, addrDefault>, LOAD_STORE<0b0001>;
+    def LD_W32  : Ld<"ld.w", GPR32Opnd, mem, load, addrDefault>, LOAD_STORE<0b0010>;
+    def ST_B32  : St<"st.b", GPR32Opnd, mem, truncstorei8>, LOAD_STORE<0b0100>;
+    def ST_H32  : St<"st.h", GPR32Opnd, mem, truncstorei16>, LOAD_STORE<0b0101>;
+    def ST_W32  : St<"st.w", GPR32Opnd, mem, store>, LOAD_STORE<0b0110>;
+    def LD_BU32 : Ld<"ld.bu", GPR32Opnd, mem_simmptr, zextloadi8, addrDefault>, LOAD_STORE<0b1000>;
+    def LD_HU32 : Ld<"ld.hu", GPR32Opnd, mem_simmptr, zextloadi16>, LOAD_STORE<0b1001>;
+
+    def PRELD32  : Preld<"preld", mem, GPR32Opnd>, PRELD_FM;
+
+    def LDPTR_W32 : LdPtr<"ldptr.w", GPR32Opnd>, LL_SC<0b100>;
+    def STPTR_W32 : StPtr<"stptr.w", GPR32Opnd>, LL_SC<0b101>;
+  }
+
+  def IBAR : Bar<"ibar", int_loongarch_ibar>, BAR_FM<1>;
+  def DBAR : Bar<"dbar", int_loongarch_dbar>, BAR_FM<0>;
+
+  def LONG_BRANCH_ADDIW : LoongArchPseudo<(outs GPR32Opnd:$dst),
+      (ins GPR32Opnd:$src, brtarget:$tgt, brtarget:$baltgt), []>;
+
+  def LONG_BRANCH_ADDIW2Op : LoongArchPseudo<(outs GPR32Opnd:$dst),
+      (ins GPR32Opnd:$src, brtarget:$tgt), []>;
+
+  def PseudoReturn : PseudoReturnBase<GPR32Opnd>;
+
+  let isCodeGenOnly = 1 in {
+    def LDX_W32 : LDX_FT_LA<"ldx.w", GPR32Opnd, load>,
+                  R3MI<0b00010000>;
+    def LDX_HU32 : LDX_FT_LA<"ldx.hu", GPR32Opnd, extloadi16>,
+                   R3MI<0b01001000>;
+    def LDX_BU32 : LDX_FT_LA<"ldx.bu", GPR32Opnd, extloadi8>,
+                   R3MI<0b01000000>;
+    def STX_W32 : SDX_FT_LA<"stx.w", GPR32Opnd, store>,
+                  R3MI<0b00110000>;
+    def LDX_H32 : LDX_FT_LA<"ldx.h", GPR32Opnd, sextloadi16>,
+                  R3MI<0b00001000>;
+    def LDX_B32 : LDX_FT_LA<"ldx.b", GPR32Opnd, sextloadi8>,
+                  R3MI<0b00000000>;
+    def STX_B32 : SDX_FT_LA<"stx.b", GPR32Opnd, truncstorei8>,
+                  R3MI<0b00100000>;
+    def STX_H32 : SDX_FT_LA<"stx.h", GPR32Opnd, truncstorei16>,
+                  R3MI<0b00101000>;
+  }
+}
+
+def LEA_ADDI_W: EffectiveAddress<"addi.w", GPR32Opnd>, LEA_ADDI_FM<0b010>;
+
+def : LoongArchPat<(LoongArchAddress (i32 tglobaladdr:$in)),
+                   (ADDI_W (PCADDU12I32 tglobaladdr:$in) ,0)>,GPR_32;
+def : LoongArchPat<(LoongArchAddress (i32 tblockaddress:$in)),
+                   (ADDI_W (PCADDU12I32 tblockaddress:$in),0)>, GPR_32;
+def : LoongArchPat<(LoongArchAddress (i32 tjumptable:$in)),
+                   (ADDI_W (PCADDU12I32 tjumptable:$in),0)>, GPR_32;
+def : LoongArchPat<(LoongArchAddress (i32 texternalsym:$in)),
+                   (ADDI_W (PCADDU12I32 texternalsym:$in),0)>, GPR_32;
+
+//===----------------------------------------------------------------------===//
+//  Arbitrary patterns that map to one or more instructions
+//===----------------------------------------------------------------------===//
+
+let isCodeGenOnly = 1 in {
+  def REVB_2W_32 : Int_Reg2<"revb.2w", GPR32Opnd>, R2I<0b01110>;
+  def REVH_2W_32 : Int_Reg2<"revh.2w", GPR32Opnd>, R2I<0b10000>;
+}
+
+// bswap pattern
+def : LoongArchPat<(bswap GPR32:$rj), (ROTRI_W (REVB_2H GPR32:$rj), 16)>;
+//def : LoongArchPat<(bswap GPR32:$rj), (REVB_2W_32 GPR32:$rj)>;
+//def : LoongArchPat<(bswap GPR32:$rj), (REVH_2W_32 (REVB_2H GPR32:$rj))>;
+
+// i32 selects
+multiclass SelectInt_Pats<ValueType RC, Instruction OROp, Instruction XORiOp,
+                          Instruction SLTiOp, Instruction SLTiuOp,
+                          Instruction MASKNEZOp, Instruction MASKEQZOp,
+                          SDPatternOperator imm_type, ValueType Opg> {
+
+// reg, immz
+def : LoongArchPat<(select (Opg (seteq RC:$cond, immz)), RC:$t, RC:$f),
+                   (OROp (MASKNEZOp RC:$t, RC:$cond), (MASKEQZOp RC:$f, RC:$cond))>;
+def : LoongArchPat<(select (Opg (setne RC:$cond, immz)), RC:$t, RC:$f),
+                   (OROp (MASKEQZOp RC:$t, RC:$cond), (MASKNEZOp RC:$f, RC:$cond))>;
+
+//def : LoongArchPat<(select (Opg (seteq RC:$cond, imm_type:$imm)), RC:$t, RC:$f),
+//                   (OROp (MASKNEZOp RC:$t, (XORiOp RC:$cond, imm_type:$imm)),
+//                         (MASKEQZOp RC:$f, (XORiOp RC:$cond, imm_type:$imm)))>;
+//def : LoongArchPat<(select (Opg (setne RC:$cond, imm_type:$imm)), RC:$t, RC:$f),
+//                   (OROp (MASKEQZOp RC:$t, (XORiOp RC:$cond, imm_type:$imm)),
+//                         (MASKNEZOp RC:$f, (XORiOp RC:$cond, imm_type:$imm)))>;
+
+// reg, immSExt12Plus1
+//def : LoongArchPat<(select (Opg (setgt RC:$cond, immSExt12Plus1:$imm)), RC:$t, RC:$f),
+//                   (OROp (MASKNEZOp RC:$t, (SLTiOp RC:$cond, (Plus1 imm:$imm))),
+//                         (MASKEQZOp RC:$f, (SLTiOp RC:$cond, (Plus1 imm:$imm))))>;
+//def : LoongArchPat<(select (Opg (setugt RC:$cond, immSExt16Plus1:$imm)), RC:$t, RC:$f),
+//                   (OROp (MASKNEZOp RC:$t, (SLTiuOp RC:$cond, (Plus1 imm:$imm))),
+//                         (MASKEQZOp RC:$f, (SLTiuOp RC:$cond, (Plus1 imm:$imm))))>;
+
+def : LoongArchPat<(select (Opg (seteq RC:$cond, immz)), RC:$t, immz),
+                   (MASKNEZOp RC:$t, RC:$cond)>;
+def : LoongArchPat<(select (Opg (setne RC:$cond, immz)), RC:$t, immz),
+                   (MASKEQZOp RC:$t, RC:$cond)>;
+def : LoongArchPat<(select (Opg (seteq RC:$cond, immz)), immz, RC:$f),
+                   (MASKEQZOp RC:$f, RC:$cond)>;
+def : LoongArchPat<(select (Opg (setne RC:$cond, immz)), immz, RC:$f),
+                   (MASKNEZOp RC:$f, RC:$cond)>;
+}
+
+defm : SelectInt_Pats<i32, OR32, XORI32, SLTI32, SLTUI32, MASKNEZ32, MASKEQZ32,
+                      immZExt12, i32>;
+
+def : LoongArchPat<(select i32:$cond, i32:$t, i32:$f),
+                   (OR32 (MASKEQZ32 i32:$t, i32:$cond),
+                         (MASKNEZ32 i32:$f, i32:$cond))>;
+def : LoongArchPat<(select i32:$cond, i32:$t, immz),
+                   (MASKEQZ32 i32:$t, i32:$cond)>;
+def : LoongArchPat<(select i32:$cond, immz, i32:$f),
+                   (MASKNEZ32 i32:$f, i32:$cond)>;
+
+// truncate
+def : LoongArchPat<(i32 (trunc (i64 (assertsext GPR64:$src)))),
+                   (EXTRACT_SUBREG GPR64:$src, sub_32)>, GPR_64;
+def : LoongArchPat<(i32 (trunc (assertzext_lt_i32 GPR64:$src))),
+                   (EXTRACT_SUBREG GPR64:$src, sub_32)>, GPR_64;
+def : LoongArchPat<(i32 (trunc GPR64:$src)),
+                   (SLLI_W (EXTRACT_SUBREG GPR64:$src, sub_32), 0)>, GPR_64;
+
+// Patterns used for matching away redundant sign extensions.
+// LA32 arithmetic instructions sign extend their result implicitly.
+def : LoongArchPat<(i64 (sext (i32 (add GPR32:$src, GPR32:$src2)))),
+                   (INSERT_SUBREG (i64 (IMPLICIT_DEF)),
+                   (ADD_W GPR32:$src, GPR32:$src2), sub_32)>;
+def : LoongArchPat<(i64 (sext (i32 (sub GPR32:$src, GPR32:$src2)))),
+                   (INSERT_SUBREG (i64 (IMPLICIT_DEF)),
+                   (SUB_W GPR32:$src, GPR32:$src2), sub_32)>;
+def : LoongArchPat<(i64 (sext (i32 (mul GPR32:$src, GPR32:$src2)))),
+                   (INSERT_SUBREG (i64 (IMPLICIT_DEF)),
+                   (MUL_W GPR32:$src, GPR32:$src2), sub_32)>;
+
+def : LoongArchPat<(store (i32 0), addr:$dst), (ST_W32 ZERO, addr:$dst)>;
+
+// Materialize constants.
+def : LoongArchPat<(i32 LU12IORIPred:$imm), (ORI32 (LU12I_W32 (HI20 imm:$imm)), (LO12 imm:$imm))>;
+def : LoongArchPat<(i32 LU12IPred:$imm), (LU12I_W32 (HI20 imm:$imm))>;
+def : LoongArchPat<(i32 ORi12Pred:$imm), (ORI32 ZERO, imm:$imm)>;
+def : LoongArchPat<(i32 immSExt12:$imm), (ADDI_W ZERO, imm:$imm)>;
+
+def : InstAlias<"break", (BREAK 0), 1>;
+def : InstAlias<"break $imm", (BREAK uimm15:$imm), 1>;
+def : LoongArchInstAlias<"move $dst, $src",
+                         (OR32 GPR32Opnd:$dst, GPR32Opnd:$src, ZERO), 1>, GPR_32;
+
+def immSExt12Plus1 : PatLeaf<(imm), [{
+  return isInt<13>(N->getSExtValue()) && isInt<12>(N->getSExtValue() + 1);
+}]>;
+
+def Plus1 : SDNodeXForm<imm, [{ return getImm(N, N->getSExtValue() + 1); }]>;
+
+multiclass BrcondPats<RegisterClass RC, Instruction BEQOp, Instruction BEQOp1,
+                      Instruction BNEOp, Instruction SLTOp, Instruction SLTUOp,
+                      Instruction SLTIOp, Instruction SLTUIOp,
+                      Register ZEROReg> {
+
+def : LoongArchPat<(brcond (i32 (setne RC:$lhs, 0)), bb:$dst),
+      (BNEOp RC:$lhs, ZEROReg, bb:$dst)>;
+def : LoongArchPat<(brcond (i32 (seteq RC:$lhs, 0)), bb:$dst),
+      (BEQOp RC:$lhs, ZEROReg, bb:$dst)>;
+def : LoongArchPat<(brcond (i32 (setge RC:$lhs, RC:$rhs)), bb:$dst),
+      (BEQOp1 (SLTOp RC:$lhs, RC:$rhs), ZEROReg, bb:$dst)>;
+def : LoongArchPat<(brcond (i32 (setuge RC:$lhs, RC:$rhs)), bb:$dst),
+      (BEQOp1 (SLTUOp RC:$lhs, RC:$rhs), ZEROReg, bb:$dst)>;
+def : LoongArchPat<(brcond (i32 (setge RC:$lhs, immSExt12:$rhs)), bb:$dst),
+      (BEQOp1 (SLTIOp RC:$lhs, immSExt12:$rhs), ZEROReg, bb:$dst)>;
+def : LoongArchPat<(brcond (i32 (setuge RC:$lhs, immSExt12:$rhs)), bb:$dst),
+      (BEQOp1 (SLTUIOp RC:$lhs, immSExt12:$rhs), ZEROReg, bb:$dst)>;
+def : LoongArchPat<(brcond (i32 (setgt RC:$lhs, immSExt12Plus1:$rhs)), bb:$dst),
+      (BEQOp1 (SLTIOp RC:$lhs, (Plus1 imm:$rhs)), ZEROReg, bb:$dst)>;
+def : LoongArchPat<(brcond (i32 (setugt RC:$lhs, immSExt12Plus1:$rhs)), bb:$dst),
+      (BEQOp1 (SLTUIOp RC:$lhs, (Plus1 imm:$rhs)), ZEROReg, bb:$dst)>;
+def : LoongArchPat<(brcond (i32 (setle RC:$lhs, RC:$rhs)), bb:$dst),
+      (BEQOp1 (SLTOp RC:$rhs, RC:$lhs), ZEROReg, bb:$dst)>;
+def : LoongArchPat<(brcond (i32 (setule RC:$lhs, RC:$rhs)), bb:$dst),
+      (BEQOp1 (SLTUOp RC:$rhs, RC:$lhs), ZEROReg, bb:$dst)>;
+def : LoongArchPat<(brcond RC:$cond, bb:$dst),
+      (BNEOp RC:$cond, ZEROReg, bb:$dst)>;
+}
+
+defm : BrcondPats<GPR32, BEQ32, BEQ32, BNE32, SLT32, SLTU32, SLTI32, SLTUI32, ZERO>, GPR_64;
+
+let usesCustomInserter = 1 in {
+  def ATOMIC_LOAD_ADD_I8   : Atomic2Ops<atomic_load_add_8, GPR32>;
+  def ATOMIC_LOAD_ADD_I16  : Atomic2Ops<atomic_load_add_16, GPR32>;
+  def ATOMIC_LOAD_ADD_I32  : Atomic2Ops<atomic_load_add_32, GPR32>;
+  def ATOMIC_LOAD_SUB_I8   : Atomic2Ops<atomic_load_sub_8, GPR32>;
+  def ATOMIC_LOAD_SUB_I16  : Atomic2Ops<atomic_load_sub_16, GPR32>;
+  def ATOMIC_LOAD_SUB_I32  : Atomic2Ops<atomic_load_sub_32, GPR32>;
+  def ATOMIC_LOAD_AND_I8   : Atomic2Ops<atomic_load_and_8, GPR32>;
+  def ATOMIC_LOAD_AND_I16  : Atomic2Ops<atomic_load_and_16, GPR32>;
+  def ATOMIC_LOAD_AND_I32  : Atomic2Ops<atomic_load_and_32, GPR32>;
+  def ATOMIC_LOAD_OR_I8    : Atomic2Ops<atomic_load_or_8, GPR32>;
+  def ATOMIC_LOAD_OR_I16   : Atomic2Ops<atomic_load_or_16, GPR32>;
+  def ATOMIC_LOAD_OR_I32   : Atomic2Ops<atomic_load_or_32, GPR32>;
+  def ATOMIC_LOAD_XOR_I8   : Atomic2Ops<atomic_load_xor_8, GPR32>;
+  def ATOMIC_LOAD_XOR_I16  : Atomic2Ops<atomic_load_xor_16, GPR32>;
+  def ATOMIC_LOAD_XOR_I32  : Atomic2Ops<atomic_load_xor_32, GPR32>;
+  def ATOMIC_LOAD_NAND_I8  : Atomic2Ops<atomic_load_nand_8, GPR32>;
+  def ATOMIC_LOAD_NAND_I16 : Atomic2Ops<atomic_load_nand_16, GPR32>;
+  def ATOMIC_LOAD_NAND_I32 : Atomic2Ops<atomic_load_nand_32, GPR32>;
+
+  def ATOMIC_SWAP_I8       : Atomic2Ops<atomic_swap_8, GPR32>;
+  def ATOMIC_SWAP_I16      : Atomic2Ops<atomic_swap_16, GPR32>;
+  def ATOMIC_SWAP_I32      : Atomic2Ops<atomic_swap_32, GPR32>;
+
+  def ATOMIC_CMP_SWAP_I8   : AtomicCmpSwap<atomic_cmp_swap_8, GPR32>;
+  def ATOMIC_CMP_SWAP_I16  : AtomicCmpSwap<atomic_cmp_swap_16, GPR32>;
+  def ATOMIC_CMP_SWAP_I32  : AtomicCmpSwap<atomic_cmp_swap_32, GPR32>;
+
+  def ATOMIC_LOAD_MAX_I8   : Atomic2Ops<atomic_load_max_8, GPR32>;
+  def ATOMIC_LOAD_MAX_I16  : Atomic2Ops<atomic_load_max_16, GPR32>;
+  def ATOMIC_LOAD_MAX_I32  : Atomic2Ops<atomic_load_max_32, GPR32>;
+
+  def ATOMIC_LOAD_MIN_I8   : Atomic2Ops<atomic_load_min_8, GPR32>;
+  def ATOMIC_LOAD_MIN_I16  : Atomic2Ops<atomic_load_min_16, GPR32>;
+  def ATOMIC_LOAD_MIN_I32  : Atomic2Ops<atomic_load_min_32, GPR32>;
+
+  def ATOMIC_LOAD_UMAX_I8   : Atomic2Ops<atomic_load_umax_8, GPR32>;
+  def ATOMIC_LOAD_UMAX_I16  : Atomic2Ops<atomic_load_umax_16, GPR32>;
+  def ATOMIC_LOAD_UMAX_I32  : Atomic2Ops<atomic_load_umax_32, GPR32>;
+
+  def ATOMIC_LOAD_UMIN_I8   : Atomic2Ops<atomic_load_umin_8, GPR32>;
+  def ATOMIC_LOAD_UMIN_I16  : Atomic2Ops<atomic_load_umin_16, GPR32>;
+  def ATOMIC_LOAD_UMIN_I32  : Atomic2Ops<atomic_load_umin_32, GPR32>;
+}
+
+def ATOMIC_LOAD_ADD_I8_POSTRA   : Atomic2OpsSubwordPostRA<GPR32>;
+def ATOMIC_LOAD_ADD_I16_POSTRA  : Atomic2OpsSubwordPostRA<GPR32>;
+def ATOMIC_LOAD_ADD_I32_POSTRA  : Atomic2OpsPostRA<GPR32>;
+def ATOMIC_LOAD_SUB_I8_POSTRA   : Atomic2OpsSubwordPostRA<GPR32>;
+def ATOMIC_LOAD_SUB_I16_POSTRA  : Atomic2OpsSubwordPostRA<GPR32>;
+def ATOMIC_LOAD_SUB_I32_POSTRA  : Atomic2OpsPostRA<GPR32>;
+def ATOMIC_LOAD_AND_I8_POSTRA   : Atomic2OpsSubwordPostRA<GPR32>;
+def ATOMIC_LOAD_AND_I16_POSTRA  : Atomic2OpsSubwordPostRA<GPR32>;
+def ATOMIC_LOAD_AND_I32_POSTRA  : Atomic2OpsPostRA<GPR32>;
+def ATOMIC_LOAD_OR_I8_POSTRA    : Atomic2OpsSubwordPostRA<GPR32>;
+def ATOMIC_LOAD_OR_I16_POSTRA   : Atomic2OpsSubwordPostRA<GPR32>;
+def ATOMIC_LOAD_OR_I32_POSTRA   : Atomic2OpsPostRA<GPR32>;
+def ATOMIC_LOAD_XOR_I8_POSTRA   : Atomic2OpsSubwordPostRA<GPR32>;
+def ATOMIC_LOAD_XOR_I16_POSTRA  : Atomic2OpsSubwordPostRA<GPR32>;
+def ATOMIC_LOAD_XOR_I32_POSTRA  : Atomic2OpsPostRA<GPR32>;
+def ATOMIC_LOAD_NAND_I8_POSTRA  : Atomic2OpsSubwordPostRA<GPR32>;
+def ATOMIC_LOAD_NAND_I16_POSTRA : Atomic2OpsSubwordPostRA<GPR32>;
+def ATOMIC_LOAD_NAND_I32_POSTRA : Atomic2OpsPostRA<GPR32>;
+
+def ATOMIC_SWAP_I8_POSTRA  : Atomic2OpsSubwordPostRA<GPR32>;
+def ATOMIC_SWAP_I16_POSTRA : Atomic2OpsSubwordPostRA<GPR32>;
+def ATOMIC_SWAP_I32_POSTRA : Atomic2OpsPostRA<GPR32>;
+
+def ATOMIC_CMP_SWAP_I8_POSTRA : AtomicCmpSwapSubwordPostRA<GPR32>;
+def ATOMIC_CMP_SWAP_I16_POSTRA : AtomicCmpSwapSubwordPostRA<GPR32>;
+def ATOMIC_CMP_SWAP_I32_POSTRA : AtomicCmpSwapPostRA<GPR32>;
+
+def ATOMIC_LOAD_MAX_I8_POSTRA   : Atomic2OpsSubwordPostRA<GPR32>;
+def ATOMIC_LOAD_MAX_I16_POSTRA  : Atomic2OpsSubwordPostRA<GPR32>;
+def ATOMIC_LOAD_MAX_I32_POSTRA  : Atomic2OpsPostRA<GPR32>;
+
+def ATOMIC_LOAD_MIN_I8_POSTRA   : Atomic2OpsSubwordPostRA<GPR32>;
+def ATOMIC_LOAD_MIN_I16_POSTRA  : Atomic2OpsSubwordPostRA<GPR32>;
+def ATOMIC_LOAD_MIN_I32_POSTRA  : Atomic2OpsPostRA<GPR32>;
+
+def ATOMIC_LOAD_UMAX_I8_POSTRA   : Atomic2OpsSubwordPostRA<GPR32>;
+def ATOMIC_LOAD_UMAX_I16_POSTRA  : Atomic2OpsSubwordPostRA<GPR32>;
+def ATOMIC_LOAD_UMAX_I32_POSTRA  : Atomic2OpsPostRA<GPR32>;
+
+def ATOMIC_LOAD_UMIN_I8_POSTRA   : Atomic2OpsSubwordPostRA<GPR32>;
+def ATOMIC_LOAD_UMIN_I16_POSTRA  : Atomic2OpsSubwordPostRA<GPR32>;
+def ATOMIC_LOAD_UMIN_I32_POSTRA  : Atomic2OpsPostRA<GPR32>;
+
+def : LoongArchPat<(atomic_load_8 addr:$a), (LD_B32 addr:$a)>;
+def : LoongArchPat<(atomic_load_16 addr:$a), (LD_H32 addr:$a)>;
+def : LoongArchPat<(atomic_load_32 addrimm14lsl2:$a), (LDPTR_W32 addrimm14lsl2:$a)>;
+def : LoongArchPat<(atomic_load_32 addr:$a), (LD_W32 addr:$a)>;
+
+def : LoongArchPat<(atomic_store_8 addr:$a, GPR32:$v),
+      (ST_B32 GPR32:$v, addr:$a)>;
+def : LoongArchPat<(atomic_store_16 addr:$a, GPR32:$v),
+      (ST_H32 GPR32:$v, addr:$a)>;
+def : LoongArchPat<(atomic_store_32 addrimm14lsl2:$a, GPR32:$v),
+      (STPTR_W32 GPR32:$v, addrimm14lsl2:$a)>;
+def : LoongArchPat<(atomic_store_32 addr:$a, GPR32:$v),
+      (ST_W32 GPR32:$v, addr:$a)>;
+
+def : LoongArchPat<(LoongArchDBAR (i32 immz)),
+              (DBAR 0)>;
+
+def : LoongArchPat<(i32 (extloadi1  addr:$src)), (LD_BU32 addr:$src)>;
+def : LoongArchPat<(i32 (extloadi8  addr:$src)), (LD_BU32 addr:$src)>;
+def : LoongArchPat<(i32 (extloadi16 addr:$src)), (LD_HU32 addr:$src)>;
+
+def : LoongArchPat<(store (i32 0), addr:$dst), (ST_W32 ZERO, addr:$dst)>;
+
+// Patterns for loads/stores with a reg+imm operand.
+let AddedComplexity = 40 in {
+  def : LoadRegImmPat<LD_B32, i32, sextloadi8>;
+  def : LoadRegImmPat<LD_H32, i32, sextloadi16>;
+  def : LoadRegImmPat<LD_W32, i32, load>;
+  def : LoadRegImmPat<LD_BU32, i32, zextloadi8>;
+  def : LoadRegImmPat<LD_HU32, i32, zextloadi16>;
+  def : StoreRegImmPat<ST_B32, i32, truncstorei8>;
+  def : StoreRegImmPat<ST_H32, i32, truncstorei16>;
+  def : StoreRegImmPat<ST_W32, i32, store>;
+
+  def : LoadRegImm14Lsl2Pat<LDPTR_W32, i32, load>;
+  def : StoreRegImm14Lsl2Pat<STPTR_W32, i32, store>;
+}
+
+let isCall=1, isCTI=1, Defs = [RA] in {
+
+  class JumpLinkRegPseudo<RegisterOperand RO, Instruction JIRLRInst,
+                          Register RetReg, RegisterOperand ResRO = RO>:
+    LoongArchPseudo<(outs), (ins RO:$rj), [(LoongArchJmpLink RO:$rj)]>,
+    PseudoInstExpansion<(JIRLRInst RetReg, ResRO:$rj)> {
+    let hasPostISelHook = 1;
+  }
+
+  class JumpLinkReg<string opstr, RegisterOperand RO>:
+    InstForm<(outs RO:$rd), (ins RO:$rj), !strconcat(opstr, "\t$rd, $rj, 0"),
+             [], FrmR, opstr> {
+    let hasPostISelHook = 1;
+  }
+
+}
+
+def JIRLR : JumpLinkReg<"jirl", GPR32Opnd>, R2_IMM16JIRL {
+  let offs16 = 0;
+}
+def JIRLRPseudo : JumpLinkRegPseudo<GPR64Opnd, JIRLR, RA, GPR32Opnd>;
+
+class BrindRegPseudo<RegisterOperand RO, Instruction JIRLRInst,
+                     Register RetReg, RegisterOperand ResRO = RO>:
+  LoongArchPseudo<(outs), (ins RO:$rj), [(brind RO:$rj)]>,
+  PseudoInstExpansion<(JIRLRInst RetReg, ResRO:$rj)> {
+  let isTerminator=1;
+  let isBarrier=1;
+  let isBranch = 1;
+  let isIndirectBranch = 1;
+  bit isCTI = 1;
+}
+
+def JIRLRBRIND : BrindRegPseudo<GPR64Opnd, JIRLR, ZERO, GPR32Opnd>;
+
+def : LoongArchPat<(addc GPR32:$src, immSExt12:$imm),
+                   (ADDI_W GPR32:$src, imm:$imm)>;
+
+defm : SeteqPats<GPR32, SLTUI32, XOR32, SLTU32, ZERO>;
+defm : SetlePats<GPR32, XORI32, SLT32, SLTU32>;
+defm : SetgtPats<GPR32, SLT32, SLTU32>;
+defm : SetgePats<GPR32, XORI32, SLT32, SLTU32>;
+defm : SetgeImmPats<GPR32, XORI32, SLTI32, SLTUI32>;
diff --git a/llvm/lib/Target/LoongArch/LoongArchAnalyzeImmediate.cpp b/llvm/lib/Target/LoongArch/LoongArchAnalyzeImmediate.cpp
new file mode 100644
index 000000000000..78c80e5fb50d
--- /dev/null
+++ b/llvm/lib/Target/LoongArch/LoongArchAnalyzeImmediate.cpp
@@ -0,0 +1,134 @@
+//===- LoongArchAnalyzeImmediate.cpp - Analyze Immediates ----------------------===//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+
+#include "LoongArchAnalyzeImmediate.h"
+#include "LoongArch.h"
+#include "llvm/Support/MathExtras.h"
+#include <cassert>
+#include <cstdint>
+#include <iterator>
+
+using namespace llvm;
+
+LoongArchAnalyzeImmediate::Inst::Inst(unsigned O, unsigned I) : Opc(O), ImmOpnd(I) {}
+
+// Add I to the instruction sequences.
+void LoongArchAnalyzeImmediate::AddInstr(InstSeqLs &SeqLs, const Inst &I) {
+  // Add an instruction seqeunce consisting of just I.
+  if (SeqLs.empty()) {
+    SeqLs.push_back(InstSeq(1, I));
+    return;
+  }
+
+  for (InstSeqLs::iterator Iter = SeqLs.begin(); Iter != SeqLs.end(); ++Iter)
+    Iter->push_back(I);
+}
+
+void LoongArchAnalyzeImmediate::GetInstSeqLsADDI(uint64_t Imm, unsigned RemSize,
+                                                 InstSeqLs &SeqLs) {
+  GetInstSeqLs((Imm + 0x800ULL) & 0xfffffffffffff000ULL, RemSize, SeqLs);
+  AddInstr(SeqLs, Inst(ADDI, Imm & 0xfffULL));
+}
+
+void LoongArchAnalyzeImmediate::GetInstSeqLsORI(uint64_t Imm, unsigned RemSize,
+                                                InstSeqLs &SeqLs) {
+  GetInstSeqLs(Imm & 0xfffffffffffff000ULL, RemSize, SeqLs);
+  AddInstr(SeqLs, Inst(ORI, Imm & 0xfffULL));
+}
+
+void LoongArchAnalyzeImmediate::GetInstSeqLsSLLI(uint64_t Imm, unsigned RemSize,
+                                                 InstSeqLs &SeqLs) {
+  unsigned Shamt = countTrailingZeros(Imm);
+  GetInstSeqLs(Imm >> Shamt, RemSize - Shamt, SeqLs);
+  AddInstr(SeqLs, Inst(SLLI, Shamt));
+}
+
+void LoongArchAnalyzeImmediate::GetInstSeqLs(uint64_t Imm, unsigned RemSize,
+                                             InstSeqLs &SeqLs) {
+  uint64_t MaskedImm = Imm & (0xffffffffffffffffULL >> (64 - Size));
+
+  // Do nothing if Imm is 0.
+  if (!MaskedImm)
+    return;
+
+  // A single ADDI will do if RemSize <= 12.
+  if (RemSize <= 12) {
+    AddInstr(SeqLs, Inst(ORI, MaskedImm));
+    return;
+  }
+
+  // Shift if the lower 12-bit is cleared.
+  if (!(Imm & 0xfff)) {
+    GetInstSeqLsSLLI(Imm, RemSize, SeqLs);
+    return;
+  }
+
+  GetInstSeqLsADDI(Imm, RemSize, SeqLs);
+
+  // If bit 11 is cleared, it doesn't make a difference whether the last
+  // instruction is an ADDI or ORI. In that case, do not call GetInstSeqLsORI.
+  if (Imm & 0x800) {
+    InstSeqLs SeqLsORI;
+    GetInstSeqLsORI(Imm, RemSize, SeqLsORI);
+    SeqLs.append(std::make_move_iterator(SeqLsORI.begin()),
+                 std::make_move_iterator(SeqLsORI.end()));
+  }
+}
+
+// Replace a ADDI & SLLI pair with a LU12I.
+// e.g. the following two instructions
+//  ADDI 0x0111
+//  SLLI 18
+// are replaced with
+//  LU12I 0x4440
+void LoongArchAnalyzeImmediate::ReplaceADDISLLIWithLU12I(InstSeq &Seq) {
+  //TODO: implement
+}
+
+void LoongArchAnalyzeImmediate::GetShortestSeq(InstSeqLs &SeqLs, InstSeq &Insts) {
+  InstSeqLs::iterator ShortestSeq = SeqLs.end();
+  // The length of an instruction sequence is at most 11.
+  unsigned ShortestLength = 12;
+
+  for (InstSeqLs::iterator S = SeqLs.begin(); S != SeqLs.end(); ++S) {
+    ReplaceADDISLLIWithLU12I(*S);
+    assert(S->size() <= 11);
+
+    if (S->size() < ShortestLength) {
+      ShortestSeq = S;
+      ShortestLength = S->size();
+    }
+  }
+
+  Insts.clear();
+  Insts.append(ShortestSeq->begin(), ShortestSeq->end());
+}
+
+const LoongArchAnalyzeImmediate::InstSeq
+&LoongArchAnalyzeImmediate::Analyze(uint64_t Imm, unsigned Size,
+                                    bool LastInstrIsADDI) {
+  this->Size = Size;
+  //TODO:loongarch32
+  ADDI = LoongArch::ADDI_D;
+  ORI = LoongArch::ORI;
+  SLLI = LoongArch::SLLI_D;
+  LU12I = LoongArch::LU12I_W;
+
+  InstSeqLs SeqLs;
+
+  // Get the list of instruction sequences.
+  if (LastInstrIsADDI | !Imm)
+    GetInstSeqLsADDI(Imm, Size, SeqLs);
+  else
+    GetInstSeqLs(Imm, Size, SeqLs);
+
+  // Set Insts to the shortest instruction sequence.
+  GetShortestSeq(SeqLs, Insts);
+
+  return Insts;
+}
diff --git a/llvm/lib/Target/LoongArch/LoongArchAnalyzeImmediate.h b/llvm/lib/Target/LoongArch/LoongArchAnalyzeImmediate.h
new file mode 100644
index 000000000000..36ac7266cf69
--- /dev/null
+++ b/llvm/lib/Target/LoongArch/LoongArchAnalyzeImmediate.h
@@ -0,0 +1,66 @@
+//===- LoongArchAnalyzeImmediate.h - Analyze Immediates -------------*- C++ -*--===//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef LLVM_LIB_TARGET_LOONGARCH_LOONGARCHANALYZEIMMEDIATE_H
+#define LLVM_LIB_TARGET_LOONGARCH_LOONGARCHANALYZEIMMEDIATE_H
+
+#include "llvm/ADT/SmallVector.h"
+#include <cstdint>
+
+namespace llvm {
+
+  class LoongArchAnalyzeImmediate {
+  public:
+    struct Inst {
+      unsigned Opc, ImmOpnd;
+
+      Inst(unsigned Opc, unsigned ImmOpnd);
+    };
+    using InstSeq = SmallVector<Inst, 11>;
+
+    /// Analyze - Get an instruction sequence to load immediate Imm. The last
+    /// instruction in the sequence must be an ADDI_{W/D} if LastInstrIsADDI is
+    /// true;
+    const InstSeq &Analyze(uint64_t Imm, unsigned Size, bool LastInstrIsADDI);
+
+  private:
+    using InstSeqLs = SmallVector<InstSeq, 9>;
+
+    /// AddInstr - Add I to all instruction sequences in SeqLs.
+    void AddInstr(InstSeqLs &SeqLs, const Inst &I);
+
+    /// GetInstSeqLsADDI - Get instruction sequences which end with an ADDI to
+    /// load immediate Imm
+    void GetInstSeqLsADDI(uint64_t Imm, unsigned RemSize, InstSeqLs &SeqLs);
+
+    /// GetInstSeqLsORI - Get instrutcion sequences which end with an ORI to
+    /// load immediate Imm
+    void GetInstSeqLsORI(uint64_t Imm, unsigned RemSize, InstSeqLs &SeqLs);
+
+    /// GetInstSeqLsSLLI - Get instruction sequences which end with a SLLI to
+    /// load immediate Imm
+    void GetInstSeqLsSLLI(uint64_t Imm, unsigned RemSize, InstSeqLs &SeqLs);
+
+    /// GetInstSeqLs - Get instruction sequences to load immediate Imm.
+    void GetInstSeqLs(uint64_t Imm, unsigned RemSize, InstSeqLs &SeqLs);
+
+    /// ReplaceADDISLLIWithLU12I - Replace an ADDI & SLLI pair with a LU12I.
+    void ReplaceADDISLLIWithLU12I(InstSeq &Seq);
+
+    /// GetShortestSeq - Find the shortest instruction sequence in SeqLs and
+    /// return it in Insts.
+    void GetShortestSeq(InstSeqLs &SeqLs, InstSeq &Insts);
+
+    unsigned Size;
+    unsigned ADDI, ORI, SLLI, LU12I;
+    InstSeq Insts;
+  };
+
+} // end namespace llvm
+
+#endif // LLVM_LIB_TARGET_LOONGARCH_LOONGARCHANALYZEIMMEDIATE_H
diff --git a/llvm/lib/Target/LoongArch/LoongArchAsmPrinter.cpp b/llvm/lib/Target/LoongArch/LoongArchAsmPrinter.cpp
new file mode 100644
index 000000000000..76ba9577fc76
--- /dev/null
+++ b/llvm/lib/Target/LoongArch/LoongArchAsmPrinter.cpp
@@ -0,0 +1,581 @@
+//===- LoongArchAsmPrinter.cpp - LoongArch LLVM Assembly Printer --------------------===//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+//
+// This file contains a printer that converts from our internal representation
+// of machine-dependent LLVM code to GAS-format LoongArch assembly language.
+//
+//===----------------------------------------------------------------------===//
+
+#include "LoongArchAsmPrinter.h"
+#include "MCTargetDesc/LoongArchInstPrinter.h"
+#include "MCTargetDesc/LoongArchABIInfo.h"
+#include "MCTargetDesc/LoongArchBaseInfo.h"
+#include "MCTargetDesc/LoongArchMCTargetDesc.h"
+#include "LoongArch.h"
+#include "LoongArchMCInstLower.h"
+#include "LoongArchMachineFunction.h"
+#include "LoongArchSubtarget.h"
+#include "LoongArchTargetMachine.h"
+#include "LoongArchTargetStreamer.h"
+#include "llvm/ADT/SmallString.h"
+#include "llvm/ADT/StringRef.h"
+#include "llvm/ADT/Triple.h"
+#include "llvm/ADT/Twine.h"
+#include "llvm/BinaryFormat/ELF.h"
+#include "llvm/CodeGen/MachineBasicBlock.h"
+#include "llvm/CodeGen/MachineConstantPool.h"
+#include "llvm/CodeGen/MachineFrameInfo.h"
+#include "llvm/CodeGen/MachineFunction.h"
+#include "llvm/CodeGen/MachineInstr.h"
+#include "llvm/CodeGen/MachineJumpTableInfo.h"
+#include "llvm/CodeGen/MachineOperand.h"
+#include "llvm/CodeGen/TargetRegisterInfo.h"
+#include "llvm/CodeGen/TargetSubtargetInfo.h"
+#include "llvm/IR/Attributes.h"
+#include "llvm/IR/BasicBlock.h"
+#include "llvm/IR/DataLayout.h"
+#include "llvm/IR/Function.h"
+#include "llvm/IR/InlineAsm.h"
+#include "llvm/IR/Instructions.h"
+#include "llvm/MC/MCContext.h"
+#include "llvm/MC/MCExpr.h"
+#include "llvm/MC/MCInst.h"
+#include "llvm/MC/MCInstBuilder.h"
+#include "llvm/MC/MCObjectFileInfo.h"
+#include "llvm/MC/MCSectionELF.h"
+#include "llvm/MC/MCSymbol.h"
+#include "llvm/MC/MCSymbolELF.h"
+#include "llvm/Support/Casting.h"
+#include "llvm/Support/ErrorHandling.h"
+#include "llvm/Support/TargetRegistry.h"
+#include "llvm/Support/raw_ostream.h"
+#include "llvm/Target/TargetMachine.h"
+#include <cassert>
+#include <cstdint>
+#include <map>
+#include <memory>
+#include <string>
+#include <vector>
+
+using namespace llvm;
+
+#define DEBUG_TYPE "loongarch-asm-printer"
+
+LoongArchTargetStreamer &LoongArchAsmPrinter::getTargetStreamer() const {
+  return static_cast<LoongArchTargetStreamer &>(*OutStreamer->getTargetStreamer());
+}
+
+bool LoongArchAsmPrinter::runOnMachineFunction(MachineFunction &MF) {
+  Subtarget = &MF.getSubtarget<LoongArchSubtarget>();
+
+  LoongArchFI = MF.getInfo<LoongArchFunctionInfo>();
+  MCP = MF.getConstantPool();
+
+  AsmPrinter::runOnMachineFunction(MF);
+
+  emitXRayTable();
+
+  return true;
+}
+
+bool LoongArchAsmPrinter::lowerOperand(const MachineOperand &MO, MCOperand &MCOp) {
+  MCOp = MCInstLowering.LowerOperand(MO);
+  return MCOp.isValid();
+}
+
+#include "LoongArchGenMCPseudoLowering.inc"
+
+// Lower PseudoReturn/PseudoIndirectBranch/PseudoIndirectBranch64 to
+// JIRL as appropriate for the target.
+void LoongArchAsmPrinter::emitPseudoIndirectBranch(MCStreamer &OutStreamer,
+                                              const MachineInstr *MI) {
+  bool HasLinkReg = false;
+  MCInst TmpInst0;
+  TmpInst0.setOpcode(LoongArch::JIRL);
+  HasLinkReg = true;
+
+  MCOperand MCOp;
+
+  if (HasLinkReg) {
+    unsigned ZeroReg = Subtarget->is64Bit() ? LoongArch::ZERO_64 : LoongArch::ZERO;
+    TmpInst0.addOperand(MCOperand::createReg(ZeroReg));
+  }
+
+  lowerOperand(MI->getOperand(0), MCOp);
+  TmpInst0.addOperand(MCOp);
+
+  TmpInst0.addOperand(MCOperand::createImm(0));
+
+  EmitToStreamer(OutStreamer, TmpInst0);
+}
+
+void LoongArchAsmPrinter::emitInstruction(const MachineInstr *MI) {
+  LoongArchTargetStreamer &TS = getTargetStreamer();
+  unsigned Opc = MI->getOpcode();
+  TS.forbidModuleDirective();
+
+  if (MI->isDebugValue()) {
+    SmallString<128> Str;
+    raw_svector_ostream OS(Str);
+
+    PrintDebugValueComment(MI, OS);
+    return;
+  }
+  if (MI->isDebugLabel())
+    return;
+  // If we just ended a constant pool, mark it as such.
+  OutStreamer->emitDataRegion(MCDR_DataRegionEnd);
+  InConstantPool = false;
+
+  switch (Opc) {
+  case LoongArch::PATCHABLE_FUNCTION_ENTER:
+    LowerPATCHABLE_FUNCTION_ENTER(*MI);
+    return;
+  case LoongArch::PATCHABLE_FUNCTION_EXIT:
+    LowerPATCHABLE_FUNCTION_EXIT(*MI);
+    return;
+  case LoongArch::PATCHABLE_TAIL_CALL:
+    LowerPATCHABLE_TAIL_CALL(*MI);
+    return;
+  }
+  MachineBasicBlock::const_instr_iterator I = MI->getIterator();
+  MachineBasicBlock::const_instr_iterator E = MI->getParent()->instr_end();
+
+  do {
+    // Do any auto-generated pseudo lowerings.
+    if (emitPseudoExpansionLowering(*OutStreamer, &*I))
+      continue;
+    if (I->getOpcode() == LoongArch::PseudoReturn ||
+        I->getOpcode() == LoongArch::PseudoReturn64){
+      emitPseudoIndirectBranch(*OutStreamer, &*I);
+      continue;
+    }
+
+    // Some instructions are marked as pseudo right now which
+    // would make the test fail for the wrong reason but
+    // that will be fixed soon. We need this here because we are
+    // removing another test for this situation downstream in the
+    // callchain.
+    //
+    if (I->isPseudo()
+        && !isLongBranchPseudo(I->getOpcode()))
+      llvm_unreachable("Pseudo opcode found in EmitInstruction()");
+
+    MCInst TmpInst0;
+    MCInstLowering.Lower(&*I, TmpInst0);
+    EmitToStreamer(*OutStreamer, TmpInst0);
+  } while ((++I != E) && I->isInsideBundle());
+}
+
+//===----------------------------------------------------------------------===//
+//
+//  LoongArch Asm Directives
+//
+//
+//===----------------------------------------------------------------------===//
+
+//===----------------------------------------------------------------------===//
+// Set directives
+//===----------------------------------------------------------------------===//
+
+/// Emit Set directives.
+const char *LoongArchAsmPrinter::getCurrentABIString() const {
+  switch (static_cast<LoongArchTargetMachine &>(TM).getABI().GetEnumValue()) {
+  case LoongArchABIInfo::ABI::LP32:  return "abilp32";
+  case LoongArchABIInfo::ABI::LPX32:  return "abilpx32";
+  case LoongArchABIInfo::ABI::LP64D:  return "abilp64d";
+  default: llvm_unreachable("Unknown LoongArch ABI");
+  }
+}
+
+void LoongArchAsmPrinter::emitFunctionEntryLabel() {
+
+  OutStreamer->emitLabel(CurrentFnSym);
+
+}
+
+/// EmitFunctionBodyStart - Targets can override this to emit stuff before
+/// the first basic block in the function.
+void LoongArchAsmPrinter::emitFunctionBodyStart() {
+
+  MCInstLowering.Initialize(&MF->getContext());
+}
+
+/// EmitFunctionBodyEnd - Targets can override this to emit stuff after
+/// the last basic block in the function.
+void LoongArchAsmPrinter::emitFunctionBodyEnd() {
+
+  // Make sure to terminate any constant pools that were at the end
+  // of the function.
+  if (!InConstantPool)
+    return;
+  InConstantPool = false;
+  OutStreamer->emitDataRegion(MCDR_DataRegionEnd);
+}
+
+void LoongArchAsmPrinter::emitBasicBlockEnd(const MachineBasicBlock &MBB) {
+  AsmPrinter::emitBasicBlockEnd(MBB);
+}
+
+/// isBlockOnlyReachableByFallthough - Return true if the basic block has
+/// exactly one predecessor and the control transfer mechanism between
+/// the predecessor and this block is a fall-through.
+bool LoongArchAsmPrinter::isBlockOnlyReachableByFallthrough(const MachineBasicBlock*
+                                                       MBB) const {
+  // The predecessor has to be immediately before this block.
+  const MachineBasicBlock *Pred = *MBB->pred_begin();
+
+  // If the predecessor is a switch statement, assume a jump table
+  // implementation, so it is not a fall through.
+  if (const BasicBlock *bb = Pred->getBasicBlock())
+    if (isa<SwitchInst>(bb->getTerminator()))
+      return false;
+
+  // Check default implementation
+  return AsmPrinter::isBlockOnlyReachableByFallthrough(MBB);
+}
+
+// Print out an operand for an inline asm expression.
+bool LoongArchAsmPrinter::PrintAsmOperand(const MachineInstr *MI,
+    unsigned OpNum, const char *ExtraCode, raw_ostream &O) {
+  // Does this asm operand have a single letter operand modifier?
+  if (ExtraCode && ExtraCode[0]) {
+    if (ExtraCode[1] != 0) return true; // Unknown modifier.
+
+    const MachineOperand &MO = MI->getOperand(OpNum);
+    switch (ExtraCode[0]) {
+    default:
+      // See if this is a generic print operand
+      return AsmPrinter::PrintAsmOperand(MI,OpNum,ExtraCode,O);
+    case 'X': // hex const int
+      if ((MO.getType()) != MachineOperand::MO_Immediate)
+        return true;
+      O << "0x" << Twine::utohexstr(MO.getImm());
+      return false;
+    case 'x': // hex const int (low 16 bits)
+      if ((MO.getType()) != MachineOperand::MO_Immediate)
+        return true;
+      O << "0x" << Twine::utohexstr(MO.getImm() & 0xffff);
+      return false;
+    case 'd': // decimal const int
+      if ((MO.getType()) != MachineOperand::MO_Immediate)
+        return true;
+      O << MO.getImm();
+      return false;
+    case 'm': // decimal const int minus 1
+      if ((MO.getType()) != MachineOperand::MO_Immediate)
+        return true;
+      O << MO.getImm() - 1;
+      return false;
+    case 'y': // exact log2
+      if ((MO.getType()) != MachineOperand::MO_Immediate)
+        return true;
+      if (!isPowerOf2_64(MO.getImm()))
+        return true;
+      O << Log2_64(MO.getImm());
+      return false;
+    case 'z':
+      // $0 if zero, regular printing otherwise
+      if (MO.getType() == MachineOperand::MO_Immediate && MO.getImm() == 0) {
+        O << "$0";
+        return false;
+      }
+      // If not, call printOperand as normal.
+      break;
+    case 'D': // Second part of a double word register operand
+    case 'L': // Low order register of a double word register operand
+    case 'M': // High order register of a double word register operand
+    {
+      if (OpNum == 0)
+        return true;
+      const MachineOperand &FlagsOP = MI->getOperand(OpNum - 1);
+      if (!FlagsOP.isImm())
+        return true;
+      unsigned Flags = FlagsOP.getImm();
+      unsigned NumVals = InlineAsm::getNumOperandRegisters(Flags);
+      // Number of registers represented by this operand. We are looking
+      // for 2 for 32 bit mode and 1 for 64 bit mode.
+      if (NumVals != 2) {
+        if (Subtarget->is64Bit() && NumVals == 1 && MO.isReg()) {
+          unsigned Reg = MO.getReg();
+          O << '$' << LoongArchInstPrinter::getRegisterName(Reg);
+          return false;
+        }
+        return true;
+      }
+
+      unsigned RegOp = OpNum;
+      if (!Subtarget->is64Bit()){
+        // Endianness reverses which register holds the high or low value
+        // between M and L.
+        switch(ExtraCode[0]) {
+        case 'M':
+          RegOp = OpNum + 1;
+          break;
+        case 'L':
+          RegOp = OpNum;
+          break;
+        case 'D': // Always the second part
+          RegOp = OpNum + 1;
+        }
+        if (RegOp >= MI->getNumOperands())
+          return true;
+        const MachineOperand &MO = MI->getOperand(RegOp);
+        if (!MO.isReg())
+          return true;
+        unsigned Reg = MO.getReg();
+        O << '$' << LoongArchInstPrinter::getRegisterName(Reg);
+        return false;
+      }
+      break;
+    }
+    }
+  }
+
+  printOperand(MI, OpNum, O);
+  return false;
+}
+
+bool LoongArchAsmPrinter::PrintAsmMemoryOperand(const MachineInstr *MI,
+                                                unsigned OpNum,
+                                                const char *ExtraCode,
+                                                raw_ostream &O) {
+  assert(OpNum + 1 < MI->getNumOperands() && "Insufficient operands");
+  const MachineOperand &BaseMO = MI->getOperand(OpNum);
+  const MachineOperand &OffsetMO = MI->getOperand(OpNum + 1);
+  assert(BaseMO.isReg() && "Unexpected base pointer for inline asm memory operand.");
+  assert(OffsetMO.isImm() && "Unexpected offset for inline asm memory operand.");
+  int Offset = OffsetMO.getImm();
+
+  // Currently we are expecting either no ExtraCode or 'D','M','L'.
+  if (ExtraCode) {
+    switch (ExtraCode[0]) {
+    case 'D':
+    case 'M':
+      Offset += 4;
+      break;
+    case 'L':
+      break;
+    default:
+      return true; // Unknown modifier.
+    }
+  }
+
+  O << "$" << LoongArchInstPrinter::getRegisterName(BaseMO.getReg()) << ", " << Offset;
+
+  return false;
+}
+
+void LoongArchAsmPrinter::printOperand(const MachineInstr *MI, int opNum,
+                                  raw_ostream &O) {
+  const MachineOperand &MO = MI->getOperand(opNum);
+
+  switch (MO.getType()) {
+    case MachineOperand::MO_Register:
+      O << '$'
+        << StringRef(LoongArchInstPrinter::getRegisterName(MO.getReg())).lower();
+      break;
+
+    case MachineOperand::MO_Immediate:
+      O << MO.getImm();
+      break;
+
+    case MachineOperand::MO_MachineBasicBlock:
+      MO.getMBB()->getSymbol()->print(O, MAI);
+      return;
+
+    case MachineOperand::MO_GlobalAddress:
+      getSymbol(MO.getGlobal())->print(O, MAI);
+      break;
+
+    case MachineOperand::MO_BlockAddress: {
+      MCSymbol *BA = GetBlockAddressSymbol(MO.getBlockAddress());
+      O << BA->getName();
+      break;
+    }
+
+    case MachineOperand::MO_ConstantPoolIndex:
+      O << getDataLayout().getPrivateGlobalPrefix() << "CPI"
+        << getFunctionNumber() << "_" << MO.getIndex();
+      if (MO.getOffset())
+        O << "+" << MO.getOffset();
+      break;
+
+    default:
+      llvm_unreachable("<unknown operand type>");
+  }
+}
+
+void LoongArchAsmPrinter::
+printMemOperand(const MachineInstr *MI, int opNum, raw_ostream &O) {
+  // Load/Store memory operands -- imm($reg)
+  // If PIC target the target is loaded as the
+  // pattern lw $25,%call16($28)
+
+  printOperand(MI, opNum+1, O);
+  O << "(";
+  printOperand(MI, opNum, O);
+  O << ")";
+}
+
+void LoongArchAsmPrinter::
+printMemOperandEA(const MachineInstr *MI, int opNum, raw_ostream &O) {
+  // when using stack locations for not load/store instructions
+  // print the same way as all normal 3 operand instructions.
+  printOperand(MI, opNum, O);
+  O << ", ";
+  printOperand(MI, opNum+1, O);
+}
+
+void LoongArchAsmPrinter::
+printRegisterList(const MachineInstr *MI, int opNum, raw_ostream &O) {
+  for (int i = opNum, e = MI->getNumOperands(); i != e; ++i) {
+    if (i != opNum) O << ", ";
+    printOperand(MI, i, O);
+  }
+}
+
+void LoongArchAsmPrinter::emitStartOfAsmFile(Module &M) {
+  LoongArchTargetStreamer &TS = getTargetStreamer();
+
+  // LoongArchTargetStreamer has an initialization order problem when emitting an
+  // object file directly (see LoongArchTargetELFStreamer for full details). Work
+  // around it by re-initializing the PIC state here.
+  TS.setPic(OutContext.getObjectFileInfo()->isPositionIndependent());
+
+  // Compute LoongArch architecture attributes based on the default subtarget
+  // that we'd have constructed. Module level directives aren't LTO
+  // clean anyhow.
+  // FIXME: For ifunc related functions we could iterate over and look
+  // for a feature string that doesn't match the default one.
+  const Triple &TT = TM.getTargetTriple();
+  StringRef CPU = LoongArch_MC::selectLoongArchCPU(TT, TM.getTargetCPU());
+  StringRef FS = TM.getTargetFeatureString();
+  const LoongArchTargetMachine &MTM = static_cast<const LoongArchTargetMachine &>(TM);
+  const LoongArchSubtarget STI(TT, CPU, FS, MTM, None);
+
+  const LoongArchABIInfo &ABI = MTM.getABI();
+
+  // Tell the assembler which ABI we are using
+  std::string SectionName = std::string(".mdebug.") + getCurrentABIString();
+  OutStreamer->SwitchSection(
+      OutContext.getELFSection(SectionName, ELF::SHT_PROGBITS, 0));
+
+  // TODO: handle O64 ABI
+
+  TS.updateABIInfo(STI);
+
+  // We should always emit a '.module fp=...' but binutils 2.31 does not accept
+  // it. We therefore emit it when it contradicts the ABI defaults (-mfp64)
+  // and omit it otherwise.
+  if ((ABI.IsLP32() && (STI.isFP64bit())) ||
+      STI.useSoftFloat())
+    TS.emitDirectiveModuleFP();
+}
+
+void LoongArchAsmPrinter::emitInlineAsmStart() const {
+
+  OutStreamer->AddBlankLine();
+}
+
+void LoongArchAsmPrinter::emitInlineAsmEnd(const MCSubtargetInfo &StartInfo,
+                                      const MCSubtargetInfo *EndInfo) const {
+  OutStreamer->AddBlankLine();
+}
+
+void LoongArchAsmPrinter::EmitInstrReg(const MCSubtargetInfo &STI, unsigned Opcode,
+                                  unsigned Reg) {
+  MCInst I;
+  I.setOpcode(Opcode);
+  I.addOperand(MCOperand::createReg(Reg));
+  OutStreamer->emitInstruction(I, STI);
+}
+
+void LoongArchAsmPrinter::EmitInstrRegReg(const MCSubtargetInfo &STI,
+                                     unsigned Opcode, unsigned Reg1,
+                                     unsigned Reg2) {
+  MCInst I;
+  //
+  // Because of the current td files for LoongArch32, the operands for MTC1
+  // appear backwards from their normal assembly order. It's not a trivial
+  // change to fix this in the td file so we adjust for it here.
+  //
+  if (Opcode == LoongArch::MOVGR2FR_W) {
+    unsigned Temp = Reg1;
+    Reg1 = Reg2;
+    Reg2 = Temp;
+  }
+  I.setOpcode(Opcode);
+  I.addOperand(MCOperand::createReg(Reg1));
+  I.addOperand(MCOperand::createReg(Reg2));
+  OutStreamer->emitInstruction(I, STI);
+}
+
+void LoongArchAsmPrinter::EmitInstrRegRegReg(const MCSubtargetInfo &STI,
+                                        unsigned Opcode, unsigned Reg1,
+                                        unsigned Reg2, unsigned Reg3) {
+  MCInst I;
+  I.setOpcode(Opcode);
+  I.addOperand(MCOperand::createReg(Reg1));
+  I.addOperand(MCOperand::createReg(Reg2));
+  I.addOperand(MCOperand::createReg(Reg3));
+  OutStreamer->emitInstruction(I, STI);
+}
+
+void LoongArchAsmPrinter::EmitMovFPIntPair(const MCSubtargetInfo &STI,
+                                      unsigned MovOpc, unsigned Reg1,
+                                      unsigned Reg2, unsigned FPReg1,
+                                      unsigned FPReg2, bool LE) {
+  if (!LE) {
+    unsigned temp = Reg1;
+    Reg1 = Reg2;
+    Reg2 = temp;
+  }
+  EmitInstrRegReg(STI, MovOpc, Reg1, FPReg1);
+  EmitInstrRegReg(STI, MovOpc, Reg2, FPReg2);
+}
+
+void LoongArchAsmPrinter::emitEndOfAsmFile(Module &M) {
+  // return to the text section
+  OutStreamer->SwitchSection(OutContext.getObjectFileInfo()->getTextSection());
+}
+
+void LoongArchAsmPrinter::EmitSled(const MachineInstr &MI, SledKind Kind) {
+// Now this is unimplemented.
+}
+
+void LoongArchAsmPrinter::LowerPATCHABLE_FUNCTION_ENTER(const MachineInstr &MI) {
+  EmitSled(MI, SledKind::FUNCTION_ENTER);
+}
+
+void LoongArchAsmPrinter::LowerPATCHABLE_FUNCTION_EXIT(const MachineInstr &MI) {
+  EmitSled(MI, SledKind::FUNCTION_EXIT);
+}
+
+void LoongArchAsmPrinter::LowerPATCHABLE_TAIL_CALL(const MachineInstr &MI) {
+  EmitSled(MI, SledKind::TAIL_CALL);
+}
+
+void LoongArchAsmPrinter::PrintDebugValueComment(const MachineInstr *MI,
+                                           raw_ostream &OS) {
+  // TODO: implement
+}
+
+bool LoongArchAsmPrinter::isLongBranchPseudo(int Opcode) const {
+    return (Opcode == LoongArch::LONG_BRANCH_ADDIW
+          || Opcode == LoongArch::LONG_BRANCH_ADDIW2Op
+          || Opcode == LoongArch::LONG_BRANCH_ADDID
+          || Opcode == LoongArch::LONG_BRANCH_ADDID2Op
+          || Opcode == LoongArch::LONG_BRANCH_PCADDU12I);
+}
+
+// Force static initialization.
+extern "C" LLVM_EXTERNAL_VISIBILITY void LLVMInitializeLoongArchAsmPrinter() {
+  RegisterAsmPrinter<LoongArchAsmPrinter> X(getTheLoongArch32Target());
+  RegisterAsmPrinter<LoongArchAsmPrinter> A(getTheLoongArch64Target());
+}
diff --git a/llvm/lib/Target/LoongArch/LoongArchAsmPrinter.h b/llvm/lib/Target/LoongArch/LoongArchAsmPrinter.h
new file mode 100644
index 000000000000..49d98895fdfa
--- /dev/null
+++ b/llvm/lib/Target/LoongArch/LoongArchAsmPrinter.h
@@ -0,0 +1,135 @@
+//===- LoongArchAsmPrinter.h - LoongArch LLVM Assembly Printer -----------*- C++ -*--===//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+//
+// LoongArch Assembly printer class.
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef LLVM_LIB_TARGET_LOONGARCH_LOONGARCHASMPRINTER_H
+#define LLVM_LIB_TARGET_LOONGARCH_LOONGARCHASMPRINTER_H
+
+#include "LoongArchMCInstLower.h"
+#include "LoongArchSubtarget.h"
+#include "llvm/CodeGen/AsmPrinter.h"
+#include "llvm/MC/MCStreamer.h"
+#include "llvm/Support/Compiler.h"
+#include <algorithm>
+#include <map>
+#include <memory>
+
+namespace llvm {
+
+class MCOperand;
+class MCSubtargetInfo;
+class MCSymbol;
+class MachineBasicBlock;
+class MachineConstantPool;
+class MachineFunction;
+class MachineInstr;
+class MachineOperand;
+class LoongArchFunctionInfo;
+class LoongArchTargetStreamer;
+class Module;
+class raw_ostream;
+class TargetMachine;
+
+class LLVM_LIBRARY_VISIBILITY LoongArchAsmPrinter : public AsmPrinter {
+  LoongArchTargetStreamer &getTargetStreamer() const;
+
+  void EmitInstrWithMacroNoAT(const MachineInstr *MI);
+
+  //===------------------------------------------------------------------===//
+  // XRay implementation
+  //===------------------------------------------------------------------===//
+
+public:
+  // XRay-specific lowering for LoongArch.
+  void LowerPATCHABLE_FUNCTION_ENTER(const MachineInstr &MI);
+  void LowerPATCHABLE_FUNCTION_EXIT(const MachineInstr &MI);
+  void LowerPATCHABLE_TAIL_CALL(const MachineInstr &MI);
+
+private:
+  /// MCP - Keep a pointer to constantpool entries of the current
+  /// MachineFunction.
+  const MachineConstantPool *MCP = nullptr;
+
+  /// InConstantPool - Maintain state when emitting a sequence of constant
+  /// pool entries so we can properly mark them as data regions.
+  bool InConstantPool = false;
+
+  void EmitSled(const MachineInstr &MI, SledKind Kind);
+
+  // tblgen'erated function.
+  bool emitPseudoExpansionLowering(MCStreamer &OutStreamer,
+                                   const MachineInstr *MI);
+
+  // Emit PseudoReturn, PseudoReturn64, PseudoIndirectBranch,
+  // and PseudoIndirectBranch64 as a JIRL as appropriate
+  // for the target.
+  void emitPseudoIndirectBranch(MCStreamer &OutStreamer,
+                                const MachineInstr *MI);
+
+  // lowerOperand - Convert a MachineOperand into the equivalent MCOperand.
+  bool lowerOperand(const MachineOperand &MO, MCOperand &MCOp);
+
+  void emitInlineAsmStart() const override;
+
+  void emitInlineAsmEnd(const MCSubtargetInfo &StartInfo,
+                        const MCSubtargetInfo *EndInfo) const override;
+
+  void EmitInstrReg(const MCSubtargetInfo &STI, unsigned Opcode, unsigned Reg);
+
+  void EmitInstrRegReg(const MCSubtargetInfo &STI, unsigned Opcode,
+                       unsigned Reg1, unsigned Reg2);
+
+  void EmitInstrRegRegReg(const MCSubtargetInfo &STI, unsigned Opcode,
+                          unsigned Reg1, unsigned Reg2, unsigned Reg3);
+
+  void EmitMovFPIntPair(const MCSubtargetInfo &STI, unsigned MovOpc,
+                        unsigned Reg1, unsigned Reg2, unsigned FPReg1,
+                        unsigned FPReg2, bool LE);
+
+  bool isLongBranchPseudo(int Opcode) const;
+
+public:
+  const LoongArchSubtarget *Subtarget;
+  const LoongArchFunctionInfo *LoongArchFI;
+  LoongArchMCInstLower MCInstLowering;
+
+  explicit LoongArchAsmPrinter(TargetMachine &TM,
+                          std::unique_ptr<MCStreamer> Streamer)
+      : AsmPrinter(TM, std::move(Streamer)), MCInstLowering(*this) {}
+
+  StringRef getPassName() const override { return "LoongArch Assembly Printer"; }
+
+  bool runOnMachineFunction(MachineFunction &MF) override;
+
+  void emitInstruction(const MachineInstr *MI) override;
+  const char *getCurrentABIString() const;
+  void emitFunctionEntryLabel() override;
+  void emitFunctionBodyStart() override;
+  void emitFunctionBodyEnd() override;
+  void emitBasicBlockEnd(const MachineBasicBlock &MBB) override;
+  bool isBlockOnlyReachableByFallthrough(
+                                   const MachineBasicBlock* MBB) const override;
+  bool PrintAsmOperand(const MachineInstr *MI, unsigned OpNo,
+                       const char *ExtraCode, raw_ostream &O) override;
+  bool PrintAsmMemoryOperand(const MachineInstr *MI, unsigned OpNum,
+                             const char *ExtraCode, raw_ostream &O) override;
+  void printOperand(const MachineInstr *MI, int opNum, raw_ostream &O);
+  void printMemOperand(const MachineInstr *MI, int opNum, raw_ostream &O);
+  void printMemOperandEA(const MachineInstr *MI, int opNum, raw_ostream &O);
+  void printRegisterList(const MachineInstr *MI, int opNum, raw_ostream &O);
+  void emitStartOfAsmFile(Module &M) override;
+  void emitEndOfAsmFile(Module &M) override;
+  void PrintDebugValueComment(const MachineInstr *MI, raw_ostream &OS);
+};
+
+} // end namespace llvm
+
+#endif // LLVM_LIB_TARGET_LOONGARCH_LOONGARCHASMPRINTER_H
diff --git a/llvm/lib/Target/LoongArch/LoongArchCCState.cpp b/llvm/lib/Target/LoongArch/LoongArchCCState.cpp
new file mode 100644
index 000000000000..6630ca7598ac
--- /dev/null
+++ b/llvm/lib/Target/LoongArch/LoongArchCCState.cpp
@@ -0,0 +1,165 @@
+//===---- LoongArchCCState.cpp - CCState with LoongArch specific extensions ---------===//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+
+#include "LoongArchCCState.h"
+#include "LoongArchSubtarget.h"
+#include "llvm/IR/Module.h"
+
+using namespace llvm;
+
+/// This function returns true if CallSym is a long double emulation routine.
+static bool isF128SoftLibCall(const char *CallSym) {
+  const char *const LibCalls[] = {
+      "__addtf3",      "__divtf3",     "__eqtf2",       "__extenddftf2",
+      "__extendsftf2", "__fixtfdi",    "__fixtfsi",     "__fixtfti",
+      "__fixunstfdi",  "__fixunstfsi", "__fixunstfti",  "__floatditf",
+      "__floatsitf",   "__floattitf",  "__floatunditf", "__floatunsitf",
+      "__floatuntitf", "__getf2",      "__gttf2",       "__letf2",
+      "__lttf2",       "__multf3",     "__netf2",       "__powitf2",
+      "__subtf3",      "__trunctfdf2", "__trunctfsf2",  "__unordtf2",
+      "ceill",         "copysignl",    "cosl",          "exp2l",
+      "expl",          "floorl",       "fmal",          "fmaxl",
+      "fmodl",         "log10l",       "log2l",         "logl",
+      "nearbyintl",    "powl",         "rintl",         "roundl",
+      "sinl",          "sqrtl",        "truncl"};
+
+  // Check that LibCalls is sorted alphabetically.
+  auto Comp = [](const char *S1, const char *S2) { return strcmp(S1, S2) < 0; };
+  assert(std::is_sorted(std::begin(LibCalls), std::end(LibCalls), Comp));
+  return std::binary_search(std::begin(LibCalls), std::end(LibCalls),
+                            CallSym, Comp);
+}
+
+/// This function returns true if Ty is fp128, {f128} or i128 which was
+/// originally a fp128.
+static bool originalTypeIsF128(const Type *Ty, const char *Func) {
+  if (Ty->isFP128Ty())
+    return true;
+
+  if (Ty->isStructTy() && Ty->getStructNumElements() == 1 &&
+      Ty->getStructElementType(0)->isFP128Ty())
+    return true;
+
+  // If the Ty is i128 and the function being called is a long double emulation
+  // routine, then the original type is f128.
+  return (Func && Ty->isIntegerTy(128) && isF128SoftLibCall(Func));
+}
+
+/// Return true if the original type was vXfXX.
+static bool originalEVTTypeIsVectorFloat(EVT Ty) {
+  if (Ty.isVector() && Ty.getVectorElementType().isFloatingPoint())
+    return true;
+
+  return false;
+}
+
+/// Return true if the original type was vXfXX / vXfXX.
+static bool originalTypeIsVectorFloat(const Type * Ty) {
+  if (Ty->isVectorTy() && Ty->isFPOrFPVectorTy())
+    return true;
+
+  return false;
+}
+
+LoongArchCCState::SpecialCallingConvType
+LoongArchCCState::getSpecialCallingConvForCallee(const SDNode *Callee,
+                                            const LoongArchSubtarget &Subtarget) {
+  LoongArchCCState::SpecialCallingConvType SpecialCallingConv = NoSpecialCallingConv;
+  return SpecialCallingConv;
+}
+
+void LoongArchCCState::PreAnalyzeCallResultForF128(
+    const SmallVectorImpl<ISD::InputArg> &Ins,
+    const Type *RetTy, const char *Call) {
+  for (unsigned i = 0; i < Ins.size(); ++i) {
+    OriginalArgWasF128.push_back(
+        originalTypeIsF128(RetTy, Call));
+    OriginalArgWasFloat.push_back(RetTy->isFloatingPointTy());
+  }
+}
+
+/// Identify lowered values that originated from f128 or float arguments and
+/// record this for use by RetCC_LoongArchLP64LPX32.
+void LoongArchCCState::PreAnalyzeReturnForF128(
+    const SmallVectorImpl<ISD::OutputArg> &Outs) {
+  const MachineFunction &MF = getMachineFunction();
+  for (unsigned i = 0; i < Outs.size(); ++i) {
+    OriginalArgWasF128.push_back(
+        originalTypeIsF128(MF.getFunction().getReturnType(), nullptr));
+    OriginalArgWasFloat.push_back(
+        MF.getFunction().getReturnType()->isFloatingPointTy());
+  }
+}
+
+/// Identify lower values that originated from vXfXX and record
+/// this.
+void LoongArchCCState::PreAnalyzeCallResultForVectorFloat(
+    const SmallVectorImpl<ISD::InputArg> &Ins, const Type *RetTy) {
+  for (unsigned i = 0; i < Ins.size(); ++i) {
+    OriginalRetWasFloatVector.push_back(originalTypeIsVectorFloat(RetTy));
+  }
+}
+
+/// Identify lowered values that originated from vXfXX arguments and record
+/// this.
+void LoongArchCCState::PreAnalyzeReturnForVectorFloat(
+    const SmallVectorImpl<ISD::OutputArg> &Outs) {
+  for (unsigned i = 0; i < Outs.size(); ++i) {
+    ISD::OutputArg Out = Outs[i];
+    OriginalRetWasFloatVector.push_back(
+        originalEVTTypeIsVectorFloat(Out.ArgVT));
+  }
+}
+
+/// Identify lowered values that originated from f128, float and sret to vXfXX
+/// arguments and record this.
+void LoongArchCCState::PreAnalyzeCallOperands(
+    const SmallVectorImpl<ISD::OutputArg> &Outs,
+    std::vector<TargetLowering::ArgListEntry> &FuncArgs,
+    const char *Func) {
+  for (unsigned i = 0; i < Outs.size(); ++i) {
+    TargetLowering::ArgListEntry FuncArg = FuncArgs[Outs[i].OrigArgIndex];
+
+    OriginalArgWasF128.push_back(originalTypeIsF128(FuncArg.Ty, Func));
+    OriginalArgWasFloat.push_back(FuncArg.Ty->isFloatingPointTy());
+    OriginalArgWasFloatVector.push_back(FuncArg.Ty->isVectorTy());
+    CallOperandIsFixed.push_back(Outs[i].IsFixed);
+  }
+}
+
+/// Identify lowered values that originated from f128, float and vXfXX arguments
+/// and record this.
+void LoongArchCCState::PreAnalyzeFormalArgumentsForF128(
+    const SmallVectorImpl<ISD::InputArg> &Ins) {
+  const MachineFunction &MF = getMachineFunction();
+  for (unsigned i = 0; i < Ins.size(); ++i) {
+    Function::const_arg_iterator FuncArg = MF.getFunction().arg_begin();
+
+    // SRet arguments cannot originate from f128 or {f128} returns so we just
+    // push false. We have to handle this specially since SRet arguments
+    // aren't mapped to an original argument.
+    if (Ins[i].Flags.isSRet()) {
+      OriginalArgWasF128.push_back(false);
+      OriginalArgWasFloat.push_back(false);
+      OriginalArgWasFloatVector.push_back(false);
+      continue;
+    }
+
+    assert(Ins[i].getOrigArgIndex() < MF.getFunction().arg_size());
+    std::advance(FuncArg, Ins[i].getOrigArgIndex());
+
+    OriginalArgWasF128.push_back(
+        originalTypeIsF128(FuncArg->getType(), nullptr));
+    OriginalArgWasFloat.push_back(FuncArg->getType()->isFloatingPointTy());
+
+    // The LoongArch vector ABI exhibits a corner case of sorts or quirk; if the
+    // first argument is actually an SRet pointer to a vector, then the next
+    // argument slot is $a2.
+    OriginalArgWasFloatVector.push_back(FuncArg->getType()->isVectorTy());
+  }
+}
diff --git a/llvm/lib/Target/LoongArch/LoongArchCCState.h b/llvm/lib/Target/LoongArch/LoongArchCCState.h
new file mode 100644
index 000000000000..1c1a1446efba
--- /dev/null
+++ b/llvm/lib/Target/LoongArch/LoongArchCCState.h
@@ -0,0 +1,165 @@
+//===---- LoongArchCCState.h - CCState with LoongArch specific extensions -----------===//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef LoongArchCCSTATE_H
+#define LoongArchCCSTATE_H
+
+#include "LoongArchISelLowering.h"
+#include "llvm/ADT/SmallVector.h"
+#include "llvm/CodeGen/CallingConvLower.h"
+
+namespace llvm {
+class SDNode;
+class LoongArchSubtarget;
+
+class LoongArchCCState : public CCState {
+public:
+  enum SpecialCallingConvType { NoSpecialCallingConv };
+
+  /// Determine the SpecialCallingConvType for the given callee
+  static SpecialCallingConvType
+  getSpecialCallingConvForCallee(const SDNode *Callee,
+                                 const LoongArchSubtarget &Subtarget);
+
+private:
+  /// Identify lowered values that originated from f128 arguments and record
+  /// this for use by RetCC_LoongArchLP64LPX32.
+  void PreAnalyzeCallResultForF128(const SmallVectorImpl<ISD::InputArg> &Ins,
+                                   const Type *RetTy, const char * Func);
+
+  /// Identify lowered values that originated from f128 arguments and record
+  /// this for use by RetCC_LoongArchLP64LPX32.
+  void PreAnalyzeReturnForF128(const SmallVectorImpl<ISD::OutputArg> &Outs);
+
+  /// Identify lowered values that originated from f128 arguments and record
+  /// this.
+  void
+  PreAnalyzeCallOperands(const SmallVectorImpl<ISD::OutputArg> &Outs,
+                         std::vector<TargetLowering::ArgListEntry> &FuncArgs,
+                         const char *Func);
+
+  /// Identify lowered values that originated from f128 arguments and record
+  /// this for use by RetCC_LoongArchLP64LPX32.
+  void
+  PreAnalyzeFormalArgumentsForF128(const SmallVectorImpl<ISD::InputArg> &Ins);
+
+  void
+  PreAnalyzeCallResultForVectorFloat(const SmallVectorImpl<ISD::InputArg> &Ins,
+                                     const Type *RetTy);
+
+  void PreAnalyzeFormalArgumentsForVectorFloat(
+      const SmallVectorImpl<ISD::InputArg> &Ins);
+
+  void
+  PreAnalyzeReturnForVectorFloat(const SmallVectorImpl<ISD::OutputArg> &Outs);
+
+  /// Records whether the value has been lowered from an f128.
+  SmallVector<bool, 4> OriginalArgWasF128;
+
+  /// Records whether the value has been lowered from float.
+  SmallVector<bool, 4> OriginalArgWasFloat;
+
+  /// Records whether the value has been lowered from a floating point vector.
+  SmallVector<bool, 4> OriginalArgWasFloatVector;
+
+  /// Records whether the return value has been lowered from a floating point
+  /// vector.
+  SmallVector<bool, 4> OriginalRetWasFloatVector;
+
+  /// Records whether the value was a fixed argument.
+  /// See ISD::OutputArg::IsFixed,
+  SmallVector<bool, 4> CallOperandIsFixed;
+
+  // FIXME: This should probably be a fully fledged calling convention.
+  SpecialCallingConvType SpecialCallingConv;
+
+public:
+  LoongArchCCState(CallingConv::ID CC, bool isVarArg, MachineFunction &MF,
+              SmallVectorImpl<CCValAssign> &locs, LLVMContext &C,
+              SpecialCallingConvType SpecialCC = NoSpecialCallingConv)
+      : CCState(CC, isVarArg, MF, locs, C), SpecialCallingConv(SpecialCC) {}
+
+  void
+  AnalyzeCallOperands(const SmallVectorImpl<ISD::OutputArg> &Outs,
+                      CCAssignFn Fn,
+                      std::vector<TargetLowering::ArgListEntry> &FuncArgs,
+                      const char *Func) {
+    PreAnalyzeCallOperands(Outs, FuncArgs, Func);
+    CCState::AnalyzeCallOperands(Outs, Fn);
+    OriginalArgWasF128.clear();
+    OriginalArgWasFloat.clear();
+    OriginalArgWasFloatVector.clear();
+    CallOperandIsFixed.clear();
+  }
+
+  // The AnalyzeCallOperands in the base class is not usable since we must
+  // provide a means of accessing ArgListEntry::IsFixed. Delete them from this
+  // class. This doesn't stop them being used via the base class though.
+  void AnalyzeCallOperands(const SmallVectorImpl<ISD::OutputArg> &Outs,
+                           CCAssignFn Fn) = delete;
+  void AnalyzeCallOperands(const SmallVectorImpl<MVT> &Outs,
+                           SmallVectorImpl<ISD::ArgFlagsTy> &Flags,
+                           CCAssignFn Fn) = delete;
+
+  void AnalyzeFormalArguments(const SmallVectorImpl<ISD::InputArg> &Ins,
+                              CCAssignFn Fn) {
+    PreAnalyzeFormalArgumentsForF128(Ins);
+    CCState::AnalyzeFormalArguments(Ins, Fn);
+    OriginalArgWasFloat.clear();
+    OriginalArgWasF128.clear();
+    OriginalArgWasFloatVector.clear();
+  }
+
+  void AnalyzeCallResult(const SmallVectorImpl<ISD::InputArg> &Ins,
+                         CCAssignFn Fn, const Type *RetTy,
+                         const char *Func) {
+    PreAnalyzeCallResultForF128(Ins, RetTy, Func);
+    PreAnalyzeCallResultForVectorFloat(Ins, RetTy);
+    CCState::AnalyzeCallResult(Ins, Fn);
+    OriginalArgWasFloat.clear();
+    OriginalArgWasF128.clear();
+    OriginalArgWasFloatVector.clear();
+  }
+
+  void AnalyzeReturn(const SmallVectorImpl<ISD::OutputArg> &Outs,
+                     CCAssignFn Fn) {
+    PreAnalyzeReturnForF128(Outs);
+    PreAnalyzeReturnForVectorFloat(Outs);
+    CCState::AnalyzeReturn(Outs, Fn);
+    OriginalArgWasFloat.clear();
+    OriginalArgWasF128.clear();
+    OriginalArgWasFloatVector.clear();
+  }
+
+  bool CheckReturn(const SmallVectorImpl<ISD::OutputArg> &ArgsFlags,
+                   CCAssignFn Fn) {
+    PreAnalyzeReturnForF128(ArgsFlags);
+    PreAnalyzeReturnForVectorFloat(ArgsFlags);
+    bool Return = CCState::CheckReturn(ArgsFlags, Fn);
+    OriginalArgWasFloat.clear();
+    OriginalArgWasF128.clear();
+    OriginalArgWasFloatVector.clear();
+    return Return;
+  }
+
+  bool WasOriginalArgF128(unsigned ValNo) { return OriginalArgWasF128[ValNo]; }
+  bool WasOriginalArgFloat(unsigned ValNo) {
+      return OriginalArgWasFloat[ValNo];
+  }
+  bool WasOriginalArgVectorFloat(unsigned ValNo) const {
+    return OriginalArgWasFloatVector[ValNo];
+  }
+  bool WasOriginalRetVectorFloat(unsigned ValNo) const {
+    return OriginalRetWasFloatVector[ValNo];
+  }
+  bool IsCallOperandFixed(unsigned ValNo) { return CallOperandIsFixed[ValNo]; }
+  SpecialCallingConvType getSpecialCallingConv() { return SpecialCallingConv; }
+};
+}
+
+#endif
diff --git a/llvm/lib/Target/LoongArch/LoongArchCallingConv.td b/llvm/lib/Target/LoongArch/LoongArchCallingConv.td
new file mode 100644
index 000000000000..13170f3f0ac3
--- /dev/null
+++ b/llvm/lib/Target/LoongArch/LoongArchCallingConv.td
@@ -0,0 +1,332 @@
+//===-- LoongArchCallingConv.td - Calling Conventions for LoongArch --*- tablegen -*-===//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+// This describes the calling conventions for LoongArch architecture.
+//===----------------------------------------------------------------------===//
+
+/// CCIfSubtarget - Match if the current subtarget has a feature F.
+class CCIfSubtarget<string F, CCAction A, string Invert = "">
+    : CCIf<!strconcat(Invert,
+                      "static_cast<const LoongArchSubtarget&>"
+			"(State.getMachineFunction().getSubtarget()).",
+                      F),
+           A>;
+
+// The inverse of CCIfSubtarget
+class CCIfSubtargetNot<string F, CCAction A> : CCIfSubtarget<F, A, "!">;
+
+/// Match if the original argument (before lowering) was a float.
+/// For example, this is true for i32's that were lowered from soft-float.
+class CCIfOrigArgWasNotFloat<CCAction A>
+    : CCIf<"!static_cast<LoongArchCCState *>(&State)->WasOriginalArgFloat(ValNo)",
+           A>;
+
+/// Match if the original argument (before lowering) was a 128-bit float (i.e.
+/// long double).
+class CCIfOrigArgWasF128<CCAction A>
+    : CCIf<"static_cast<LoongArchCCState *>(&State)->WasOriginalArgF128(ValNo)", A>;
+
+/// Match if this specific argument is a vararg.
+/// This is slightly different fro CCIfIsVarArg which matches if any argument is
+/// a vararg.
+class CCIfArgIsVarArg<CCAction A>
+    : CCIf<"!static_cast<LoongArchCCState *>(&State)->IsCallOperandFixed(ValNo)", A>;
+
+/// Match if the return was a floating point vector.
+class CCIfOrigArgWasNotVectorFloat<CCAction A>
+    : CCIf<"!static_cast<LoongArchCCState *>(&State)"
+                "->WasOriginalRetVectorFloat(ValNo)", A>;
+
+/// Match if the special calling conv is the specified value.
+class CCIfSpecialCallingConv<string CC, CCAction A>
+    : CCIf<"static_cast<LoongArchCCState *>(&State)->getSpecialCallingConv() == "
+               "LoongArchCCState::" # CC, A>;
+
+// For soft-float, f128 values are returned in A0_64 rather than V1_64.
+def RetCC_F128SoftFloat : CallingConv<[
+  CCAssignToReg<[A0_64, A1_64]>
+]>;
+
+//
+// For hard-float, f128 values are returned as a pair of f64's rather than a
+// pair of i64's.
+def RetCC_F128HardFloat : CallingConv<[
+  //CCBitConvertToType<f64>,
+
+  // Contrary to the ABI documentation, a struct containing a long double is
+  // returned in $f0, and $f1 instead of the usual $f0, and $f2. This is to
+  // match the de facto ABI as implemented by GCC.
+  CCIfInReg<CCAssignToReg<[A0_64, A1_64]>>,
+
+  CCAssignToReg<[A0_64, A1_64]>
+]>;
+
+// Handle F128 specially since we can't identify the original type during the
+// tablegen-erated code.
+def RetCC_F128 : CallingConv<[
+  CCIfSubtarget<"useSoftFloat()",
+      CCIfType<[i64], CCDelegateTo<RetCC_F128SoftFloat>>>,
+  CCIfSubtargetNot<"useSoftFloat()",
+      CCIfType<[i64], CCDelegateTo<RetCC_F128HardFloat>>>
+]>;
+
+//===----------------------------------------------------------------------===//
+// LoongArch LP32 Calling Convention
+//===----------------------------------------------------------------------===//
+
+def CC_LoongArchLP32 : CallingConv<[
+  // Promote i8/i16 arguments to i32.
+  CCIfType<[i1, i8, i16], CCPromoteToType<i32>>,
+
+  // Integer values get stored in stack slots that are 4 bytes in
+  // size and 4-byte aligned.
+  CCIfType<[i32, f32], CCAssignToStack<4, 4>>,
+
+  // Integer values get stored in stack slots that are 8 bytes in
+  // size and 8-byte aligned.
+  CCIfType<[f64], CCAssignToStack<8, 8>>
+]>;
+
+// Only the return rules are defined here for LP32. The rules for argument
+// passing are defined in LoongArchISelLowering.cpp.
+def RetCC_LoongArchLP32 : CallingConv<[
+  // Promote i1/i8/i16 return values to i32.
+  CCIfType<[i1, i8, i16], CCPromoteToType<i32>>,
+
+  // i32 are returned in registers V0, V1, A0, A1, unless the original return
+  // type was a vector of floats.
+  CCIfOrigArgWasNotVectorFloat<CCIfType<[i32],
+                                        CCAssignToReg<[A0, A1]>>>,
+
+  // f32 are returned in registers F0, F2
+  CCIfType<[f32], CCAssignToReg<[F0, F1]>>,
+
+  // f64 arguments are returned in F0_64 and F2_64 in FP64bit mode or
+  // in F0 and F1 in FP32bit mode.
+  CCIfType<[f64], CCIfSubtarget<"isFP64bit()", CCAssignToReg<[F0_64, F1_64]>>>
+]>;
+
+def CC_LoongArchLP32_FP32 : CustomCallingConv;
+def CC_LoongArchLP32_FP64 : CustomCallingConv;
+def CC_LoongArch_F128 : CustomCallingConv;
+
+def CC_LoongArchLP32_FP : CallingConv<[
+  CCIfSubtargetNot<"isFP64bit()", CCDelegateTo<CC_LoongArchLP32_FP32>>,
+  CCIfSubtarget<"isFP64bit()", CCDelegateTo<CC_LoongArchLP32_FP64>>
+]>;
+
+//===----------------------------------------------------------------------===//
+// LoongArch LPX32/LP64 Calling Convention
+//===----------------------------------------------------------------------===//
+
+def CC_LoongArchLP64LPX32_SoftFloat : CallingConv<[
+  CCAssignToReg<[A0, A1, A2, A3,
+                 A4, A5, A6, A7]>,
+  CCAssignToStack<4, 8>
+]>;
+
+def CC_LoongArchLP64LPX32 : CallingConv<[
+
+  // All integers (except soft-float integers) are promoted to 64-bit.
+  CCIfType<[i8, i16, i32], CCIfOrigArgWasNotFloat<CCPromoteToType<i64>>>,
+
+  // The only i32's we have left are soft-float arguments.
+  CCIfSubtarget<"useSoftFloat()", CCIfType<[i32], CCDelegateTo<CC_LoongArchLP64LPX32_SoftFloat>>>,
+
+  // Integer arguments are passed in integer registers.
+  //CCIfType<[i64], CCAssignToRegWithShadow<[A0_64, A1_64, A2_64, A3_64,
+  //                                         A4_64, A5_64, A6_64, A7_64],
+  //                                        [F0_64, F1_64, F2_64, F3_64,
+  //                                         F4_64, F5_64, F6_64, F7_64]>>,
+  CCIfType<[i64], CCAssignToReg<[A0_64, A1_64, A2_64, A3_64,
+                                 A4_64, A5_64, A6_64, A7_64]>>,
+
+  // f32 arguments are passed in single precision FP registers.
+  CCIfType<[f32], CCAssignToReg<[F0, F1, F2, F3,
+                                 F4, F5, F6, F7]>>,
+
+  // f64 arguments are passed in double precision FP registers.
+  CCIfType<[f64], CCAssignToReg<[F0_64, F1_64, F2_64, F3_64,
+                                 F4_64, F5_64, F6_64, F7_64]>>,
+
+  // others f32 arguments are passed in single precision FP registers.
+  CCIfType<[f32], CCAssignToReg<[A0, A1, A2, A3, A4, A5, A6, A7]>>,
+
+  // others f64 arguments are passed in double precision FP registers.
+  CCIfType<[f64], CCAssignToReg<[A0_64, A1_64, A2_64, A3_64,
+                                 A4_64, A5_64, A6_64, A7_64]>>,
+
+  // All stack parameter slots become 64-bit doublewords and are 8-byte aligned.
+  CCIfType<[f32], CCAssignToStack<4, 8>>,
+  CCIfType<[i64, f64], CCAssignToStack<8, 8>>,
+  CCIfType<[v16i8, v8i16, v4i32, v2i64, v4f32, v2f64],
+         CCAssignToStack<16, 16>>,
+  CCIfType<[v32i8, v16i16, v8i32, v4i64, v8f32, v4f64],
+         CCAssignToStack<32, 32>>
+]>;
+
+// LPX32/LP64 variable arguments.
+// All arguments are passed in integer registers.
+def CC_LoongArchLP64LPX32_VarArg : CallingConv<[
+  // All integers are promoted to 64-bit.
+  CCIfType<[i8, i16, i32], CCPromoteToType<i64>>,
+
+  CCIfType<[f32], CCAssignToReg<[A0, A1, A2, A3, A4, A5, A6, A7]>>,
+
+  CCIfType<[i64], CCIfOrigArgWasF128<CCDelegateTo<CC_LoongArch_F128>>>,
+
+  CCIfType<[i64, f64], CCAssignToReg<[A0_64, A1_64, A2_64, A3_64,
+                                      A4_64, A5_64, A6_64, A7_64]>>,
+
+  // All stack parameter slots become 64-bit doublewords and are 8-byte aligned.
+  CCIfType<[f32], CCAssignToStack<4, 8>>,
+  CCIfType<[i64, f64], CCAssignToStack<8, 8>>
+]>;
+
+def RetCC_LoongArchLP64LPX32 : CallingConv<[
+  // f128 needs to be handled similarly to f32 and f64. However, f128 is not
+  // legal and is lowered to i128 which is further lowered to a pair of i64's.
+  // This presents us with a problem for the calling convention since hard-float
+  // still needs to pass them in FPU registers, and soft-float needs to use $v0,
+  // and $a0 instead of the usual $v0, and $v1. We therefore resort to a
+  // pre-analyze (see PreAnalyzeReturnForF128()) step to pass information on
+  // whether the result was originally an f128 into the tablegen-erated code.
+  //
+  // f128 should only occur for the LP64D ABI where long double is 128-bit. On
+  // LPX32, long double is equivalent to double.
+  CCIfType<[i64], CCIfOrigArgWasF128<CCDelegateTo<RetCC_F128>>>,
+
+  CCIfType<[i8, i16, i32, i64], CCIfInReg<CCPromoteToType<i64>>>,
+
+  // i64 are returned in registers V0_64, V1_64
+  CCIfType<[i64], CCAssignToReg<[A0_64, A1_64]>>,
+
+  // f32 are returned in registers F0, F2
+  CCIfType<[f32], CCAssignToReg<[F0, F1]>>,
+
+  // f64 are returned in registers D0, D2
+  CCIfType<[f64], CCAssignToReg<[F0_64, F1_64]>>
+]>;
+
+//===----------------------------------------------------------------------===//
+// LoongArch FastCC Calling Convention
+//===----------------------------------------------------------------------===//
+//LP32 has been removed because of not support
+
+def CC_LoongArchLP64LPX32_FastCC : CallingConv<[
+  // Integer arguments are passed in integer registers.
+  CCIfType<[i64], CCAssignToReg<[A0_64, A1_64, A2_64, A3_64, T0_64, T1_64,
+                                 T2_64, T3_64, T4_64, T5_64, T6_64, T7_64,
+                                 T8_64]>>,
+
+  // f64 arguments are passed in double-precision floating pointer registers.
+  CCIfType<[f64], CCAssignToReg<[F0_64, F1_64, F2_64, F3_64, F4_64, F5_64,
+                                 F6_64, F7_64, F8_64, F9_64, F10_64, F11_64,
+                                 F12_64, F13_64, F14_64, F15_64, F16_64, F17_64,
+                                 F18_64, F19]>>,
+
+  // Stack parameter slots for i64 and f64 are 64-bit doublewords and
+  // 8-byte aligned.
+  CCIfType<[i64, f64], CCAssignToStack<8, 8>>
+]>;
+
+def CC_LoongArch_FastCC : CallingConv<[
+  // Handles byval parameters.
+  CCIfByVal<CCPassByVal<4, 4>>,
+
+  // Promote i8/i16 arguments to i32.
+  CCIfType<[i8, i16], CCPromoteToType<i32>>,
+
+  // Integer arguments are passed in integer registers. All scratch registers,
+  // except for V0 and T9, are available to be used as argument registers.
+  CCIfType<[i32], CCAssignToReg<[A0, A1, A2, A3, T0, T1, T2, T3, T4, T5, T6, T7, T8]>>,
+
+  // Stack parameter slots for i32 and f32 are 32-bit words and 4-byte aligned.
+  CCIfType<[i32, f32], CCAssignToStack<4, 4>>,
+
+//  CCIfSubtarget<"isABI_LP32()", CCDelegateTo<CC_LoongArchLP32_FastCC>>,
+  CCDelegateTo<CC_LoongArchLP64LPX32_FastCC>
+]>;
+
+//===----------------------------------------------------------------------===//
+// LoongArch Calling Convention Dispatch
+//===----------------------------------------------------------------------===//
+
+def RetCC_LoongArch : CallingConv<[
+  CCIfSubtarget<"isABI_LPX32()", CCDelegateTo<RetCC_LoongArchLP64LPX32>>,
+  CCIfSubtarget<"isABI_LP64D()", CCDelegateTo<RetCC_LoongArchLP64LPX32>>,
+  CCDelegateTo<RetCC_LoongArchLP32>
+]>;
+
+def CC_LoongArch_ByVal : CallingConv<[
+  CCIfSubtarget<"isABI_LP32()", CCIfByVal<CCPassByVal<4, 4>>>,
+  CCIfByVal<CCPassByVal<8, 8>>
+]>;
+
+def CC_LoongArch_FixedArg : CallingConv<[
+  CCIfByVal<CCDelegateTo<CC_LoongArch_ByVal>>,
+  //CCIfByVal<CCIfType<[i64],  CCAssignToReg<[A0_64,  A1_64,  A2_64,  A3_64,
+  //                                          A4_64,  A5_64,  A6_64,  A7_64]>>>,
+
+  // f128 needs to be handled similarly to f32 and f64 on hard-float. However,
+  // f128 is not legal and is lowered to i128 which is further lowered to a pair
+  // of i64's.
+  // This presents us with a problem for the calling convention since hard-float
+  // still needs to pass them in FPU registers. We therefore resort to a
+  // pre-analyze (see PreAnalyzeFormalArgsForF128()) step to pass information on
+  // whether the argument was originally an f128 into the tablegen-erated code.
+  //
+  // f128 should only occur for the LP64D ABI where long double is 128-bit. On
+  // LPX32, long double is equivalent to double.
+  CCIfType<[i64],
+      CCIfSubtargetNot<"useSoftFloat()",
+          CCIfOrigArgWasF128<CCBitConvertToType<i64>>>>,
+
+  CCIfCC<"CallingConv::Fast", CCDelegateTo<CC_LoongArch_FastCC>>,
+
+  CCIfSubtarget<"isABI_LP32()", CCDelegateTo<CC_LoongArchLP32_FP>>,
+  CCDelegateTo<CC_LoongArchLP64LPX32>
+]>;
+
+def CC_LoongArch_VarArg : CallingConv<[
+  CCIfByVal<CCDelegateTo<CC_LoongArch_ByVal>>,
+
+  CCIfSubtarget<"isABI_LP32()", CCDelegateTo<CC_LoongArchLP32_FP>>,
+  CCDelegateTo<CC_LoongArchLP64LPX32_VarArg>
+]>;
+
+def CC_LoongArch : CallingConv<[
+  CCIfVarArg<CCIfArgIsVarArg<CCDelegateTo<CC_LoongArch_VarArg>>>,
+  CCDelegateTo<CC_LoongArch_FixedArg>
+]>;
+
+//===----------------------------------------------------------------------===//
+// Callee-saved register lists.
+//===----------------------------------------------------------------------===//
+
+def CSR_SingleFloatOnly : CalleeSavedRegs<(add (sequence "F%u", 31, 24), RA, FP,
+                                               (sequence "S%u", 8, 0))>;
+
+//def CSR_LP32_FPXX : CalleeSavedRegs<(add (sequence "D%u", 15, 10), RA, FP,
+//                                        (sequence "S%u", 8, 0))> {
+//  let OtherPreserved = (add (decimate (sequence "F%u", 30, 20), 2));
+//}
+
+def CSR_LP32 : CalleeSavedRegs<(add (sequence "F%u_64", 31, 24), RA, FP,
+                                   (sequence "S%u", 8, 0))>;
+
+//def CSR_LP32_FP64 :
+//  CalleeSavedRegs<(add (decimate (sequence "D%u_64", 30, 20), 2), RA, FP,
+//                       (sequence "S%u", 8, 0))>;
+
+def CSR_LPX32 : CalleeSavedRegs<(add F20_64, F22_64, F24_64, F26_64, F28_64,
+                                   F30_64, RA_64, FP_64,
+                                   (sequence "S%u_64", 8, 0))>;
+
+//def CSR_LP64 : CalleeSavedRegs<(add (sequence "D%u_64", 31, 24), RA_64, SP_64, FP_64,
+def CSR_LP64 : CalleeSavedRegs<(add (sequence "F%u_64", 31, 24), RA_64, FP_64,
+                                   (sequence "S%u_64", 8, 0))>;
diff --git a/llvm/lib/Target/LoongArch/LoongArchExpandPseudo.cpp b/llvm/lib/Target/LoongArch/LoongArchExpandPseudo.cpp
new file mode 100644
index 000000000000..fc1b501965c3
--- /dev/null
+++ b/llvm/lib/Target/LoongArch/LoongArchExpandPseudo.cpp
@@ -0,0 +1,1082 @@
+//===-- LoongArchExpandPseudoInsts.cpp - Expand pseudo instructions ------------===//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+//
+// This file contains a pass that expands pseudo instructions into target
+// instructions to allow proper scheduling, if-conversion, and other late
+// optimizations. This pass should be run after register allocation but before
+// the post-regalloc scheduling pass.
+//
+// This is currently only used for expanding atomic pseudos after register
+// allocation. We do this to avoid the fast register allocator introducing
+// spills between ll and sc. These stores cause some LoongArch implementations to
+// abort the atomic RMW sequence.
+//
+//===----------------------------------------------------------------------===//
+
+#include "LoongArch.h"
+#include "LoongArchInstrInfo.h"
+#include "LoongArchSubtarget.h"
+#include "MCTargetDesc/LoongArchMCTargetDesc.h"
+#include "llvm/CodeGen/LivePhysRegs.h"
+#include "llvm/CodeGen/MachineFunctionPass.h"
+#include "llvm/CodeGen/MachineInstrBuilder.h"
+
+using namespace llvm;
+
+static cl::opt<bool>
+EnableLoongson3FixLLSC("loongarch-fix-loongson3-llsc", cl::Hidden,
+                cl::desc("Work around loongson3 llsc erratum"),
+                cl::init(true));
+
+#define DEBUG_TYPE "loongarch-pseudo"
+
+namespace {
+  class LoongArchExpandPseudo : public MachineFunctionPass {
+  public:
+    static char ID;
+    LoongArchExpandPseudo() : MachineFunctionPass(ID) {}
+
+    const LoongArchInstrInfo *TII;
+    const LoongArchSubtarget *STI;
+
+    bool runOnMachineFunction(MachineFunction &Fn) override;
+
+    MachineFunctionProperties getRequiredProperties() const override {
+      return MachineFunctionProperties().set(
+          MachineFunctionProperties::Property::NoVRegs);
+    }
+
+    StringRef getPassName() const override {
+      return "LoongArch pseudo instruction expansion pass";
+    }
+
+  private:
+    bool expandAtomicCmpSwap(MachineBasicBlock &MBB,
+                             MachineBasicBlock::iterator MBBI,
+                             MachineBasicBlock::iterator &NextMBBI);
+    bool expandAtomicCmpSwapSubword(MachineBasicBlock &MBB,
+                                    MachineBasicBlock::iterator MBBI,
+                                    MachineBasicBlock::iterator &NextMBBI);
+
+    bool expandAtomicBinOp(MachineBasicBlock &BB,
+                           MachineBasicBlock::iterator I,
+                           MachineBasicBlock::iterator &NMBBI, unsigned Size);
+    bool expandAtomicBinOpSubword(MachineBasicBlock &BB,
+                                  MachineBasicBlock::iterator I,
+                                  MachineBasicBlock::iterator &NMBBI);
+
+    bool expandPseudoCall(MachineBasicBlock &BB,
+                          MachineBasicBlock::iterator I,
+                          MachineBasicBlock::iterator &NMBBI);
+    bool expandPseudoTEQ(MachineBasicBlock &BB,
+                         MachineBasicBlock::iterator I,
+                         MachineBasicBlock::iterator &NMBBI);
+
+    bool expandLoadAddr(MachineBasicBlock &BB,
+                        MachineBasicBlock::iterator I,
+                        MachineBasicBlock::iterator &NMBBI);
+
+    bool expandMI(MachineBasicBlock &MBB, MachineBasicBlock::iterator MBBI,
+                  MachineBasicBlock::iterator &NMBB);
+    bool expandMBB(MachineBasicBlock &MBB);
+   };
+  char LoongArchExpandPseudo::ID = 0;
+}
+
+bool LoongArchExpandPseudo::expandAtomicCmpSwapSubword(
+    MachineBasicBlock &BB, MachineBasicBlock::iterator I,
+    MachineBasicBlock::iterator &NMBBI) {
+
+  MachineFunction *MF = BB.getParent();
+
+  DebugLoc DL = I->getDebugLoc();
+  unsigned LL, SC;
+  unsigned ZERO = LoongArch::ZERO;
+  unsigned BNE = LoongArch::BNE32;
+  unsigned BEQ = LoongArch::BEQ32;
+  unsigned SEOp =
+      I->getOpcode() == LoongArch::ATOMIC_CMP_SWAP_I8_POSTRA ? LoongArch::EXT_W_B32 : LoongArch::EXT_W_H32;
+
+  LL = LoongArch::LL_W;
+  SC = LoongArch::SC_W;
+
+  unsigned Dest = I->getOperand(0).getReg();
+  unsigned Ptr = I->getOperand(1).getReg();
+  unsigned Mask = I->getOperand(2).getReg();
+  unsigned ShiftCmpVal = I->getOperand(3).getReg();
+  unsigned Mask2 = I->getOperand(4).getReg();
+  unsigned ShiftNewVal = I->getOperand(5).getReg();
+  unsigned ShiftAmnt = I->getOperand(6).getReg();
+  unsigned Scratch = I->getOperand(7).getReg();
+  unsigned Scratch2 = I->getOperand(8).getReg();
+
+  // insert new blocks after the current block
+  const BasicBlock *LLVM_BB = BB.getBasicBlock();
+  MachineBasicBlock *loop1MBB = MF->CreateMachineBasicBlock(LLVM_BB);
+  MachineBasicBlock *loop2MBB = MF->CreateMachineBasicBlock(LLVM_BB);
+  MachineBasicBlock *sinkMBB = MF->CreateMachineBasicBlock(LLVM_BB);
+  MachineBasicBlock *exitMBB = MF->CreateMachineBasicBlock(LLVM_BB);
+  MachineFunction::iterator It = ++BB.getIterator();
+  MF->insert(It, loop1MBB);
+  MF->insert(It, loop2MBB);
+  MF->insert(It, sinkMBB);
+  MF->insert(It, exitMBB);
+
+  // Transfer the remainder of BB and its successor edges to exitMBB.
+  exitMBB->splice(exitMBB->begin(), &BB,
+                  std::next(MachineBasicBlock::iterator(I)), BB.end());
+  exitMBB->transferSuccessorsAndUpdatePHIs(&BB);
+
+  //  thisMBB:
+  //    ...
+  //    fallthrough --> loop1MBB
+  BB.addSuccessor(loop1MBB, BranchProbability::getOne());
+  loop1MBB->addSuccessor(sinkMBB);
+  loop1MBB->addSuccessor(loop2MBB);
+  loop1MBB->normalizeSuccProbs();
+  loop2MBB->addSuccessor(loop1MBB);
+  loop2MBB->addSuccessor(sinkMBB);
+  loop2MBB->normalizeSuccProbs();
+  sinkMBB->addSuccessor(exitMBB, BranchProbability::getOne());
+
+  // loop1MBB:
+  //   ll dest, 0(ptr)
+  //   and Mask', dest, Mask
+  //   bne Mask', ShiftCmpVal, exitMBB
+  BuildMI(loop1MBB, DL, TII->get(LL), Scratch).addReg(Ptr).addImm(0);
+  BuildMI(loop1MBB, DL, TII->get(LoongArch::AND32), Scratch2)
+      .addReg(Scratch)
+      .addReg(Mask);
+  BuildMI(loop1MBB, DL, TII->get(BNE))
+    .addReg(Scratch2).addReg(ShiftCmpVal).addMBB(sinkMBB);
+
+  // loop2MBB:
+  //   and dest, dest, mask2
+  //   or dest, dest, ShiftNewVal
+  //   sc dest, dest, 0(ptr)
+  //   beq dest, $0, loop1MBB
+  BuildMI(loop2MBB, DL, TII->get(LoongArch::AND32), Scratch)
+      .addReg(Scratch, RegState::Kill)
+      .addReg(Mask2);
+  BuildMI(loop2MBB, DL, TII->get(LoongArch::OR32), Scratch)
+      .addReg(Scratch, RegState::Kill)
+      .addReg(ShiftNewVal);
+  BuildMI(loop2MBB, DL, TII->get(SC), Scratch)
+      .addReg(Scratch, RegState::Kill)
+      .addReg(Ptr)
+      .addImm(0);
+  BuildMI(loop2MBB, DL, TII->get(BEQ))
+      .addReg(Scratch, RegState::Kill)
+      .addReg(ZERO)
+      .addMBB(loop1MBB);
+
+  //  sinkMBB:
+  //    srl     srlres, Mask', shiftamt
+  //    sign_extend dest,srlres
+  BuildMI(sinkMBB, DL, TII->get(LoongArch::SRL_W), Dest)
+      .addReg(Scratch2)
+      .addReg(ShiftAmnt);
+
+  BuildMI(sinkMBB, DL, TII->get(SEOp), Dest).addReg(Dest);
+
+  if (EnableLoongson3FixLLSC) {
+    bool Has_sync = false;
+    for (MachineBasicBlock::iterator MBBb = sinkMBB->begin(), MBBe = sinkMBB->end();
+         MBBb != MBBe; ++MBBb) {
+      Has_sync |= MBBb->getOpcode() == LoongArch::DBAR ? true : false;
+      if (MBBb->mayLoad() || MBBb->mayStore())
+        break;
+    }
+
+    if (!Has_sync) {
+      MachineBasicBlock::iterator Pos = sinkMBB->begin();
+      BuildMI(*sinkMBB, Pos, DL, TII->get(LoongArch::DBAR)).addImm(0);
+    }
+  }
+
+  LivePhysRegs LiveRegs;
+  computeAndAddLiveIns(LiveRegs, *loop1MBB);
+  computeAndAddLiveIns(LiveRegs, *loop2MBB);
+  computeAndAddLiveIns(LiveRegs, *sinkMBB);
+  computeAndAddLiveIns(LiveRegs, *exitMBB);
+
+  NMBBI = BB.end();
+  I->eraseFromParent();
+  return true;
+}
+
+bool LoongArchExpandPseudo::expandAtomicCmpSwap(MachineBasicBlock &BB,
+                                           MachineBasicBlock::iterator I,
+                                           MachineBasicBlock::iterator &NMBBI) {
+
+  const unsigned Size =
+      I->getOpcode() == LoongArch::ATOMIC_CMP_SWAP_I32_POSTRA ? 4 : 8;
+  MachineFunction *MF = BB.getParent();
+
+  DebugLoc DL = I->getDebugLoc();
+
+  unsigned LL, SC, ZERO, BNE, BEQ, MOVE;
+
+  if (Size == 4) {
+    LL = LoongArch::LL_W;
+    SC = LoongArch::SC_W;
+    BNE = LoongArch::BNE32;
+    BEQ = LoongArch::BEQ32;
+
+    ZERO = LoongArch::ZERO;
+    MOVE = LoongArch::OR32;
+  } else {
+    LL = LoongArch::LL_D;
+    SC = LoongArch::SC_D;
+    ZERO = LoongArch::ZERO_64;
+    BNE = LoongArch::BNE;
+    BEQ = LoongArch::BEQ;
+    MOVE = LoongArch::OR;
+  }
+
+  unsigned Dest = I->getOperand(0).getReg();
+  unsigned Ptr = I->getOperand(1).getReg();
+  unsigned OldVal = I->getOperand(2).getReg();
+  unsigned NewVal = I->getOperand(3).getReg();
+  unsigned Scratch = I->getOperand(4).getReg();
+
+  // insert new blocks after the current block
+  const BasicBlock *LLVM_BB = BB.getBasicBlock();
+  MachineBasicBlock *loop1MBB = MF->CreateMachineBasicBlock(LLVM_BB);
+  MachineBasicBlock *loop2MBB = MF->CreateMachineBasicBlock(LLVM_BB);
+  MachineBasicBlock *exitMBB = MF->CreateMachineBasicBlock(LLVM_BB);
+  MachineFunction::iterator It = ++BB.getIterator();
+  MF->insert(It, loop1MBB);
+  MF->insert(It, loop2MBB);
+  MF->insert(It, exitMBB);
+
+  // Transfer the remainder of BB and its successor edges to exitMBB.
+  exitMBB->splice(exitMBB->begin(), &BB,
+                  std::next(MachineBasicBlock::iterator(I)), BB.end());
+  exitMBB->transferSuccessorsAndUpdatePHIs(&BB);
+
+  //  thisMBB:
+  //    ...
+  //    fallthrough --> loop1MBB
+  BB.addSuccessor(loop1MBB, BranchProbability::getOne());
+  loop1MBB->addSuccessor(exitMBB);
+  loop1MBB->addSuccessor(loop2MBB);
+  loop1MBB->normalizeSuccProbs();
+  loop2MBB->addSuccessor(loop1MBB);
+  loop2MBB->addSuccessor(exitMBB);
+  loop2MBB->normalizeSuccProbs();
+
+  // loop1MBB:
+  //   ll dest, 0(ptr)
+  //   bne dest, oldval, exitMBB
+  BuildMI(loop1MBB, DL, TII->get(LL), Dest).addReg(Ptr).addImm(0);
+  BuildMI(loop1MBB, DL, TII->get(BNE))
+    .addReg(Dest, RegState::Kill).addReg(OldVal).addMBB(exitMBB);
+
+  // loop2MBB:
+  //   move scratch, NewVal
+  //   sc Scratch, Scratch, 0(ptr)
+  //   beq Scratch, $0, loop1MBB
+  BuildMI(loop2MBB, DL, TII->get(MOVE), Scratch).addReg(NewVal).addReg(ZERO);
+  BuildMI(loop2MBB, DL, TII->get(SC), Scratch)
+    .addReg(Scratch).addReg(Ptr).addImm(0);
+  BuildMI(loop2MBB, DL, TII->get(BEQ))
+    .addReg(Scratch, RegState::Kill).addReg(ZERO).addMBB(loop1MBB);
+
+  if (EnableLoongson3FixLLSC) {
+    bool Has_sync = false;
+    for (MachineBasicBlock::iterator MBBb = exitMBB->begin(), MBBe = exitMBB->end();
+         MBBb != MBBe; ++MBBb) {
+      Has_sync |= MBBb->getOpcode() == LoongArch::DBAR ? true : false;
+      if (MBBb->mayLoad() || MBBb->mayStore())
+        break;
+    }
+    if (!Has_sync) {
+      MachineBasicBlock::iterator Pos = exitMBB->begin();
+      BuildMI(*exitMBB, Pos, DL, TII->get(LoongArch::DBAR)).addImm(0);
+    }
+  }
+
+  LivePhysRegs LiveRegs;
+  computeAndAddLiveIns(LiveRegs, *loop1MBB);
+  computeAndAddLiveIns(LiveRegs, *loop2MBB);
+  computeAndAddLiveIns(LiveRegs, *exitMBB);
+
+  NMBBI = BB.end();
+  I->eraseFromParent();
+  return true;
+}
+
+bool LoongArchExpandPseudo::expandAtomicBinOpSubword(
+    MachineBasicBlock &BB, MachineBasicBlock::iterator I,
+    MachineBasicBlock::iterator &NMBBI) {
+
+  MachineFunction *MF = BB.getParent();
+
+  DebugLoc DL = I->getDebugLoc();
+  unsigned LL, SC;
+  unsigned BEQ = LoongArch::BEQ32;
+  unsigned SEOp = LoongArch::EXT_W_H32;
+
+  LL = LoongArch::LL_W;
+  SC = LoongArch::SC_W;
+
+  bool IsSwap = false;
+  bool IsNand = false;
+
+  unsigned Opcode = 0;
+  switch (I->getOpcode()) {
+  case LoongArch::ATOMIC_LOAD_NAND_I8_POSTRA:
+    SEOp = LoongArch::EXT_W_B32;
+    LLVM_FALLTHROUGH;
+  case LoongArch::ATOMIC_LOAD_NAND_I16_POSTRA:
+    IsNand = true;
+    break;
+  case LoongArch::ATOMIC_SWAP_I8_POSTRA:
+    SEOp = LoongArch::EXT_W_B32;
+    LLVM_FALLTHROUGH;
+  case LoongArch::ATOMIC_SWAP_I16_POSTRA:
+    IsSwap = true;
+    break;
+  case LoongArch::ATOMIC_LOAD_ADD_I8_POSTRA:
+    SEOp = LoongArch::EXT_W_B32;
+    LLVM_FALLTHROUGH;
+  case LoongArch::ATOMIC_LOAD_ADD_I16_POSTRA:
+    Opcode = LoongArch::ADD_W;
+    break;
+  case LoongArch::ATOMIC_LOAD_MAX_I8_POSTRA:
+    SEOp = LoongArch::EXT_W_B32;
+    LLVM_FALLTHROUGH;
+  case LoongArch::ATOMIC_LOAD_MAX_I16_POSTRA:
+    Opcode = LoongArch::AMMAX_DB_W;
+    break;
+  case LoongArch::ATOMIC_LOAD_MIN_I8_POSTRA:
+    SEOp = LoongArch::EXT_W_B32;
+    LLVM_FALLTHROUGH;
+  case LoongArch::ATOMIC_LOAD_MIN_I16_POSTRA:
+    Opcode = LoongArch::AMMIN_DB_W;
+    break;
+  case LoongArch::ATOMIC_LOAD_UMAX_I8_POSTRA:
+    SEOp = LoongArch::EXT_W_B32;
+    LLVM_FALLTHROUGH;
+  case LoongArch::ATOMIC_LOAD_UMAX_I16_POSTRA:
+    Opcode = LoongArch::AMMAX_DB_WU;
+    break;
+  case LoongArch::ATOMIC_LOAD_UMIN_I8_POSTRA:
+    SEOp = LoongArch::EXT_W_B32;
+    LLVM_FALLTHROUGH;
+  case LoongArch::ATOMIC_LOAD_UMIN_I16_POSTRA:
+    Opcode = LoongArch::AMMIN_DB_WU;
+    break;
+  case LoongArch::ATOMIC_LOAD_SUB_I8_POSTRA:
+    SEOp = LoongArch::EXT_W_B32;
+    LLVM_FALLTHROUGH;
+  case LoongArch::ATOMIC_LOAD_SUB_I16_POSTRA:
+    Opcode = LoongArch::SUB_W;
+    break;
+  case LoongArch::ATOMIC_LOAD_AND_I8_POSTRA:
+    SEOp = LoongArch::EXT_W_B32;
+    LLVM_FALLTHROUGH;
+  case LoongArch::ATOMIC_LOAD_AND_I16_POSTRA:
+    Opcode = LoongArch::AND32;
+    break;
+  case LoongArch::ATOMIC_LOAD_OR_I8_POSTRA:
+    SEOp = LoongArch::EXT_W_B32;
+    LLVM_FALLTHROUGH;
+  case LoongArch::ATOMIC_LOAD_OR_I16_POSTRA:
+    Opcode = LoongArch::OR32;
+    break;
+  case LoongArch::ATOMIC_LOAD_XOR_I8_POSTRA:
+    SEOp = LoongArch::EXT_W_B32;
+    LLVM_FALLTHROUGH;
+  case LoongArch::ATOMIC_LOAD_XOR_I16_POSTRA:
+    Opcode = LoongArch::XOR32;
+    break;
+  default:
+    llvm_unreachable("Unknown subword atomic pseudo for expansion!");
+  }
+
+  unsigned Dest = I->getOperand(0).getReg();
+  unsigned Ptr = I->getOperand(1).getReg();
+  unsigned Incr = I->getOperand(2).getReg();
+  unsigned Mask = I->getOperand(3).getReg();
+  unsigned Mask2 = I->getOperand(4).getReg();
+  unsigned ShiftAmnt = I->getOperand(5).getReg();
+  unsigned OldVal = I->getOperand(6).getReg();
+  unsigned BinOpRes = I->getOperand(7).getReg();
+  unsigned StoreVal = I->getOperand(8).getReg();
+
+  const BasicBlock *LLVM_BB = BB.getBasicBlock();
+  MachineBasicBlock *loopMBB = MF->CreateMachineBasicBlock(LLVM_BB);
+  MachineBasicBlock *sinkMBB = MF->CreateMachineBasicBlock(LLVM_BB);
+  MachineBasicBlock *exitMBB = MF->CreateMachineBasicBlock(LLVM_BB);
+  MachineFunction::iterator It = ++BB.getIterator();
+  MF->insert(It, loopMBB);
+  MF->insert(It, sinkMBB);
+  MF->insert(It, exitMBB);
+
+  exitMBB->splice(exitMBB->begin(), &BB, std::next(I), BB.end());
+  exitMBB->transferSuccessorsAndUpdatePHIs(&BB);
+
+  BB.addSuccessor(loopMBB, BranchProbability::getOne());
+  loopMBB->addSuccessor(sinkMBB);
+  loopMBB->addSuccessor(loopMBB);
+  loopMBB->normalizeSuccProbs();
+
+  BuildMI(loopMBB, DL, TII->get(LL), OldVal).addReg(Ptr).addImm(0);
+  if (IsNand) {
+    //  and andres, oldval, incr2
+    //  nor binopres, $0, andres
+    //  and newval, binopres, mask
+    BuildMI(loopMBB, DL, TII->get(LoongArch::AND32), BinOpRes)
+        .addReg(OldVal)
+        .addReg(Incr);
+    BuildMI(loopMBB, DL, TII->get(LoongArch::NOR32), BinOpRes)
+        .addReg(LoongArch::ZERO)
+        .addReg(BinOpRes);
+    BuildMI(loopMBB, DL, TII->get(LoongArch::AND32), BinOpRes)
+        .addReg(BinOpRes)
+        .addReg(Mask);
+  } else if (!IsSwap) {
+    //  <binop> binopres, oldval, incr2
+    //  and newval, binopres, mask
+    BuildMI(loopMBB, DL, TII->get(Opcode), BinOpRes)
+        .addReg(OldVal)
+        .addReg(Incr);
+    BuildMI(loopMBB, DL, TII->get(LoongArch::AND32), BinOpRes)
+        .addReg(BinOpRes)
+        .addReg(Mask);
+  } else { // atomic.swap
+    //  and newval, incr2, mask
+    BuildMI(loopMBB, DL, TII->get(LoongArch::AND32), BinOpRes)
+        .addReg(Incr)
+        .addReg(Mask);
+  }
+
+  // and StoreVal, OlddVal, Mask2
+  // or StoreVal, StoreVal, BinOpRes
+  // StoreVal<tied1> = sc StoreVal, 0(Ptr)
+  // beq StoreVal, zero, loopMBB
+  BuildMI(loopMBB, DL, TII->get(LoongArch::AND32), StoreVal)
+    .addReg(OldVal).addReg(Mask2);
+  BuildMI(loopMBB, DL, TII->get(LoongArch::OR32), StoreVal)
+    .addReg(StoreVal).addReg(BinOpRes);
+  BuildMI(loopMBB, DL, TII->get(SC), StoreVal)
+    .addReg(StoreVal).addReg(Ptr).addImm(0);
+  BuildMI(loopMBB, DL, TII->get(BEQ))
+    .addReg(StoreVal).addReg(LoongArch::ZERO).addMBB(loopMBB);
+
+  //  sinkMBB:
+  //    and     maskedoldval1,oldval,mask
+  //    srl     srlres,maskedoldval1,shiftamt
+  //    sign_extend dest,srlres
+
+  sinkMBB->addSuccessor(exitMBB, BranchProbability::getOne());
+
+  BuildMI(sinkMBB, DL, TII->get(LoongArch::AND32), Dest)
+    .addReg(OldVal).addReg(Mask);
+  BuildMI(sinkMBB, DL, TII->get(LoongArch::SRL_W), Dest)
+      .addReg(Dest).addReg(ShiftAmnt);
+
+  BuildMI(sinkMBB, DL, TII->get(SEOp), Dest).addReg(Dest);
+
+  LivePhysRegs LiveRegs;
+  computeAndAddLiveIns(LiveRegs, *loopMBB);
+  computeAndAddLiveIns(LiveRegs, *sinkMBB);
+  computeAndAddLiveIns(LiveRegs, *exitMBB);
+
+  NMBBI = BB.end();
+  I->eraseFromParent();
+
+  return true;
+}
+
+bool LoongArchExpandPseudo::expandAtomicBinOp(MachineBasicBlock &BB,
+                                         MachineBasicBlock::iterator I,
+                                         MachineBasicBlock::iterator &NMBBI,
+                                         unsigned Size) {
+  MachineFunction *MF = BB.getParent();
+
+  DebugLoc DL = I->getDebugLoc();
+
+  unsigned LL, SC, ZERO, BEQ, SUB;
+  if (Size == 4) {
+    LL = LoongArch::LL_W;
+    SC = LoongArch::SC_W;
+    BEQ = LoongArch::BEQ32;
+    ZERO = LoongArch::ZERO;
+    SUB = LoongArch::SUB_W;
+  } else {
+    LL = LoongArch::LL_D;
+    SC = LoongArch::SC_D;
+    ZERO = LoongArch::ZERO_64;
+    BEQ = LoongArch::BEQ;
+    SUB = LoongArch::SUB_D;
+  }
+
+  unsigned OldVal = I->getOperand(0).getReg();
+  unsigned Ptr = I->getOperand(1).getReg();
+  unsigned Incr = I->getOperand(2).getReg();
+  unsigned Scratch = I->getOperand(3).getReg();
+
+  unsigned Opcode = 0;
+  unsigned OR = 0;
+  unsigned AND = 0;
+  unsigned NOR = 0;
+  bool IsNand = false;
+  bool IsSub = false;
+  switch (I->getOpcode()) {
+  case LoongArch::ATOMIC_LOAD_ADD_I32_POSTRA:
+    Opcode = LoongArch::AMADD_DB_W;
+    break;
+  case LoongArch::ATOMIC_LOAD_SUB_I32_POSTRA:
+    IsSub = true;
+    Opcode = LoongArch::AMADD_DB_W;
+    break;
+  case LoongArch::ATOMIC_LOAD_AND_I32_POSTRA:
+    Opcode = LoongArch::AMAND_DB_W;
+    break;
+  case LoongArch::ATOMIC_LOAD_OR_I32_POSTRA:
+    Opcode = LoongArch::AMOR_DB_W;
+    break;
+  case LoongArch::ATOMIC_LOAD_XOR_I32_POSTRA:
+    Opcode = LoongArch::AMXOR_DB_W;
+    break;
+  case LoongArch::ATOMIC_LOAD_NAND_I32_POSTRA:
+    IsNand = true;
+    AND = LoongArch::AND32;
+    NOR = LoongArch::NOR32;
+    break;
+  case LoongArch::ATOMIC_SWAP_I32_POSTRA:
+    OR = LoongArch::AMSWAP_DB_W;
+    break;
+  case LoongArch::ATOMIC_LOAD_MAX_I32_POSTRA:
+    Opcode = LoongArch::AMMAX_DB_W;
+    break;
+  case LoongArch::ATOMIC_LOAD_MIN_I32_POSTRA:
+    Opcode = LoongArch::AMMIN_DB_W;
+    break;
+  case LoongArch::ATOMIC_LOAD_UMAX_I32_POSTRA:
+    Opcode = LoongArch::AMMAX_DB_WU;
+    break;
+  case LoongArch::ATOMIC_LOAD_UMIN_I32_POSTRA:
+    Opcode = LoongArch::AMMIN_DB_WU;
+    break;
+  case LoongArch::ATOMIC_LOAD_ADD_I64_POSTRA:
+    Opcode = LoongArch::AMADD_DB_D;
+    break;
+  case LoongArch::ATOMIC_LOAD_SUB_I64_POSTRA:
+    IsSub = true;
+    Opcode = LoongArch::AMADD_DB_D;
+    break;
+  case LoongArch::ATOMIC_LOAD_AND_I64_POSTRA:
+    Opcode = LoongArch::AMAND_DB_D;
+    break;
+  case LoongArch::ATOMIC_LOAD_OR_I64_POSTRA:
+    Opcode = LoongArch::AMOR_DB_D;
+    break;
+  case LoongArch::ATOMIC_LOAD_XOR_I64_POSTRA:
+    Opcode = LoongArch::AMXOR_DB_D;
+    break;
+  case LoongArch::ATOMIC_LOAD_NAND_I64_POSTRA:
+    IsNand = true;
+    AND = LoongArch::AND;
+    NOR = LoongArch::NOR;
+    break;
+  case LoongArch::ATOMIC_SWAP_I64_POSTRA:
+    OR = LoongArch::AMSWAP_DB_D;
+    break;
+  case LoongArch::ATOMIC_LOAD_MAX_I64_POSTRA:
+    Opcode = LoongArch::AMMAX_DB_D;
+    break;
+  case LoongArch::ATOMIC_LOAD_MIN_I64_POSTRA:
+    Opcode = LoongArch::AMMIN_DB_D;
+    break;
+  case LoongArch::ATOMIC_LOAD_UMAX_I64_POSTRA:
+    Opcode = LoongArch::AMMAX_DB_DU;
+    break;
+  case LoongArch::ATOMIC_LOAD_UMIN_I64_POSTRA:
+    Opcode = LoongArch::AMMIN_DB_DU;
+    break;
+  default:
+    llvm_unreachable("Unknown pseudo atomic!");
+  }
+
+  const BasicBlock *LLVM_BB = BB.getBasicBlock();
+  MachineBasicBlock *loopMBB = MF->CreateMachineBasicBlock(LLVM_BB);
+  MachineBasicBlock *exitMBB = MF->CreateMachineBasicBlock(LLVM_BB);
+  MachineFunction::iterator It = ++BB.getIterator();
+  MF->insert(It, loopMBB);
+  MF->insert(It, exitMBB);
+
+  exitMBB->splice(exitMBB->begin(), &BB, std::next(I), BB.end());
+  exitMBB->transferSuccessorsAndUpdatePHIs(&BB);
+
+  BB.addSuccessor(loopMBB, BranchProbability::getOne());
+  loopMBB->addSuccessor(exitMBB);
+  loopMBB->addSuccessor(loopMBB);
+  loopMBB->normalizeSuccProbs();
+
+  assert((OldVal != Ptr) && "Clobbered the wrong ptr reg!");
+  assert((OldVal != Incr) && "Clobbered the wrong reg!");
+  if (Opcode) {
+    if(IsSub){
+      BuildMI(loopMBB, DL, TII->get(SUB), Scratch).addReg(ZERO).addReg(Incr);
+      BuildMI(loopMBB, DL, TII->get(Opcode), OldVal).addReg(Scratch).addReg(Ptr).addImm(0);
+    }
+    else{
+      BuildMI(loopMBB, DL, TII->get(Opcode), OldVal).addReg(Incr).addReg(Ptr).addImm(0);
+    }
+  } else if (IsNand) {
+    assert(AND && NOR &&
+           "Unknown nand instruction for atomic pseudo expansion");
+    BuildMI(loopMBB, DL, TII->get(LL), OldVal).addReg(Ptr).addImm(0);
+    BuildMI(loopMBB, DL, TII->get(AND), Scratch).addReg(OldVal).addReg(Incr);
+    BuildMI(loopMBB, DL, TII->get(NOR), Scratch).addReg(ZERO).addReg(Scratch);
+    BuildMI(loopMBB, DL, TII->get(SC), Scratch).addReg(Scratch).addReg(Ptr).addImm(0);
+    BuildMI(loopMBB, DL, TII->get(BEQ)).addReg(Scratch).addReg(ZERO).addMBB(loopMBB);
+  } else {
+    assert(OR && "Unknown instruction for atomic pseudo expansion!");
+    BuildMI(loopMBB, DL, TII->get(OR), OldVal).addReg(Incr).addReg(Ptr).addImm(0);
+  }
+
+
+  NMBBI = BB.end();
+  I->eraseFromParent();
+
+  LivePhysRegs LiveRegs;
+  computeAndAddLiveIns(LiveRegs, *loopMBB);
+  computeAndAddLiveIns(LiveRegs, *exitMBB);
+
+  return true;
+}
+
+bool LoongArchExpandPseudo::expandLoadAddr(MachineBasicBlock &BB,
+                                           MachineBasicBlock::iterator I,
+                                           MachineBasicBlock::iterator &NMBBI) {
+  MachineInstr &MI = *I;
+  DebugLoc DL = MI.getDebugLoc();
+
+  unsigned Op = MI.getOpcode();
+  unsigned DestReg = MI.getOperand(0).getReg();
+  unsigned TmpReg;
+  const MachineOperand &MO = MI.getOperand(1);
+
+  MachineInstrBuilder MIB1, MIB2, MIB3, MIB4, MIB5;
+  unsigned HiFlag, LoFlag, HigherFlag, HighestFlag;
+  unsigned HiOp, LoOp, HigherOp, HighestOp, LastOp;
+  bool UseGot = false;
+
+  HiOp = LoongArch::PCADDU12I_ri;
+  LoOp = LoongArch::ORI_rri;
+  HigherOp = LoongArch::LU32I_D_ri;
+  HighestOp = LoongArch::LU52I_D_rri;
+
+  switch (Op) {
+  case LoongArch::LoadAddrLocal:
+    // pcaddu12i + addi.d
+    LoFlag = LoongArchII::MO_PCREL_LO;
+    HiFlag = LoongArchII::MO_PCREL_HI;
+    LoOp = LoongArch::ADDI_D_rri;
+    break;
+  case LoongArch::LoadAddrLocalRR:
+    // pcaddu12i + ori + lu32i.d + lu52i.d + add.d
+    LoFlag = LoongArchII::MO_PCREL_RRLO;
+    HiFlag = LoongArchII::MO_PCREL_RRHI;
+    HigherFlag = LoongArchII::MO_PCREL_RRHIGHER;
+    HighestFlag = LoongArchII::MO_PCREL_RRHIGHEST;
+    LastOp = LoongArch::ADD_D_rrr;
+    break;
+  case LoongArch::LoadAddrGlobal:
+  case LoongArch::LoadAddrGlobal_Alias:
+    // pcaddu12i + ld.d
+    LoFlag = LoongArchII::MO_GOT_LO;
+    HiFlag = LoongArchII::MO_GOT_HI;
+    HiOp = LoongArch::PCADDU12I_rii;
+    LoOp = LoongArch::LD_D_rrii;
+    UseGot = true;
+    break;
+  case LoongArch::LoadAddrGlobalRR:
+    // pcaddu12i + ori + lu32i.d + lu52i.d +ldx.d
+    LoFlag = LoongArchII::MO_GOT_RRLO;
+    HiFlag = LoongArchII::MO_GOT_RRHI;
+    HigherFlag = LoongArchII::MO_GOT_RRHIGHER;
+    HighestFlag = LoongArchII::MO_GOT_RRHIGHEST;
+    HiOp = LoongArch::PCADDU12I_rii;
+    LoOp = LoongArch::ORI_rrii;
+    HigherOp = LoongArch::LU32I_D_rii;
+    HighestOp = LoongArch::LU52I_D_rrii;
+    LastOp = LoongArch::LDX_D_rrr;
+    UseGot = true;
+    break;
+  case LoongArch::LoadAddrTLS_LE:
+    // lu12i.w + ori + lu32i.d + lu52i.d
+    LoFlag = LoongArchII::MO_TLSLE_LO;
+    HiFlag = LoongArchII::MO_TLSLE_HI;
+    HigherFlag = LoongArchII::MO_TLSLE_HIGHER;
+    HighestFlag = LoongArchII::MO_TLSLE_HIGHEST;
+    HiOp = LoongArch::LU12I_W_ri;
+    break;
+  case LoongArch::LoadAddrTLS_IE:
+    // pcaddu12i + ld.d
+    LoFlag = LoongArchII::MO_TLSIE_LO;
+    HiFlag = LoongArchII::MO_TLSIE_HI;
+    HiOp = LoongArch::PCADDU12I_rii;
+    LoOp = LoongArch::LD_D_rrii;
+    UseGot = true;
+    break;
+  case LoongArch::LoadAddrTLS_IE_RR:
+    // pcaddu12i + ori + lu32i.d + lu52i.d +ldx.d
+    LoFlag = LoongArchII::MO_TLSIE_RRLO;
+    HiFlag = LoongArchII::MO_TLSIE_RRHI;
+    HigherFlag = LoongArchII::MO_TLSIE_RRHIGHER;
+    HighestFlag = LoongArchII::MO_TLSIE_RRHIGHEST;
+    HiOp = LoongArch::PCADDU12I_rii;
+    LoOp = LoongArch::ORI_rrii;
+    HigherOp = LoongArch::LU32I_D_rii;
+    HighestOp = LoongArch::LU52I_D_rrii;
+    LastOp = LoongArch::LDX_D_rrr;
+    UseGot = true;
+    break;
+  case LoongArch::LoadAddrTLS_LD:
+  case LoongArch::LoadAddrTLS_GD:
+    // pcaddu12i + addi.d
+    LoFlag = LoongArchII::MO_TLSGD_LO;
+    HiFlag = LoongArchII::MO_TLSGD_HI;
+    HiOp = LoongArch::PCADDU12I_rii;
+    LoOp = LoongArch::ADDI_D_rrii;
+    UseGot = true;
+    break;
+  case LoongArch::LoadAddrTLS_LD_RR:
+  case LoongArch::LoadAddrTLS_GD_RR:
+    // pcaddu12i + ori + lu32i.d + lu52i.d + add.d
+    LoFlag = LoongArchII::MO_TLSGD_RRLO;
+    HiFlag = LoongArchII::MO_TLSGD_RRHI;
+    HigherFlag = LoongArchII::MO_TLSGD_RRHIGHER;
+    HighestFlag = LoongArchII::MO_TLSGD_RRHIGHEST;
+    HiOp = LoongArch::PCADDU12I_rii;
+    LoOp = LoongArch::ORI_rrii;
+    HigherOp = LoongArch::LU32I_D_rii;
+    HighestOp = LoongArch::LU52I_D_rrii;
+    LastOp = LoongArch::ADD_D_rrr;
+    UseGot = true;
+    break;
+  default:
+    break;
+  }
+
+  MIB1 = BuildMI(BB, I, DL, TII->get(HiOp), DestReg);
+
+  switch (Op) {
+  case LoongArch::LoadAddrLocal: // la.local rd, symbol
+  case LoongArch::LoadAddrGlobal: // la.global rd, symbol
+  case LoongArch::LoadAddrGlobal_Alias: // la rd, symbol
+  case LoongArch::LoadAddrTLS_IE: // la.tls.ie rd, symbol
+  case LoongArch::LoadAddrTLS_LD: // la.tls.ld rd, symbol
+  case LoongArch::LoadAddrTLS_GD: // la.tls.gd rd, symbol
+    MIB2 = BuildMI(BB, I, DL, TII->get(LoOp), DestReg)
+      .addReg(DestReg);
+    if (MO.isJTI()) {
+      MIB1.addJumpTableIndex(MO.getIndex(), HiFlag);
+      MIB2.addJumpTableIndex(MO.getIndex(), LoFlag);
+    } else if (MO.isBlockAddress()) {
+      MIB1.addBlockAddress(MO.getBlockAddress(), 0, HiFlag);
+      MIB2.addBlockAddress(MO.getBlockAddress(), 0, LoFlag);
+    } else {
+      MIB1.addDisp(MO, 0, HiFlag);
+      MIB2.addDisp(MO, 0, LoFlag);
+    }
+    if (UseGot == true) {
+      MIB1.addExternalSymbol("_GLOBAL_OFFSET_TABLE_");
+      MIB2.addExternalSymbol("_GLOBAL_OFFSET_TABLE_");
+    }
+    break;
+
+  case LoongArch::LoadAddrLocalRR: //la.local rd, rs, symbol
+  case LoongArch::LoadAddrGlobalRR: // la.global rd, rs, symbol
+  case LoongArch::LoadAddrTLS_IE_RR: // la.tls.ie rd, rs, symbol
+  case LoongArch::LoadAddrTLS_LD_RR: // la.tls.ld rd, rs, symbol
+  case LoongArch::LoadAddrTLS_GD_RR: // la.tls.gd rd, rs, symbol
+    TmpReg = MI.getOperand(MI.getNumOperands()-1).getReg();
+    MIB2 = BuildMI(BB, I, DL, TII->get(LoOp), TmpReg)
+                  .addReg(TmpReg);
+    MIB3 = BuildMI(BB, I, DL, TII->get(HigherOp), TmpReg);
+    MIB4 = BuildMI(BB, I, DL, TII->get(HighestOp), TmpReg)
+                  .addReg(TmpReg);
+    MIB5 = BuildMI(BB, I, DL, TII->get(LastOp), DestReg)
+                  .addReg(DestReg)
+                  .addReg(TmpReg);
+    if (MO.isJTI()) {
+      MIB1.addJumpTableIndex(MO.getIndex(), HiFlag);
+      MIB2.addJumpTableIndex(MO.getIndex(), LoFlag);
+      MIB3.addJumpTableIndex(MO.getIndex(), HigherFlag);
+      MIB4.addJumpTableIndex(MO.getIndex(), HighestFlag);
+    } else if (MO.isBlockAddress()) {
+      MIB1.addBlockAddress(MO.getBlockAddress(), 0, HiFlag);
+      MIB2.addBlockAddress(MO.getBlockAddress(), 0, LoFlag);
+      MIB3.addBlockAddress(MO.getBlockAddress(), 0, HigherFlag);
+      MIB4.addBlockAddress(MO.getBlockAddress(), 0, HighestFlag);
+    } else {
+      MIB1.addDisp(MO, 0, HiFlag);
+      MIB2.addDisp(MO, 0, LoFlag);
+      MIB3.addDisp(MO, 0, HigherFlag);
+      MIB4.addDisp(MO, 0, HighestFlag);
+    }
+    if (UseGot == true) {
+      MIB1.addExternalSymbol("_GLOBAL_OFFSET_TABLE_");
+      MIB2.addExternalSymbol("_GLOBAL_OFFSET_TABLE_");
+      MIB3.addExternalSymbol("_GLOBAL_OFFSET_TABLE_");
+      MIB4.addExternalSymbol("_GLOBAL_OFFSET_TABLE_");
+    }
+    break;
+  case LoongArch::LoadAddrTLS_LE: // la.tls.le rd, symbol
+    MIB2 = BuildMI(BB, I, DL, TII->get(LoOp), DestReg)
+                  .addReg(DestReg);
+    MIB3 = BuildMI(BB, I, DL, TII->get(HigherOp), DestReg);
+    MIB4 = BuildMI(BB, I, DL, TII->get(HighestOp), DestReg)
+                  .addReg(DestReg);
+    if (MO.isJTI()) {
+      MIB1.addJumpTableIndex(MO.getIndex(), HiFlag);
+      MIB2.addJumpTableIndex(MO.getIndex(), LoFlag);
+      MIB3.addJumpTableIndex(MO.getIndex(), HigherFlag);
+      MIB4.addJumpTableIndex(MO.getIndex(), HighestFlag);
+    } else if (MO.isBlockAddress()) {
+      MIB1.addBlockAddress(MO.getBlockAddress(), 0, HiFlag);
+      MIB2.addBlockAddress(MO.getBlockAddress(), 0, LoFlag);
+      MIB3.addBlockAddress(MO.getBlockAddress(), 0, HigherFlag);
+      MIB4.addBlockAddress(MO.getBlockAddress(), 0, HighestFlag);
+    } else {
+      MIB1.addDisp(MO, 0, HiFlag);
+      MIB2.addDisp(MO, 0, LoFlag);
+      MIB3.addDisp(MO, 0, HigherFlag);
+      MIB4.addDisp(MO, 0, HighestFlag);
+    }
+    break;
+  default:
+    break;
+  }
+
+  MI.eraseFromParent();
+
+  return true;
+}
+
+bool LoongArchExpandPseudo::expandPseudoCall(MachineBasicBlock &BB,
+                                           MachineBasicBlock::iterator I,
+                                           MachineBasicBlock::iterator &NMBBI) {
+  MachineFunction *MF = BB.getParent();
+  MachineInstr &MI = *I;
+  DebugLoc DL = MI.getDebugLoc();
+  CodeModel::Model M = MF->getTarget().getCodeModel();
+  Reloc::Model RM = MF->getTarget().getRelocationModel();
+
+  unsigned Ra = LoongArch::RA_64;
+  const MachineOperand &MO = MI.getOperand(0);
+  unsigned HiFlag, LoFlag, HigherFlag, HighestFlag, NoFlag;
+
+  HiFlag = LoongArchII::MO_CALL_HI;
+  LoFlag = LoongArchII::MO_CALL_LO;
+  NoFlag = LoongArchII::MO_NO_FLAG;
+
+  if (RM == Reloc::Static) { // for jit
+    MachineInstrBuilder MIB1, MIB2, MIB3, MIB4, MIB5;
+
+    HiFlag = LoongArchII::MO_ABS_HI;
+    LoFlag = LoongArchII::MO_ABS_LO;
+    HigherFlag = LoongArchII::MO_ABS_HIGHER;
+    HighestFlag = LoongArchII::MO_ABS_HIGHEST;
+    // lu12i.w + ori + lu32i.d + lu52i.d + jirl
+
+    MIB1 = BuildMI(BB, I, DL, TII->get(LoongArch::LU12I_W), Ra);
+    MIB2 = BuildMI(BB, I, DL, TII->get(LoongArch::ORI), Ra)
+                  .addReg(Ra);
+    MIB3 = BuildMI(BB, I, DL, TII->get(LoongArch::LU32I_D), Ra);
+    MIB4 = BuildMI(BB, I, DL, TII->get(LoongArch::LU52I_D), Ra)
+                  .addReg(Ra);
+    MIB5 = BuildMI(BB, I, DL, TII->get(LoongArch::JIRL), Ra)
+                  .addReg(Ra)
+      .addImm(0);
+    if (MO.isSymbol()) {
+      MIB1.addExternalSymbol(MO.getSymbolName(), HiFlag);
+      MIB2.addExternalSymbol(MO.getSymbolName(), LoFlag);
+      MIB3.addExternalSymbol(MO.getSymbolName(), HigherFlag);
+      MIB4.addExternalSymbol(MO.getSymbolName(), HighestFlag);
+    } else {
+      MIB1.addDisp(MO, 0, HiFlag);
+      MIB2.addDisp(MO, 0, LoFlag);
+      MIB3.addDisp(MO, 0, HigherFlag);
+      MIB4.addDisp(MO, 0, HighestFlag);
+    }
+  } else if (M == CodeModel::Large) {
+    // pcaddu18i + jirl
+    MachineInstrBuilder MIB1;
+    MachineInstrBuilder MIB2;
+
+    MIB1 = BuildMI(BB, I, DL, TII->get(LoongArch::PCADDU18I), Ra);
+    MIB2 = BuildMI(BB, I, DL, TII->get(LoongArch::JIRL_CALL), Ra)
+                  .addReg(Ra);
+    if (MO.isSymbol()) {
+      MIB1.addExternalSymbol(MO.getSymbolName(), HiFlag);
+      MIB2.addExternalSymbol(MO.getSymbolName(), LoFlag);
+    } else {
+      MIB1.addDisp(MO, 0, HiFlag);
+      MIB2.addDisp(MO, 0, LoFlag);
+    }
+  } else {
+    // bl
+    MachineInstrBuilder MIB1;
+    MIB1 = BuildMI(BB, I, DL, TII->get(LoongArch::BL));
+    if (MO.isSymbol()) {
+      MIB1.addExternalSymbol(MO.getSymbolName(), NoFlag);
+    } else {
+      MIB1.addDisp(MO, 0, NoFlag);
+    }
+  }
+
+  MI.eraseFromParent();
+
+  return true;
+}
+
+bool LoongArchExpandPseudo::expandPseudoTEQ(MachineBasicBlock &BB,
+                                           MachineBasicBlock::iterator I,
+                                           MachineBasicBlock::iterator &NMBBI) {
+  MachineInstr &MI = *I;
+  DebugLoc DL = MI.getDebugLoc();
+
+  unsigned Divisor = MI.getOperand(0).getReg();
+  unsigned BneOp = LoongArch::BNE;
+  unsigned Zero = LoongArch::ZERO_64;
+
+  // beq $Divisor, $zero, 8
+  BuildMI(BB, I, DL, TII->get(BneOp), Divisor)
+    .addReg(Zero)
+    .addImm(8);
+  // break 7
+  BuildMI(BB, I, DL, TII->get(LoongArch::BREAK))
+    .addImm(7);;
+
+  MI.eraseFromParent();
+
+  return true;
+}
+bool LoongArchExpandPseudo::expandMI(MachineBasicBlock &MBB,
+                                MachineBasicBlock::iterator MBBI,
+                                MachineBasicBlock::iterator &NMBB) {
+
+  bool Modified = false;
+
+  switch (MBBI->getOpcode()) {
+  case LoongArch::PseudoTEQ:
+    return expandPseudoTEQ(MBB, MBBI, NMBB);
+  case LoongArch::PseudoCall:
+    return expandPseudoCall(MBB, MBBI, NMBB);
+  case LoongArch::LoadAddrLocal:
+  case LoongArch::LoadAddrLocalRR:
+  case LoongArch::LoadAddrGlobal:
+  case LoongArch::LoadAddrGlobalRR:
+  case LoongArch::LoadAddrGlobal_Alias:
+  case LoongArch::LoadAddrTLS_LD:
+  case LoongArch::LoadAddrTLS_LD_RR:
+  case LoongArch::LoadAddrTLS_GD:
+  case LoongArch::LoadAddrTLS_GD_RR:
+  case LoongArch::LoadAddrTLS_IE:
+  case LoongArch::LoadAddrTLS_IE_RR:
+  case LoongArch::LoadAddrTLS_LE:
+    return expandLoadAddr(MBB, MBBI, NMBB);
+  case LoongArch::ATOMIC_CMP_SWAP_I32_POSTRA:
+  case LoongArch::ATOMIC_CMP_SWAP_I64_POSTRA:
+    return expandAtomicCmpSwap(MBB, MBBI, NMBB);
+  case LoongArch::ATOMIC_CMP_SWAP_I8_POSTRA:
+  case LoongArch::ATOMIC_CMP_SWAP_I16_POSTRA:
+    return expandAtomicCmpSwapSubword(MBB, MBBI, NMBB);
+  case LoongArch::ATOMIC_SWAP_I8_POSTRA:
+  case LoongArch::ATOMIC_SWAP_I16_POSTRA:
+  case LoongArch::ATOMIC_LOAD_NAND_I8_POSTRA:
+  case LoongArch::ATOMIC_LOAD_NAND_I16_POSTRA:
+  case LoongArch::ATOMIC_LOAD_ADD_I8_POSTRA:
+  case LoongArch::ATOMIC_LOAD_ADD_I16_POSTRA:
+  case LoongArch::ATOMIC_LOAD_SUB_I8_POSTRA:
+  case LoongArch::ATOMIC_LOAD_SUB_I16_POSTRA:
+  case LoongArch::ATOMIC_LOAD_AND_I8_POSTRA:
+  case LoongArch::ATOMIC_LOAD_AND_I16_POSTRA:
+  case LoongArch::ATOMIC_LOAD_OR_I8_POSTRA:
+  case LoongArch::ATOMIC_LOAD_OR_I16_POSTRA:
+  case LoongArch::ATOMIC_LOAD_XOR_I8_POSTRA:
+  case LoongArch::ATOMIC_LOAD_XOR_I16_POSTRA:
+  case LoongArch::ATOMIC_LOAD_MAX_I8_POSTRA:
+  case LoongArch::ATOMIC_LOAD_MAX_I16_POSTRA:
+  case LoongArch::ATOMIC_LOAD_MIN_I8_POSTRA:
+  case LoongArch::ATOMIC_LOAD_MIN_I16_POSTRA:
+  case LoongArch::ATOMIC_LOAD_UMAX_I8_POSTRA:
+  case LoongArch::ATOMIC_LOAD_UMAX_I16_POSTRA:
+  case LoongArch::ATOMIC_LOAD_UMIN_I8_POSTRA:
+  case LoongArch::ATOMIC_LOAD_UMIN_I16_POSTRA:
+    return expandAtomicBinOpSubword(MBB, MBBI, NMBB);
+  case LoongArch::ATOMIC_LOAD_ADD_I32_POSTRA:
+  case LoongArch::ATOMIC_LOAD_SUB_I32_POSTRA:
+  case LoongArch::ATOMIC_LOAD_AND_I32_POSTRA:
+  case LoongArch::ATOMIC_LOAD_OR_I32_POSTRA:
+  case LoongArch::ATOMIC_LOAD_XOR_I32_POSTRA:
+  case LoongArch::ATOMIC_LOAD_NAND_I32_POSTRA:
+  case LoongArch::ATOMIC_SWAP_I32_POSTRA:
+  case LoongArch::ATOMIC_LOAD_MAX_I32_POSTRA:
+  case LoongArch::ATOMIC_LOAD_MIN_I32_POSTRA:
+  case LoongArch::ATOMIC_LOAD_UMAX_I32_POSTRA:
+  case LoongArch::ATOMIC_LOAD_UMIN_I32_POSTRA:
+    return expandAtomicBinOp(MBB, MBBI, NMBB, 4);
+  case LoongArch::ATOMIC_LOAD_ADD_I64_POSTRA:
+  case LoongArch::ATOMIC_LOAD_SUB_I64_POSTRA:
+  case LoongArch::ATOMIC_LOAD_AND_I64_POSTRA:
+  case LoongArch::ATOMIC_LOAD_OR_I64_POSTRA:
+  case LoongArch::ATOMIC_LOAD_XOR_I64_POSTRA:
+  case LoongArch::ATOMIC_LOAD_NAND_I64_POSTRA:
+  case LoongArch::ATOMIC_SWAP_I64_POSTRA:
+  case LoongArch::ATOMIC_LOAD_MAX_I64_POSTRA:
+  case LoongArch::ATOMIC_LOAD_MIN_I64_POSTRA:
+  case LoongArch::ATOMIC_LOAD_UMAX_I64_POSTRA:
+  case LoongArch::ATOMIC_LOAD_UMIN_I64_POSTRA:
+    return expandAtomicBinOp(MBB, MBBI, NMBB, 8);
+  default:
+    return Modified;
+  }
+}
+
+bool LoongArchExpandPseudo::expandMBB(MachineBasicBlock &MBB) {
+  bool Modified = false;
+
+  MachineBasicBlock::iterator MBBI = MBB.begin(), E = MBB.end();
+  while (MBBI != E) {
+    MachineBasicBlock::iterator NMBBI = std::next(MBBI);
+    Modified |= expandMI(MBB, MBBI, NMBBI);
+    MBBI = NMBBI;
+  }
+
+  return Modified;
+}
+
+bool LoongArchExpandPseudo::runOnMachineFunction(MachineFunction &MF) {
+  STI = &static_cast<const LoongArchSubtarget &>(MF.getSubtarget());
+  TII = STI->getInstrInfo();
+
+  bool Modified = false;
+  for (MachineFunction::iterator MFI = MF.begin(), E = MF.end(); MFI != E;
+       ++MFI)
+    Modified |= expandMBB(*MFI);
+
+  if (Modified)
+    MF.RenumberBlocks();
+
+  return Modified;
+}
+
+/// createLoongArchExpandPseudoPass - returns an instance of the pseudo instruction
+/// expansion pass.
+FunctionPass *llvm::createLoongArchExpandPseudoPass() {
+  return new LoongArchExpandPseudo();
+}
diff --git a/llvm/lib/Target/LoongArch/LoongArchFrameLowering.cpp b/llvm/lib/Target/LoongArch/LoongArchFrameLowering.cpp
new file mode 100644
index 000000000000..82113a5a54c2
--- /dev/null
+++ b/llvm/lib/Target/LoongArch/LoongArchFrameLowering.cpp
@@ -0,0 +1,469 @@
+//===-- LoongArchFrameLowering.cpp - LoongArch Frame Information --------------------===//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+//
+// This file contains the LoongArch implementation of TargetFrameLowering class.
+//
+//===----------------------------------------------------------------------===//
+
+#include "LoongArchFrameLowering.h"
+#include "MCTargetDesc/LoongArchBaseInfo.h"
+#include "MCTargetDesc/LoongArchABIInfo.h"
+#include "LoongArchInstrInfo.h"
+#include "LoongArchMachineFunction.h"
+#include "LoongArchTargetMachine.h"
+#include "LoongArchRegisterInfo.h"
+#include "LoongArchSubtarget.h"
+#include "llvm/ADT/BitVector.h"
+#include "llvm/ADT/StringRef.h"
+#include "llvm/ADT/StringSwitch.h"
+#include "llvm/CodeGen/MachineBasicBlock.h"
+#include "llvm/CodeGen/MachineFrameInfo.h"
+#include "llvm/CodeGen/MachineFunction.h"
+#include "llvm/CodeGen/MachineInstr.h"
+#include "llvm/CodeGen/MachineInstrBuilder.h"
+#include "llvm/CodeGen/MachineModuleInfo.h"
+#include "llvm/CodeGen/MachineOperand.h"
+#include "llvm/CodeGen/MachineRegisterInfo.h"
+#include "llvm/CodeGen/RegisterScavenging.h"
+#include "llvm/CodeGen/TargetInstrInfo.h"
+#include "llvm/CodeGen/TargetRegisterInfo.h"
+#include "llvm/CodeGen/TargetSubtargetInfo.h"
+#include "llvm/IR/DataLayout.h"
+#include "llvm/IR/DebugLoc.h"
+#include "llvm/IR/Function.h"
+#include "llvm/MC/MCDwarf.h"
+#include "llvm/MC/MCRegisterInfo.h"
+#include "llvm/MC/MachineLocation.h"
+#include "llvm/Support/CodeGen.h"
+#include "llvm/Support/ErrorHandling.h"
+#include "llvm/Support/MathExtras.h"
+#include "llvm/Target/TargetOptions.h"
+#include <cassert>
+#include <cstdint>
+#include <utility>
+#include <vector>
+
+using namespace llvm;
+
+//===----------------------------------------------------------------------===//
+//
+// Stack Frame Processing methods
+// +----------------------------+
+//
+// The stack is allocated decrementing the stack pointer on
+// the first instruction of a function prologue. Once decremented,
+// all stack references are done thought a positive offset
+// from the stack/frame pointer, so the stack is considering
+// to grow up! Otherwise terrible hacks would have to be made
+// to get this stack ABI compliant :)
+//
+//  The stack frame required by the ABI (after call):
+//  Offset
+//
+//  0                 ----------
+//  4                 Args to pass
+//  .                 Alloca allocations
+//  .                 Local Area
+//  .                 CPU "Callee Saved" Registers
+//  .                 saved FP
+//  .                 saved RA
+//  .                 FPU "Callee Saved" Registers
+//  StackSize         -----------
+//
+// Offset - offset from sp after stack allocation on function prologue
+//
+// The sp is the stack pointer subtracted/added from the stack size
+// at the Prologue/Epilogue
+//
+// References to the previous stack (to obtain arguments) are done
+// with offsets that exceeds the stack size: (stacksize+(4*(num_arg-1))
+//
+// Examples:
+// - reference to the actual stack frame
+//   for any local area var there is smt like : FI >= 0, StackOffset: 4
+//     st.w REGX, SP, 4
+//
+// - reference to previous stack frame
+//   suppose there's a load to the 5th arguments : FI < 0, StackOffset: 16.
+//   The emitted instruction will be something like:
+//     ld.w REGX, SP, 16+StackSize
+//
+// Since the total stack size is unknown on LowerFormalArguments, all
+// stack references (ObjectOffset) created to reference the function
+// arguments, are negative numbers. This way, on eliminateFrameIndex it's
+// possible to detect those references and the offsets are adjusted to
+// their real location.
+//
+//===----------------------------------------------------------------------===//
+//
+LoongArchFrameLowering::LoongArchFrameLowering(const LoongArchSubtarget &STI)
+      : TargetFrameLowering(StackGrowsDown, STI.getStackAlignment(), 0,
+                                    STI.getStackAlignment()), STI(STI) {}
+
+void LoongArchFrameLowering::emitPrologue(MachineFunction &MF,
+                                          MachineBasicBlock &MBB) const {
+  MachineFrameInfo &MFI    = MF.getFrameInfo();
+  LoongArchFunctionInfo *LoongArchFI = MF.getInfo<LoongArchFunctionInfo>();
+
+  const LoongArchInstrInfo &TII =
+      *static_cast<const LoongArchInstrInfo *>(STI.getInstrInfo());
+  const LoongArchRegisterInfo &RegInfo =
+      *static_cast<const LoongArchRegisterInfo *>(STI.getRegisterInfo());
+  MachineBasicBlock::iterator MBBI = MBB.begin();
+  DebugLoc dl;
+  LoongArchABIInfo ABI = STI.getABI();
+  unsigned SP = ABI.GetStackPtr();
+  unsigned FP = ABI.GetFramePtr();
+  unsigned ZERO = ABI.GetNullPtr();
+  unsigned MOVE = ABI.GetGPRMoveOp();
+  unsigned ADDI = ABI.GetPtrAddiOp();
+  unsigned AND = ABI.IsLP64D() ? LoongArch::AND : LoongArch::AND32;
+  unsigned SLLI = ABI.IsLP64D() ? LoongArch::SLLI_D : LoongArch::SLLI_W;
+
+  const TargetRegisterClass *RC = ABI.ArePtrs64bit() ?
+        &LoongArch::GPR64RegClass : &LoongArch::GPR32RegClass;
+
+  // First, compute final stack size.
+  uint64_t StackSize = MFI.getStackSize();
+
+  // No need to allocate space on the stack.
+  if (StackSize == 0 && !MFI.adjustsStack()) return;
+
+  MachineModuleInfo &MMI = MF.getMMI();
+  const MCRegisterInfo *MRI = MMI.getContext().getRegisterInfo();
+
+  // Adjust stack.
+  TII.adjustStackPtr(SP, -StackSize, MBB, MBBI);
+
+  // emit ".cfi_def_cfa_offset StackSize"
+  unsigned CFIIndex = MF.addFrameInst(
+      MCCFIInstruction::cfiDefCfaOffset(nullptr, StackSize));
+  BuildMI(MBB, MBBI, dl, TII.get(TargetOpcode::CFI_INSTRUCTION))
+      .addCFIIndex(CFIIndex);
+
+  const std::vector<CalleeSavedInfo> &CSI = MFI.getCalleeSavedInfo();
+
+  if (!CSI.empty()) {
+    // Find the instruction past the last instruction that saves a callee-saved
+    // register to the stack.
+    for (unsigned i = 0; i < CSI.size(); ++i)
+      ++MBBI;
+
+    // Iterate over list of callee-saved registers and emit .cfi_offset
+    // directives.
+    for (std::vector<CalleeSavedInfo>::const_iterator I = CSI.begin(),
+           E = CSI.end(); I != E; ++I) {
+      int64_t Offset = MFI.getObjectOffset(I->getFrameIdx());
+      unsigned Reg = I->getReg();
+      // If Reg is a double precision register, emit two cfa_offsets,
+      // one for each of the paired single precision registers.
+      if (LoongArch::FGR64RegClass.contains(Reg)) {
+        unsigned Reg0 = MRI->getDwarfRegNum(Reg, true);
+        unsigned Reg1 = MRI->getDwarfRegNum(Reg, true) + 1;
+
+        unsigned CFIIndex = MF.addFrameInst(
+          MCCFIInstruction::createOffset(nullptr, Reg0, Offset));
+        BuildMI(MBB, MBBI, dl, TII.get(TargetOpcode::CFI_INSTRUCTION))
+            .addCFIIndex(CFIIndex);
+
+        CFIIndex = MF.addFrameInst(
+          MCCFIInstruction::createOffset(nullptr, Reg1, Offset + 4));
+        BuildMI(MBB, MBBI, dl, TII.get(TargetOpcode::CFI_INSTRUCTION))
+            .addCFIIndex(CFIIndex);
+      } else {
+        // Reg is either in GPR32 or FGR32.
+        unsigned CFIIndex = MF.addFrameInst(MCCFIInstruction::createOffset(
+            nullptr, MRI->getDwarfRegNum(Reg, true), Offset));
+        BuildMI(MBB, MBBI, dl, TII.get(TargetOpcode::CFI_INSTRUCTION))
+            .addCFIIndex(CFIIndex);
+      }
+    }
+  }
+
+  if (LoongArchFI->callsEhReturn()) {
+    // Insert instructions that spill eh data registers.
+    for (int I = 0; I < 4; ++I) {
+      if (!MBB.isLiveIn(ABI.GetEhDataReg(I)))
+        MBB.addLiveIn(ABI.GetEhDataReg(I));
+      TII.storeRegToStackSlot(MBB, MBBI, ABI.GetEhDataReg(I), false,
+                              LoongArchFI->getEhDataRegFI(I), RC, &RegInfo);
+    }
+
+    // Emit .cfi_offset directives for eh data registers.
+    for (int I = 0; I < 4; ++I) {
+      int64_t Offset = MFI.getObjectOffset(LoongArchFI->getEhDataRegFI(I));
+      unsigned Reg = MRI->getDwarfRegNum(ABI.GetEhDataReg(I), true);
+      unsigned CFIIndex = MF.addFrameInst(
+          MCCFIInstruction::createOffset(nullptr, Reg, Offset));
+      BuildMI(MBB, MBBI, dl, TII.get(TargetOpcode::CFI_INSTRUCTION))
+          .addCFIIndex(CFIIndex);
+    }
+  }
+
+  // if framepointer enabled, set it to point to the stack pointer.
+  if (hasFP(MF)) {
+    // Insert instruction "move $fp, $sp" at this location.
+    BuildMI(MBB, MBBI, dl, TII.get(MOVE), FP).addReg(SP).addReg(ZERO)
+      .setMIFlag(MachineInstr::FrameSetup);
+
+    // emit ".cfi_def_cfa_register $fp"
+    unsigned CFIIndex = MF.addFrameInst(MCCFIInstruction::createDefCfaRegister(
+        nullptr, MRI->getDwarfRegNum(FP, true)));
+    BuildMI(MBB, MBBI, dl, TII.get(TargetOpcode::CFI_INSTRUCTION))
+        .addCFIIndex(CFIIndex);
+
+    if (RegInfo.needsStackRealignment(MF)) {
+      // addiu $Reg, $zero, -MaxAlignment
+      // andi $sp, $sp, $Reg
+      unsigned VR = MF.getRegInfo().createVirtualRegister(RC);
+      assert((Log2(MFI.getMaxAlign()) < 16) &&
+             "Function's alignment size requirement is not supported.");
+      int MaxAlign = -(int)MFI.getMaxAlign().value();
+      int Alignment = (int)MFI.getMaxAlign().value();
+
+      if( Alignment <= 2048 ){
+         BuildMI(MBB, MBBI, dl, TII.get(ADDI), VR).addReg(ZERO) .addImm(MaxAlign);
+         BuildMI(MBB, MBBI, dl, TII.get(AND), SP).addReg(SP).addReg(VR);
+      }else{
+         const unsigned NrBitsToZero = countTrailingZeros((unsigned)Alignment);
+         BuildMI(MBB, MBBI, dl, TII.get(ADDI), VR).addReg(ZERO).addImm(-1);
+         BuildMI(MBB, MBBI, dl, TII.get(SLLI), VR).addReg(VR).addImm(NrBitsToZero);
+         BuildMI(MBB, MBBI, dl, TII.get(AND), SP).addReg(SP).addReg(VR);
+      }
+
+      if (hasBP(MF)) {
+        // move $s7, $sp
+        unsigned BP = STI.isABI_LP64D() ? LoongArch::S7_64 : LoongArch::S7;
+        BuildMI(MBB, MBBI, dl, TII.get(MOVE), BP)
+          .addReg(SP)
+          .addReg(ZERO);
+      }
+    }
+  }
+}
+
+void LoongArchFrameLowering::emitEpilogue(MachineFunction &MF,
+                                          MachineBasicBlock &MBB) const {
+  MachineBasicBlock::iterator MBBI = MBB.getFirstTerminator();
+  MachineFrameInfo &MFI            = MF.getFrameInfo();
+  LoongArchFunctionInfo *LoongArchFI = MF.getInfo<LoongArchFunctionInfo>();
+
+  const LoongArchInstrInfo &TII =
+      *static_cast<const LoongArchInstrInfo *>(STI.getInstrInfo());
+  const LoongArchRegisterInfo &RegInfo =
+      *static_cast<const LoongArchRegisterInfo *>(STI.getRegisterInfo());
+
+  DebugLoc DL = MBBI != MBB.end() ? MBBI->getDebugLoc() : DebugLoc();
+  LoongArchABIInfo ABI = STI.getABI();
+  unsigned SP = ABI.GetStackPtr();
+  unsigned FP = ABI.GetFramePtr();
+  unsigned ZERO = ABI.GetNullPtr();
+  unsigned MOVE = ABI.GetGPRMoveOp();
+
+  // if framepointer enabled, restore the stack pointer.
+  if (hasFP(MF)) {
+    // Find the first instruction that restores a callee-saved register.
+    MachineBasicBlock::iterator I = MBBI;
+
+    for (unsigned i = 0; i < MFI.getCalleeSavedInfo().size(); ++i)
+      --I;
+
+    // Insert instruction "move $sp, $fp" at this location.
+    BuildMI(MBB, I, DL, TII.get(MOVE), SP).addReg(FP).addReg(ZERO);
+  }
+
+  if (LoongArchFI->callsEhReturn()) {
+    const TargetRegisterClass *RC =
+        ABI.ArePtrs64bit() ? &LoongArch::GPR64RegClass : &LoongArch::GPR32RegClass;
+
+    // Find first instruction that restores a callee-saved register.
+    MachineBasicBlock::iterator I = MBBI;
+    for (unsigned i = 0; i < MFI.getCalleeSavedInfo().size(); ++i)
+      --I;
+
+    // Insert instructions that restore eh data registers.
+    for (int J = 0; J < 4; ++J) {
+      TII.loadRegFromStackSlot(MBB, I, ABI.GetEhDataReg(J),
+                               LoongArchFI->getEhDataRegFI(J), RC, &RegInfo);
+    }
+  }
+
+  // Get the number of bytes from FrameInfo
+  uint64_t StackSize = MFI.getStackSize();
+
+  if (!StackSize)
+    return;
+
+  // Adjust stack.
+  TII.adjustStackPtr(SP, StackSize, MBB, MBBI);
+}
+
+int
+LoongArchFrameLowering::getFrameIndexReference(const MachineFunction &MF,
+                                               int FI,
+                                               Register &FrameReg) const {
+  const MachineFrameInfo &MFI = MF.getFrameInfo();
+  LoongArchABIInfo ABI = STI.getABI();
+
+  if (MFI.isFixedObjectIndex(FI))
+    FrameReg = hasFP(MF) ? ABI.GetFramePtr() : ABI.GetStackPtr();
+  else
+    FrameReg = hasBP(MF) ? ABI.GetBasePtr() : ABI.GetStackPtr();
+
+  return MFI.getObjectOffset(FI) + MFI.getStackSize() -
+         getOffsetOfLocalArea() + MFI.getOffsetAdjustment();
+}
+
+bool LoongArchFrameLowering::spillCalleeSavedRegisters(
+    MachineBasicBlock &MBB, MachineBasicBlock::iterator MI,
+    ArrayRef<CalleeSavedInfo> CSI, const TargetRegisterInfo *TRI) const {
+  MachineFunction *MF = MBB.getParent();
+  const TargetInstrInfo &TII = *STI.getInstrInfo();
+
+  for (unsigned i = 0, e = CSI.size(); i != e; ++i) {
+    // Add the callee-saved register as live-in. Do not add if the register is
+    // RA and return address is taken, because it has already been added in
+    // method LoongArchTargetLowering::lowerRETURNADDR.
+    // It's killed at the spill, unless the register is RA and return address
+    // is taken.
+    unsigned Reg = CSI[i].getReg();
+    bool IsRAAndRetAddrIsTaken = (Reg == LoongArch::RA || Reg == LoongArch::RA_64)
+        && MF->getFrameInfo().isReturnAddressTaken();
+    if (!IsRAAndRetAddrIsTaken)
+      MBB.addLiveIn(Reg);
+
+    // Insert the spill to the stack frame.
+    bool IsKill = !IsRAAndRetAddrIsTaken;
+    const TargetRegisterClass *RC = TRI->getMinimalPhysRegClass(Reg);
+    TII.storeRegToStackSlot(MBB, MI, Reg, IsKill,
+                            CSI[i].getFrameIdx(), RC, TRI);
+  }
+
+  return true;
+}
+
+bool
+LoongArchFrameLowering::hasReservedCallFrame(const MachineFunction &MF) const {
+  const MachineFrameInfo &MFI = MF.getFrameInfo();
+  // Reserve call frame if the size of the maximum call frame fits into 12-bit
+  // immediate field and there are no variable sized objects on the stack.
+  // Make sure the second register scavenger spill slot can be accessed with one
+  // instruction.
+  return isInt<12>(MFI.getMaxCallFrameSize() + getStackAlignment()) &&
+    !MFI.hasVarSizedObjects();
+}
+
+/// Mark \p Reg and all registers aliasing it in the bitset.
+static void setAliasRegs(MachineFunction &MF, BitVector &SavedRegs,
+                         unsigned Reg) {
+  const TargetRegisterInfo *TRI = MF.getSubtarget().getRegisterInfo();
+  for (MCRegAliasIterator AI(Reg, TRI, true); AI.isValid(); ++AI)
+    SavedRegs.set(*AI);
+}
+
+void LoongArchFrameLowering::determineCalleeSaves(MachineFunction &MF,
+                                                  BitVector &SavedRegs,
+                                                  RegScavenger *RS) const {
+  TargetFrameLowering::determineCalleeSaves(MF, SavedRegs, RS);
+  const TargetRegisterInfo *TRI = MF.getSubtarget().getRegisterInfo();
+  LoongArchFunctionInfo *LoongArchFI = MF.getInfo<LoongArchFunctionInfo>();
+  LoongArchABIInfo ABI = STI.getABI();
+  unsigned FP = ABI.GetFramePtr();
+  unsigned BP = ABI.IsLP64D() ? LoongArch::S7_64 : LoongArch::S7;
+
+  // Mark $fp as used if function has dedicated frame pointer.
+  if (hasFP(MF))
+    setAliasRegs(MF, SavedRegs, FP);
+  // Mark $s7 as used if function has dedicated base pointer.
+  if (hasBP(MF))
+    setAliasRegs(MF, SavedRegs, BP);
+
+  // Create spill slots for eh data registers if function calls eh_return.
+  if (LoongArchFI->callsEhReturn())
+    LoongArchFI->createEhDataRegsFI();
+
+  // Set scavenging frame index if necessary.
+  uint64_t MaxSPOffset = estimateStackSize(MF);
+
+  // If there is a variable
+  // sized object on the stack, the estimation cannot account for it.
+  if (isIntN(12, MaxSPOffset) &&
+      !MF.getFrameInfo().hasVarSizedObjects())
+    return;
+
+  const TargetRegisterClass &RC =
+      ABI.ArePtrs64bit() ? LoongArch::GPR64RegClass : LoongArch::GPR32RegClass;
+  int FI = MF.getFrameInfo().CreateStackObject(TRI->getSpillSize(RC),
+                                               TRI->getSpillAlign(RC), false);
+  RS->addScavengingFrameIndex(FI);
+}
+
+// hasFP - Return true if the specified function should have a dedicated frame
+// pointer register.  This is true if the function has variable sized allocas,
+// if it needs dynamic stack realignment, if frame pointer elimination is
+// disabled, or if the frame address is taken.
+bool LoongArchFrameLowering::hasFP(const MachineFunction &MF) const {
+  const MachineFrameInfo &MFI = MF.getFrameInfo();
+  const TargetRegisterInfo *TRI = STI.getRegisterInfo();
+
+  return MF.getTarget().Options.DisableFramePointerElim(MF) ||
+      MFI.hasVarSizedObjects() || MFI.isFrameAddressTaken() ||
+      TRI->needsStackRealignment(MF);
+}
+
+bool LoongArchFrameLowering::hasBP(const MachineFunction &MF) const {
+  const MachineFrameInfo &MFI = MF.getFrameInfo();
+  const TargetRegisterInfo *TRI = STI.getRegisterInfo();
+
+  return MFI.hasVarSizedObjects() && TRI->needsStackRealignment(MF);
+}
+
+// Estimate the size of the stack, including the incoming arguments. We need to
+// account for register spills, local objects, reserved call frame and incoming
+// arguments. This is required to determine the largest possible positive offset
+// from $sp so that it can be determined if an emergency spill slot for stack
+// addresses is required.
+uint64_t LoongArchFrameLowering::
+estimateStackSize(const MachineFunction &MF) const {
+  const MachineFrameInfo &MFI = MF.getFrameInfo();
+  const TargetRegisterInfo &TRI = *STI.getRegisterInfo();
+
+  int64_t Size = 0;
+
+  // Iterate over fixed sized objects which are incoming arguments.
+  for (int I = MFI.getObjectIndexBegin(); I != 0; ++I)
+    if (MFI.getObjectOffset(I) > 0)
+      Size += MFI.getObjectSize(I);
+
+  // Conservatively assume all callee-saved registers will be saved.
+  for (const MCPhysReg *R = TRI.getCalleeSavedRegs(&MF); *R; ++R) {
+    unsigned RegSize = TRI.getSpillSize(*TRI.getMinimalPhysRegClass(*R));
+    Size = alignTo(Size + RegSize, RegSize);
+  }
+
+  // Get the size of the rest of the frame objects and any possible reserved
+  // call frame, accounting for alignment.
+  return Size + MFI.estimateStackSize(MF);
+}
+
+// Eliminate ADJCALLSTACKDOWN, ADJCALLSTACKUP pseudo instructions
+MachineBasicBlock::iterator LoongArchFrameLowering::
+eliminateCallFramePseudoInstr(MachineFunction &MF, MachineBasicBlock &MBB,
+                              MachineBasicBlock::iterator I) const {
+  unsigned SP = STI.getABI().IsLP64D() ? LoongArch::SP_64 : LoongArch::SP;
+
+  if (!hasReservedCallFrame(MF)) {
+    int64_t Amount = I->getOperand(0).getImm();
+    if (I->getOpcode() == LoongArch::ADJCALLSTACKDOWN)
+      Amount = -Amount;
+
+    STI.getInstrInfo()->adjustStackPtr(SP, Amount, MBB, I);
+  }
+
+  return MBB.erase(I);
+}
diff --git a/llvm/lib/Target/LoongArch/LoongArchFrameLowering.h b/llvm/lib/Target/LoongArch/LoongArchFrameLowering.h
new file mode 100644
index 000000000000..dff057173215
--- /dev/null
+++ b/llvm/lib/Target/LoongArch/LoongArchFrameLowering.h
@@ -0,0 +1,67 @@
+//===-- LoongArchFrameLowering.h - Define frame lowering for LoongArch ----*- C++ -*-===//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+//
+//
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef LLVM_LIB_TARGET_LOONGARCH_LOONGARCHFRAMELOWERING_H
+#define LLVM_LIB_TARGET_LOONGARCH_LOONGARCHFRAMELOWERING_H
+
+#include "LoongArch.h"
+#include "llvm/CodeGen/TargetFrameLowering.h"
+
+namespace llvm {
+  class LoongArchSubtarget;
+
+class LoongArchFrameLowering : public TargetFrameLowering {
+  const LoongArchSubtarget &STI;
+
+public:
+  explicit LoongArchFrameLowering(const LoongArchSubtarget &STI);
+
+  /// emitProlog/emitEpilog - These methods insert prolog and epilog code into
+  /// the function.
+  void emitPrologue(MachineFunction &MF, MachineBasicBlock &MBB) const override;
+  void emitEpilogue(MachineFunction &MF, MachineBasicBlock &MBB) const override;
+
+  int getFrameIndexReference(const MachineFunction &MF, int FI,
+                             Register &FrameReg) const override;
+
+  bool spillCalleeSavedRegisters(MachineBasicBlock &MBB,
+                                 MachineBasicBlock::iterator MI,
+                                 ArrayRef<CalleeSavedInfo> CSI,
+                                 const TargetRegisterInfo *TRI) const override;
+
+  bool hasReservedCallFrame(const MachineFunction &MF) const override;
+
+  void determineCalleeSaves(MachineFunction &MF, BitVector &SavedRegs,
+                            RegScavenger *RS) const override;
+
+  bool hasFP(const MachineFunction &MF) const override;
+
+  bool hasBP(const MachineFunction &MF) const;
+
+  bool isFPCloseToIncomingSP() const override { return false; }
+
+  bool enableShrinkWrapping(const MachineFunction &MF) const override {
+    return true;
+  }
+
+  MachineBasicBlock::iterator
+  eliminateCallFramePseudoInstr(MachineFunction &MF,
+                                MachineBasicBlock &MBB,
+                                MachineBasicBlock::iterator I) const override;
+
+protected:
+  uint64_t estimateStackSize(const MachineFunction &MF) const;
+};
+
+} // End llvm namespace
+
+#endif
diff --git a/llvm/lib/Target/LoongArch/LoongArchISelDAGToDAG.cpp b/llvm/lib/Target/LoongArch/LoongArchISelDAGToDAG.cpp
new file mode 100644
index 000000000000..e92b08b3e724
--- /dev/null
+++ b/llvm/lib/Target/LoongArch/LoongArchISelDAGToDAG.cpp
@@ -0,0 +1,379 @@
+//===-- LoongArchISelDAGToDAG.cpp - A Dag to Dag Inst Selector for LoongArch --------===//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+//
+// This file defines an instruction selector for the LoongArch target.
+//
+//===----------------------------------------------------------------------===//
+
+#include "LoongArchISelDAGToDAG.h"
+#include "MCTargetDesc/LoongArchBaseInfo.h"
+#include "LoongArch.h"
+#include "LoongArchAnalyzeImmediate.h"
+#include "LoongArchMachineFunction.h"
+#include "LoongArchRegisterInfo.h"
+#include "llvm/CodeGen/MachineConstantPool.h"
+#include "llvm/CodeGen/MachineFrameInfo.h"
+#include "llvm/CodeGen/MachineFunction.h"
+#include "llvm/CodeGen/MachineInstrBuilder.h"
+#include "llvm/CodeGen/MachineRegisterInfo.h"
+#include "llvm/CodeGen/SelectionDAGNodes.h"
+#include "llvm/IR/CFG.h"
+#include "llvm/IR/Dominators.h"
+#include "llvm/IR/GlobalValue.h"
+#include "llvm/IR/Instructions.h"
+#include "llvm/IR/Intrinsics.h"
+#include "llvm/IR/IntrinsicsLoongArch.h"
+#include "llvm/IR/Type.h"
+#include "llvm/Support/Debug.h"
+#include "llvm/Support/ErrorHandling.h"
+#include "llvm/Support/raw_ostream.h"
+#include "llvm/Target/TargetMachine.h"
+using namespace llvm;
+
+#define DEBUG_TYPE "loongarch-isel"
+
+//===----------------------------------------------------------------------===//
+// Instruction Selector Implementation
+//===----------------------------------------------------------------------===//
+
+//===----------------------------------------------------------------------===//
+// LoongArchDAGToDAGISel - LoongArch specific code to select LoongArch machine
+// instructions for SelectionDAG operations.
+//===----------------------------------------------------------------------===//
+
+void LoongArchDAGToDAGISel::getAnalysisUsage(AnalysisUsage &AU) const {
+  AU.addRequired<DominatorTreeWrapperPass>();
+  SelectionDAGISel::getAnalysisUsage(AU);
+}
+
+bool LoongArchDAGToDAGISel::runOnMachineFunction(MachineFunction &MF) {
+  Subtarget = &static_cast<const LoongArchSubtarget &>(MF.getSubtarget());
+  bool Ret = SelectionDAGISel::runOnMachineFunction(MF);
+
+  return Ret;
+}
+
+/// Match frameindex
+bool LoongArchDAGToDAGISel::selectAddrFrameIndex(SDValue Addr, SDValue &Base,
+                                              SDValue &Offset) const {
+  if (FrameIndexSDNode *FIN = dyn_cast<FrameIndexSDNode>(Addr)) {
+    EVT ValTy = Addr.getValueType();
+
+    Base   = CurDAG->getTargetFrameIndex(FIN->getIndex(), ValTy);
+    Offset = CurDAG->getTargetConstant(0, SDLoc(Addr), ValTy);
+    return true;
+  }
+  return false;
+}
+
+/// Match frameindex+offset and frameindex|offset
+bool LoongArchDAGToDAGISel::selectAddrFrameIndexOffset(
+    SDValue Addr, SDValue &Base, SDValue &Offset, unsigned OffsetBits,
+    unsigned ShiftAmount = 0) const {
+  if (CurDAG->isBaseWithConstantOffset(Addr)) {
+    ConstantSDNode *CN = dyn_cast<ConstantSDNode>(Addr.getOperand(1));
+    if (isIntN(OffsetBits + ShiftAmount, CN->getSExtValue())) {
+      EVT ValTy = Addr.getValueType();
+
+      // If the first operand is a FI, get the TargetFI Node
+      if (FrameIndexSDNode *FIN =
+              dyn_cast<FrameIndexSDNode>(Addr.getOperand(0)))
+        Base = CurDAG->getTargetFrameIndex(FIN->getIndex(), ValTy);
+      else {
+        Base = Addr.getOperand(0);
+        // If base is a FI, additional offset calculation is done in
+        // eliminateFrameIndex, otherwise we need to check the alignment
+        const Align Alignment(1ULL << ShiftAmount);
+        if (!isAligned(Alignment, CN->getZExtValue()))
+          return false;
+      }
+
+      Offset = CurDAG->getTargetConstant(CN->getZExtValue(), SDLoc(Addr),
+                                         ValTy);
+      return true;
+    }
+  }
+  return false;
+}
+
+/// ComplexPattern used on LoongArchInstrInfo
+/// Used on LoongArch Load/Store instructions
+bool LoongArchDAGToDAGISel::selectAddrRegImm(SDValue Addr, SDValue &Base,
+                                        SDValue &Offset) const {
+  // if Address is FI, get the TargetFrameIndex.
+  if (selectAddrFrameIndex(Addr, Base, Offset))
+    return true;
+
+  if (!TM.isPositionIndependent()) {
+    if ((Addr.getOpcode() == ISD::TargetExternalSymbol ||
+        Addr.getOpcode() == ISD::TargetGlobalAddress))
+      return false;
+  }
+
+  // Addresses of the form FI+const or FI|const
+  if (selectAddrFrameIndexOffset(Addr, Base, Offset, 12))
+    return true;
+
+  return false;
+}
+
+/// ComplexPattern used on LoongArchInstrInfo
+/// Used on LoongArch Load/Store instructions
+bool LoongArchDAGToDAGISel::selectAddrDefault(SDValue Addr, SDValue &Base,
+                                         SDValue &Offset) const {
+  Base = Addr;
+  Offset = CurDAG->getTargetConstant(0, SDLoc(Addr), Addr.getValueType());
+  return true;
+}
+
+bool LoongArchDAGToDAGISel::selectIntAddr(SDValue Addr, SDValue &Base,
+                                     SDValue &Offset) const {
+  return selectAddrRegImm(Addr, Base, Offset) ||
+    selectAddrDefault(Addr, Base, Offset);
+}
+
+bool LoongArchDAGToDAGISel::selectAddrRegImm12(SDValue Addr, SDValue &Base,
+                                            SDValue &Offset) const {
+  if (selectAddrFrameIndex(Addr, Base, Offset))
+    return true;
+
+  if (selectAddrFrameIndexOffset(Addr, Base, Offset, 12))
+    return true;
+
+  return false;
+}
+
+bool LoongArchDAGToDAGISel::selectIntAddrSImm12(SDValue Addr, SDValue &Base,
+                                           SDValue &Offset) const {
+  if (selectAddrFrameIndex(Addr, Base, Offset))
+    return true;
+
+  if (selectAddrFrameIndexOffset(Addr, Base, Offset, 12))
+    return true;
+
+  return selectAddrDefault(Addr, Base, Offset);
+}
+
+bool LoongArchDAGToDAGISel::selectIntAddrSImm10Lsl1(SDValue Addr, SDValue &Base,
+                                               SDValue &Offset) const {
+  if (selectAddrFrameIndex(Addr, Base, Offset))
+    return true;
+
+  if (selectAddrFrameIndexOffset(Addr, Base, Offset, 10, 1))
+    return true;
+
+  return selectAddrDefault(Addr, Base, Offset);
+}
+
+bool LoongArchDAGToDAGISel::selectIntAddrSImm10(SDValue Addr, SDValue &Base,
+                                               SDValue &Offset) const {
+  if (selectAddrFrameIndex(Addr, Base, Offset))
+    return true;
+
+  if (selectAddrFrameIndexOffset(Addr, Base, Offset, 10))
+    return true;
+
+  return selectAddrDefault(Addr, Base, Offset);
+}
+
+bool LoongArchDAGToDAGISel::selectIntAddrSImm10Lsl2(SDValue Addr, SDValue &Base,
+                                               SDValue &Offset) const {
+  if (selectAddrFrameIndex(Addr, Base, Offset))
+    return true;
+
+  if (selectAddrFrameIndexOffset(Addr, Base, Offset, 10, 2))
+    return true;
+
+  return selectAddrDefault(Addr, Base, Offset);
+}
+
+bool LoongArchDAGToDAGISel::selectIntAddrSImm11Lsl1(SDValue Addr, SDValue &Base,
+                                               SDValue &Offset) const {
+  if (selectAddrFrameIndex(Addr, Base, Offset))
+    return true;
+
+  if (selectAddrFrameIndexOffset(Addr, Base, Offset, 11, 1))
+    return true;
+
+  return selectAddrDefault(Addr, Base, Offset);
+}
+
+bool LoongArchDAGToDAGISel::selectIntAddrSImm9Lsl3(SDValue Addr, SDValue &Base,
+                                               SDValue &Offset) const {
+  if (selectAddrFrameIndex(Addr, Base, Offset))
+    return true;
+
+  if (selectAddrFrameIndexOffset(Addr, Base, Offset, 9, 3))
+    return true;
+
+  return selectAddrDefault(Addr, Base, Offset);
+}
+
+bool LoongArchDAGToDAGISel::selectIntAddrSImm14Lsl2(SDValue Addr, SDValue &Base,
+                                               SDValue &Offset) const {
+  if (selectAddrFrameIndex(Addr, Base, Offset))
+    return true;
+
+  if (selectAddrFrameIndexOffset(Addr, Base, Offset, 14, 2))
+    return true;
+
+  return false;
+}
+
+bool LoongArchDAGToDAGISel::selectIntAddrSImm10Lsl3(SDValue Addr, SDValue &Base,
+                                               SDValue &Offset) const {
+  if (selectAddrFrameIndex(Addr, Base, Offset))
+    return true;
+
+  if (selectAddrFrameIndexOffset(Addr, Base, Offset, 10, 3))
+    return true;
+
+  return selectAddrDefault(Addr, Base, Offset);
+}
+
+bool LoongArchDAGToDAGISel::trySelect(SDNode *Node) {
+  unsigned Opcode = Node->getOpcode();
+  SDLoc DL(Node);
+
+  ///
+  // Instruction Selection not handled by the auto-generated
+  // tablegen selection should be handled here.
+  ///
+  switch(Opcode) {
+  default: break;
+  case ISD::ConstantFP: {
+    ConstantFPSDNode *CN = dyn_cast<ConstantFPSDNode>(Node);
+    if (Node->getValueType(0) == MVT::f64 && CN->isExactlyValue(+0.0)) {
+      if (Subtarget->is64Bit()) {
+        SDValue Zero = CurDAG->getCopyFromReg(CurDAG->getEntryNode(), DL,
+                                              LoongArch::ZERO_64, MVT::i64);
+        ReplaceNode(Node,
+                    CurDAG->getMachineNode(LoongArch::MOVGR2FR_D, DL, MVT::f64, Zero));
+      }
+      return true;
+    }
+    break;
+  }
+
+  case ISD::Constant: {
+    const ConstantSDNode *CN = dyn_cast<ConstantSDNode>(Node);
+    int64_t Imm = CN->getSExtValue();
+    unsigned Size = CN->getValueSizeInBits(0);
+
+    if (isInt<32>(Imm))
+      break;
+
+    LoongArchAnalyzeImmediate AnalyzeImm;
+
+    const LoongArchAnalyzeImmediate::InstSeq &Seq =
+      AnalyzeImm.Analyze(Imm, Size, false);
+
+    LoongArchAnalyzeImmediate::InstSeq::const_iterator Inst = Seq.begin();
+    SDLoc DL(CN);
+    SDNode *RegOpnd;
+    SDValue ImmOpnd = CurDAG->getTargetConstant(SignExtend64<12>(Inst->ImmOpnd),
+                                                DL, MVT::i64);
+
+    // The first instruction can be a LUi which is different from other
+    // instructions (ADDI, ORI and SLL) in that it does not have a register
+    // operand.
+    if (Inst->Opc == LoongArch::LU12I_W)
+      RegOpnd = CurDAG->getMachineNode(Inst->Opc, DL, MVT::i64, ImmOpnd);
+    else
+      RegOpnd =
+        CurDAG->getMachineNode(Inst->Opc, DL, MVT::i64,
+                               CurDAG->getRegister(LoongArch::ZERO_64, MVT::i64),
+                               ImmOpnd);
+
+    // The remaining instructions in the sequence are handled here.
+    for (++Inst; Inst != Seq.end(); ++Inst) {
+      ImmOpnd = CurDAG->getTargetConstant(SignExtend64<12>(Inst->ImmOpnd), DL,
+                                          MVT::i64);
+      RegOpnd = CurDAG->getMachineNode(Inst->Opc, DL, MVT::i64,
+                                       SDValue(RegOpnd, 0), ImmOpnd);
+    }
+
+    ReplaceNode(Node, RegOpnd);
+    return true;
+  }
+
+  }
+
+  return false;
+}
+
+/// Select instructions not customized! Used for
+/// expanded, promoted and normal instructions
+void LoongArchDAGToDAGISel::Select(SDNode *Node) {
+  // If we have a custom node, we already have selected!
+  if (Node->isMachineOpcode()) {
+    LLVM_DEBUG(errs() << "== "; Node->dump(CurDAG); errs() << "\n");
+    Node->setNodeId(-1);
+    return;
+  }
+
+  // See if subclasses can handle this node.
+  if (trySelect(Node))
+    return;
+
+  // Select the default instruction
+  SelectCode(Node);
+}
+
+bool LoongArchDAGToDAGISel::
+SelectInlineAsmMemoryOperand(const SDValue &Op, unsigned ConstraintID,
+                             std::vector<SDValue> &OutOps) {
+  SDValue Base, Offset;
+
+  switch(ConstraintID) {
+  default:
+    llvm_unreachable("Unexpected asm memory constraint");
+  // All memory constraints can at least accept raw pointers.
+  case InlineAsm::Constraint_i:
+    OutOps.push_back(Op);
+    OutOps.push_back(CurDAG->getTargetConstant(0, SDLoc(Op), MVT::i32));
+    return false;
+  case InlineAsm::Constraint_m:
+    if (selectAddrRegImm12(Op, Base, Offset)) {
+      OutOps.push_back(Base);
+      OutOps.push_back(Offset);
+      return false;
+    }
+    OutOps.push_back(Op);
+    OutOps.push_back(CurDAG->getTargetConstant(0, SDLoc(Op), MVT::i32));
+    return false;
+  case InlineAsm::Constraint_R:
+    if (selectAddrRegImm12(Op, Base, Offset)) {
+      OutOps.push_back(Base);
+      OutOps.push_back(Offset);
+      return false;
+    }
+    OutOps.push_back(Op);
+    OutOps.push_back(CurDAG->getTargetConstant(0, SDLoc(Op), MVT::i32));
+    return false;
+  case InlineAsm::Constraint_ZC:
+    if (selectIntAddrSImm14Lsl2(Op, Base, Offset)) {
+      OutOps.push_back(Base);
+      OutOps.push_back(Offset);
+      return false;
+    }
+    OutOps.push_back(Op);
+    OutOps.push_back(CurDAG->getTargetConstant(0, SDLoc(Op), MVT::i32));
+    return false;
+  case InlineAsm::Constraint_ZB:
+    OutOps.push_back(Op);
+    OutOps.push_back(CurDAG->getTargetConstant(0, SDLoc(Op), MVT::i32));
+    return false;
+  }
+  return true;
+}
+
+FunctionPass *llvm::createLoongArchISelDag(LoongArchTargetMachine &TM,
+                                           CodeGenOpt::Level OptLevel) {
+  return new LoongArchDAGToDAGISel(TM, OptLevel);
+}
diff --git a/llvm/lib/Target/LoongArch/LoongArchISelDAGToDAG.h b/llvm/lib/Target/LoongArch/LoongArchISelDAGToDAG.h
new file mode 100644
index 000000000000..9309c2569240
--- /dev/null
+++ b/llvm/lib/Target/LoongArch/LoongArchISelDAGToDAG.h
@@ -0,0 +1,147 @@
+//===---- LoongArchISelDAGToDAG.h - A Dag to Dag Inst Selector for LoongArch --------===//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+//
+// This file defines an instruction selector for the LoongArch target.
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef LLVM_LIB_TARGET_LOONGARCH_LOONGARCHISELDAGTODAG_H
+#define LLVM_LIB_TARGET_LOONGARCH_LOONGARCHISELDAGTODAG_H
+
+#include "LoongArch.h"
+#include "LoongArchSubtarget.h"
+#include "LoongArchTargetMachine.h"
+#include "llvm/CodeGen/SelectionDAGISel.h"
+
+//===----------------------------------------------------------------------===//
+// Instruction Selector Implementation
+//===----------------------------------------------------------------------===//
+
+//===----------------------------------------------------------------------===//
+// LoongArchDAGToDAGISel - LoongArch specific code to select LoongArch machine
+// instructions for SelectionDAG operations.
+//===----------------------------------------------------------------------===//
+namespace llvm {
+
+class LoongArchDAGToDAGISel : public SelectionDAGISel {
+public:
+  explicit LoongArchDAGToDAGISel(LoongArchTargetMachine &TM, CodeGenOpt::Level OL)
+                : SelectionDAGISel(TM, OL), Subtarget(nullptr) {}
+
+  // Pass Name
+  StringRef getPassName() const override {
+    return "LoongArch DAG->DAG Pattern Instruction Selection";
+  }
+
+  bool runOnMachineFunction(MachineFunction &MF) override;
+
+  void getAnalysisUsage(AnalysisUsage &AU) const override;
+
+private:
+  /// Keep a pointer to the LoongArchSubtarget around so that we can make the right
+  /// decision when generating code for different targets.
+  const LoongArchSubtarget *Subtarget;
+  // Include the pieces autogenerated from the target description.
+  #include "LoongArchGenDAGISel.inc"
+
+  bool selectAddrFrameIndex(SDValue Addr, SDValue &Base, SDValue &Offset) const;
+
+  bool selectAddrFrameIndexOffset(SDValue Addr, SDValue &Base, SDValue &Offset,
+                                  unsigned OffsetBits,
+                                  unsigned ShiftAmount) const;
+
+  // Complex Pattern.
+  /// (reg + imm).
+  bool selectAddrRegImm(SDValue Addr, SDValue &Base, SDValue &Offset) const;
+
+  /// Fall back on this function if all else fails.
+  bool selectAddrDefault(SDValue Addr, SDValue &Base, SDValue &Offset) const;
+
+  /// Match integer address pattern.
+  bool selectIntAddr(SDValue Addr, SDValue &Base, SDValue &Offset) const;
+
+  bool selectAddrRegImm12(SDValue Addr, SDValue &Base,
+                         SDValue &Offset) const;
+
+  /// Match addr+simm12 and addr
+  bool selectIntAddrSImm12(SDValue Addr, SDValue &Base,
+                           SDValue &Offset) const;
+
+  bool selectIntAddrSImm10(SDValue Addr, SDValue &Base,
+                           SDValue &Offset) const;
+
+  bool selectIntAddrSImm10Lsl1(SDValue Addr, SDValue &Base,
+                               SDValue &Offset) const;
+
+  bool selectIntAddrSImm10Lsl2(SDValue Addr, SDValue &Base,
+                               SDValue &Offset) const;
+
+  bool selectIntAddrSImm9Lsl3(SDValue Addr, SDValue &Base,
+                              SDValue &Offset) const;
+
+  bool selectIntAddrSImm11Lsl1(SDValue Addr, SDValue &Base,
+                               SDValue &Offset) const;
+
+  bool selectIntAddrSImm14Lsl2(SDValue Addr, SDValue &Base,
+                               SDValue &Offset) const;
+
+  bool selectIntAddrSImm10Lsl3(SDValue Addr, SDValue &Base,
+                               SDValue &Offset) const;
+
+  /// Select constant vector splats.
+  bool selectVSplat(SDNode *N, APInt &Imm, unsigned MinSizeInBits) const;
+  /// Select constant vector splats whose value fits in a given integer.
+  bool selectVSplatCommon(SDValue N, SDValue &Imm, bool Signed,
+                          unsigned ImmBitSize) const;
+  /// Select constant vector splats whose value fits in a uimm1.
+  bool selectVSplatUimm1(SDValue N, SDValue &Imm) const;
+  /// Select constant vector splats whose value fits in a uimm2.
+  bool selectVSplatUimm2(SDValue N, SDValue &Imm) const;
+  /// Select constant vector splats whose value fits in a uimm3.
+  bool selectVSplatUimm3(SDValue N, SDValue &Imm) const;
+  /// Select constant vector splats whose value fits in a uimm4.
+  bool selectVSplatUimm4(SDValue N, SDValue &Imm) const;
+  /// Select constant vector splats whose value fits in a uimm5.
+  bool selectVSplatUimm5(SDValue N, SDValue &Imm) const;
+  /// Select constant vector splats whose value fits in a uimm6.
+  bool selectVSplatUimm6(SDValue N, SDValue &Imm) const;
+  /// Select constant vector splats whose value fits in a uimm8.
+  bool selectVSplatUimm8(SDValue N, SDValue &Imm) const;
+  /// Select constant vector splats whose value fits in a simm5.
+  bool selectVSplatSimm5(SDValue N, SDValue &Imm) const;
+  /// Select constant vector splats whose value is a power of 2.
+  bool selectVSplatUimmPow2(SDValue N, SDValue &Imm) const;
+  /// Select constant vector splats whose value is the inverse of a
+  /// power of 2.
+  bool selectVSplatUimmInvPow2(SDValue N, SDValue &Imm) const;
+  /// Select constant vector splats whose value is a run of set bits
+  /// ending at the most significant bit
+  bool selectVSplatMaskL(SDValue N, SDValue &Imm) const;
+  /// Select constant vector splats whose value is a run of set bits
+  /// starting at bit zero.
+  bool selectVSplatMaskR(SDValue N, SDValue &Imm) const;
+
+  void Select(SDNode *N) override;
+
+  bool trySelect(SDNode *Node);
+
+  // getImm - Return a target constant with the specified value.
+  inline SDValue getImm(const SDNode *Node, uint64_t Imm) {
+    return CurDAG->getTargetConstant(Imm, SDLoc(Node), Node->getValueType(0));
+  }
+
+  bool SelectInlineAsmMemoryOperand(const SDValue &Op,
+                                    unsigned ConstraintID,
+                                    std::vector<SDValue> &OutOps) override;
+};
+
+FunctionPass *createLoongArchISelDag(LoongArchTargetMachine &TM,
+                                       CodeGenOpt::Level OptLevel);
+}
+
+#endif
diff --git a/llvm/lib/Target/LoongArch/LoongArchISelLowering.cpp b/llvm/lib/Target/LoongArch/LoongArchISelLowering.cpp
new file mode 100644
index 000000000000..a85fb4540e42
--- /dev/null
+++ b/llvm/lib/Target/LoongArch/LoongArchISelLowering.cpp
@@ -0,0 +1,3864 @@
+//===- LoongArchISelLowering.cpp - LoongArch DAG Lowering Implementation ------------===//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+//
+// This file defines the interfaces that LoongArch uses to lower LLVM code into a
+// selection DAG.
+//
+//===----------------------------------------------------------------------===//
+
+#include "LoongArchISelLowering.h"
+#include "MCTargetDesc/LoongArchBaseInfo.h"
+#include "MCTargetDesc/LoongArchInstPrinter.h"
+#include "MCTargetDesc/LoongArchMCTargetDesc.h"
+#include "LoongArchCCState.h"
+#include "LoongArchInstrInfo.h"
+#include "LoongArchMachineFunction.h"
+#include "LoongArchRegisterInfo.h"
+#include "LoongArchSubtarget.h"
+#include "LoongArchTargetMachine.h"
+#include "LoongArchTargetObjectFile.h"
+#include "llvm/ADT/APFloat.h"
+#include "llvm/ADT/APInt.h"
+#include "llvm/ADT/ArrayRef.h"
+#include "llvm/ADT/STLExtras.h"
+#include "llvm/ADT/SmallVector.h"
+#include "llvm/ADT/Statistic.h"
+#include "llvm/ADT/StringRef.h"
+#include "llvm/ADT/StringSwitch.h"
+#include "llvm/ADT/Triple.h"
+#include "llvm/CodeGen/CallingConvLower.h"
+#include "llvm/CodeGen/FunctionLoweringInfo.h"
+#include "llvm/CodeGen/ISDOpcodes.h"
+#include "llvm/CodeGen/MachineBasicBlock.h"
+#include "llvm/CodeGen/MachineFrameInfo.h"
+#include "llvm/CodeGen/MachineFunction.h"
+#include "llvm/CodeGen/MachineInstr.h"
+#include "llvm/CodeGen/MachineInstrBuilder.h"
+#include "llvm/CodeGen/MachineJumpTableInfo.h"
+#include "llvm/CodeGen/MachineMemOperand.h"
+#include "llvm/CodeGen/MachineOperand.h"
+#include "llvm/CodeGen/MachineRegisterInfo.h"
+#include "llvm/CodeGen/RuntimeLibcalls.h"
+#include "llvm/CodeGen/SelectionDAG.h"
+#include "llvm/CodeGen/SelectionDAGNodes.h"
+#include "llvm/CodeGen/TargetFrameLowering.h"
+#include "llvm/CodeGen/TargetInstrInfo.h"
+#include "llvm/CodeGen/TargetRegisterInfo.h"
+#include "llvm/CodeGen/TargetSubtargetInfo.h"
+#include "llvm/CodeGen/ValueTypes.h"
+#include "llvm/IR/CallingConv.h"
+#include "llvm/IR/Constants.h"
+#include "llvm/IR/DataLayout.h"
+#include "llvm/IR/DebugLoc.h"
+#include "llvm/IR/DerivedTypes.h"
+#include "llvm/IR/Function.h"
+#include "llvm/IR/GlobalValue.h"
+#include "llvm/IR/Intrinsics.h"
+#include "llvm/IR/IntrinsicsLoongArch.h"
+#include "llvm/IR/Type.h"
+#include "llvm/IR/Value.h"
+#include "llvm/MC/MCContext.h"
+#include "llvm/MC/MCRegisterInfo.h"
+#include "llvm/Support/Casting.h"
+#include "llvm/Support/CodeGen.h"
+#include "llvm/Support/CommandLine.h"
+#include "llvm/Support/Compiler.h"
+#include "llvm/Support/Debug.h"
+#include "llvm/Support/ErrorHandling.h"
+#include "llvm/Support/MachineValueType.h"
+#include "llvm/Support/MathExtras.h"
+#include "llvm/Support/raw_ostream.h"
+#include "llvm/Target/TargetMachine.h"
+#include "llvm/Target/TargetOptions.h"
+#include <algorithm>
+#include <cassert>
+#include <cctype>
+#include <cstdint>
+#include <deque>
+#include <iterator>
+#include <utility>
+#include <vector>
+
+using namespace llvm;
+
+#define DEBUG_TYPE "loongarch-lower"
+
+STATISTIC(NumTailCalls, "Number of tail calls");
+
+static cl::opt<bool>
+NoZeroDivCheck("mnocheck-zero-division", cl::Hidden,
+               cl::desc("LoongArch: Don't trap on integer division by zero."),
+               cl::init(false));
+
+static cl::opt<bool>
+UseLoongArchTailCalls("loongarch-tail-calls", cl::Hidden,
+                      cl::desc("LoongArch: permit tail calls."), cl::init(false));
+
+static const MCPhysReg LoongArch64DPRegs[8] = {
+  LoongArch::F0_64, LoongArch::F1_64, LoongArch::F2_64, LoongArch::F3_64,
+  LoongArch::F4_64, LoongArch::F5_64, LoongArch::F6_64, LoongArch::F7_64
+};
+
+// If I is a shifted mask, set the size (SMSize) and the first bit of the
+// mask (SMLsb), and return true.
+// For example, if I is 0x003ff800, (SMLsb, SMSize) = (11, 11).
+static bool isShiftedMask(uint64_t I, uint64_t &SMLsb, uint64_t &SMSize) {
+  if (!isShiftedMask_64(I))
+    return false;
+
+  SMSize = countPopulation(I);
+  SMLsb = countTrailingZeros(I);
+  return true;
+}
+
+SDValue LoongArchTargetLowering::getTargetNode(GlobalAddressSDNode *N, EVT Ty,
+                                          SelectionDAG &DAG,
+                                          unsigned Flag) const {
+  return DAG.getTargetGlobalAddress(N->getGlobal(), SDLoc(N), Ty, 0, Flag);
+}
+
+SDValue LoongArchTargetLowering::getTargetNode(ExternalSymbolSDNode *N, EVT Ty,
+                                          SelectionDAG &DAG,
+                                          unsigned Flag) const {
+  return DAG.getTargetExternalSymbol(N->getSymbol(), Ty, Flag);
+}
+
+SDValue LoongArchTargetLowering::getTargetNode(BlockAddressSDNode *N, EVT Ty,
+                                          SelectionDAG &DAG,
+                                          unsigned Flag) const {
+  return DAG.getTargetBlockAddress(N->getBlockAddress(), Ty, N->getOffset(), Flag);
+}
+
+SDValue LoongArchTargetLowering::getTargetNode(JumpTableSDNode *N, EVT Ty,
+                                          SelectionDAG &DAG,
+                                          unsigned Flag) const {
+  return DAG.getTargetJumpTable(N->getIndex(), Ty, Flag);
+}
+
+SDValue LoongArchTargetLowering::getTargetNode(ConstantPoolSDNode *N, EVT Ty,
+                                          SelectionDAG &DAG,
+                                          unsigned Flag) const {
+  return DAG.getTargetConstantPool(N->getConstVal(), Ty, N->getAlign(),
+                                   N->getOffset(), Flag);
+}
+
+const char *LoongArchTargetLowering::getTargetNodeName(unsigned Opcode) const {
+  switch ((LoongArchISD::NodeType)Opcode) {
+  case LoongArchISD::FIRST_NUMBER:      break;
+  case LoongArchISD::JmpLink:           return "LoongArchISD::JmpLink";
+  case LoongArchISD::TailCall:          return "LoongArchISD::TailCall";
+  case LoongArchISD::GlobalAddress:     return "LoongArchISD::GlobalAddress";
+  case LoongArchISD::Ret:               return "LoongArchISD::Ret";
+  case LoongArchISD::ERet:              return "LoongArchISD::ERet";
+  case LoongArchISD::EH_RETURN:         return "LoongArchISD::EH_RETURN";
+  case LoongArchISD::FPBrcond:          return "LoongArchISD::FPBrcond";
+  case LoongArchISD::FPCmp:             return "LoongArchISD::FPCmp";
+  case LoongArchISD::CMovFP_T:          return "LoongArchISD::CMovFP_T";
+  case LoongArchISD::CMovFP_F:          return "LoongArchISD::CMovFP_F";
+  case LoongArchISD::TruncIntFP:        return "LoongArchISD::TruncIntFP";
+  case LoongArchISD::DBAR:              return "LoongArchISD::DBAR";
+  case LoongArchISD::BSTRPICK:          return "LoongArchISD::BSTRPICK";
+  case LoongArchISD::BSTRINS:           return "LoongArchISD::BSTRINS";
+  }
+  return nullptr;
+}
+
+LoongArchTargetLowering::LoongArchTargetLowering(const LoongArchTargetMachine &TM,
+                                       const LoongArchSubtarget &STI)
+    : TargetLowering(TM), Subtarget(STI), ABI(TM.getABI()) {
+  // Set up the register classes
+  addRegisterClass(MVT::i32, &LoongArch::GPR32RegClass);
+
+  if (Subtarget.is64Bit())
+    addRegisterClass(MVT::i64, &LoongArch::GPR64RegClass);
+
+  // LoongArch does not have i1 type, so use i32 for
+  // setcc operations results (slt, sgt, ...).
+  setBooleanContents(ZeroOrOneBooleanContent);
+  setBooleanVectorContents(ZeroOrNegativeOneBooleanContent);
+
+  // Load extented operations for i1 types must be promoted
+  for (MVT VT : MVT::integer_valuetypes()) {
+    setLoadExtAction(ISD::EXTLOAD,  VT, MVT::i1,  Promote);
+    setLoadExtAction(ISD::ZEXTLOAD, VT, MVT::i1,  Promote);
+    setLoadExtAction(ISD::SEXTLOAD, VT, MVT::i1,  Promote);
+  }
+
+  // LoongArch doesn't have extending float->double load/store.  Set LoadExtAction
+  // for f32, f16
+  for (MVT VT : MVT::fp_valuetypes()) {
+    setLoadExtAction(ISD::EXTLOAD, VT, MVT::f32, Expand);
+    setLoadExtAction(ISD::EXTLOAD, VT, MVT::f16, Expand);
+  }
+
+  // Set LoadExtAction for f16 vectors to Expand
+  for (MVT VT : MVT::fp_fixedlen_vector_valuetypes()) {
+    MVT F16VT = MVT::getVectorVT(MVT::f16, VT.getVectorNumElements());
+    if (F16VT.isValid())
+      setLoadExtAction(ISD::EXTLOAD, VT, F16VT, Expand);
+  }
+
+  setTruncStoreAction(MVT::f32, MVT::f16, Expand);
+  setTruncStoreAction(MVT::f64, MVT::f16, Expand);
+
+  setTruncStoreAction(MVT::f64, MVT::f32, Expand);
+
+  // Used by legalize types to correctly generate the setcc result.
+  // Without this, every float setcc comes with a AND/OR with the result,
+  // we don't want this, since the fpcmp result goes to a flag register,
+  // which is used implicitly by brcond and select operations.
+  AddPromotedToType(ISD::SETCC, MVT::i1, MVT::i32);
+
+  // LoongArch Custom Operations
+  setOperationAction(ISD::BR_JT,              MVT::Other, Expand);
+  setOperationAction(ISD::GlobalAddress,      MVT::i32,   Custom);
+  setOperationAction(ISD::BlockAddress,       MVT::i32,   Custom);
+  setOperationAction(ISD::GlobalTLSAddress,   MVT::i32,   Custom);
+  setOperationAction(ISD::JumpTable,          MVT::i32,   Custom);
+  setOperationAction(ISD::ConstantPool,       MVT::i32,   Custom);
+  setOperationAction(ISD::SELECT,             MVT::f32,   Custom);
+  setOperationAction(ISD::SELECT,             MVT::f64,   Custom);
+  setOperationAction(ISD::SELECT,             MVT::i32,   Custom);
+  setOperationAction(ISD::SETCC,              MVT::f32,   Custom);
+  setOperationAction(ISD::SETCC,              MVT::f64,   Custom);
+  setOperationAction(ISD::BRCOND,             MVT::Other, Custom);
+  // fcopysign does not use 'custom' in the instruction legalization phase
+  // when using llvm's Intrinsic-'copysign'.
+  //setOperationAction(ISD::FCOPYSIGN,          MVT::f32,   Custom);
+  //setOperationAction(ISD::FCOPYSIGN,          MVT::f64,   Custom);
+  setOperationAction(ISD::FP_TO_SINT,         MVT::i32,   Custom);
+
+  if (Subtarget.is64Bit()) {
+    setOperationAction(ISD::GlobalAddress,      MVT::i64,   Custom);
+    setOperationAction(ISD::BlockAddress,       MVT::i64,   Custom);
+    setOperationAction(ISD::GlobalTLSAddress,   MVT::i64,   Custom);
+    setOperationAction(ISD::JumpTable,          MVT::i64,   Custom);
+    setOperationAction(ISD::ConstantPool,       MVT::i64,   Custom);
+    setOperationAction(ISD::SELECT,             MVT::i64,   Custom);
+    setOperationAction(ISD::LOAD,               MVT::i64,   Legal);
+    setOperationAction(ISD::STORE,              MVT::i64,   Custom);
+    setOperationAction(ISD::FP_TO_SINT,         MVT::i64,   Custom);
+    setOperationAction(ISD::SHL_PARTS,          MVT::i64,   Custom);
+    setOperationAction(ISD::SRA_PARTS,          MVT::i64,   Custom);
+    setOperationAction(ISD::SRL_PARTS,          MVT::i64,   Custom);
+  }
+
+  if (!Subtarget.is64Bit()) {
+    setOperationAction(ISD::SHL_PARTS,          MVT::i32,   Custom);
+    setOperationAction(ISD::SRA_PARTS,          MVT::i32,   Custom);
+    setOperationAction(ISD::SRL_PARTS,          MVT::i32,   Custom);
+  }
+
+  setOperationAction(ISD::EH_DWARF_CFA,         MVT::i32,   Custom);
+  if (Subtarget.is64Bit())
+    setOperationAction(ISD::EH_DWARF_CFA,       MVT::i64,   Custom);
+
+  setOperationAction(ISD::SDIV, MVT::i32, Expand);
+  setOperationAction(ISD::SREM, MVT::i32, Expand);
+  setOperationAction(ISD::UDIV, MVT::i32, Expand);
+  setOperationAction(ISD::UREM, MVT::i32, Expand);
+  setOperationAction(ISD::SDIV, MVT::i64, Expand);
+  setOperationAction(ISD::SREM, MVT::i64, Expand);
+  setOperationAction(ISD::UDIV, MVT::i64, Expand);
+  setOperationAction(ISD::UREM, MVT::i64, Expand);
+
+  // Operations not directly supported by LoongArch.
+  setOperationAction(ISD::BR_CC,             MVT::f32,   Expand);
+  setOperationAction(ISD::BR_CC,             MVT::f64,   Expand);
+  setOperationAction(ISD::BR_CC,             MVT::i32,   Expand);
+  setOperationAction(ISD::BR_CC,             MVT::i64,   Expand);
+  setOperationAction(ISD::SELECT_CC,         MVT::i32,   Expand);
+  setOperationAction(ISD::SELECT_CC,         MVT::i64,   Expand);
+  setOperationAction(ISD::SELECT_CC,         MVT::f32,   Expand);
+  setOperationAction(ISD::SELECT_CC,         MVT::f64,   Expand);
+  setOperationAction(ISD::UINT_TO_FP,        MVT::i32,   Expand);
+  setOperationAction(ISD::UINT_TO_FP,        MVT::i64,   Expand);
+  setOperationAction(ISD::FP_TO_UINT,        MVT::i32,   Expand);
+  setOperationAction(ISD::FP_TO_UINT,        MVT::i64,   Expand);
+  setOperationAction(ISD::SIGN_EXTEND_INREG, MVT::i1,    Expand);
+  setOperationAction(ISD::CTPOP,           MVT::i32,   Expand);
+  setOperationAction(ISD::CTPOP,           MVT::i64,   Expand);
+  setOperationAction(ISD::ROTL,              MVT::i32,   Expand);
+  setOperationAction(ISD::ROTL,              MVT::i64,   Expand);
+  setOperationAction(ISD::DYNAMIC_STACKALLOC, MVT::i32,  Expand);
+  setOperationAction(ISD::DYNAMIC_STACKALLOC, MVT::i64,  Expand);
+
+  setOperationAction(ISD::FSIN,              MVT::f32,   Expand);
+  setOperationAction(ISD::FSIN,              MVT::f64,   Expand);
+  setOperationAction(ISD::FCOS,              MVT::f32,   Expand);
+  setOperationAction(ISD::FCOS,              MVT::f64,   Expand);
+  setOperationAction(ISD::FSINCOS,           MVT::f32,   Expand);
+  setOperationAction(ISD::FSINCOS,           MVT::f64,   Expand);
+  setOperationAction(ISD::FPOW,              MVT::f32,   Expand);
+  setOperationAction(ISD::FPOW,              MVT::f64,   Expand);
+  setOperationAction(ISD::FLOG,              MVT::f32,   Expand);
+  setOperationAction(ISD::FRINT,             MVT::f32,   Legal);
+  setOperationAction(ISD::FRINT,             MVT::f64,   Legal);
+
+  setOperationAction(ISD::FLOG10,            MVT::f32,   Expand);
+  setOperationAction(ISD::FEXP,              MVT::f32,   Expand);
+  setOperationAction(ISD::FMA,               MVT::f32,   Legal);
+  setOperationAction(ISD::FMA,               MVT::f64,   Legal);
+  setOperationAction(ISD::FREM,              MVT::f32,   Expand);
+  setOperationAction(ISD::FREM,              MVT::f64,   Expand);
+
+  setOperationAction(ISD::FMINNUM_IEEE,      MVT::f32,   Legal);
+  setOperationAction(ISD::FMINNUM_IEEE,      MVT::f64,   Legal);
+  setOperationAction(ISD::FMAXNUM_IEEE,      MVT::f32,   Legal);
+  setOperationAction(ISD::FMAXNUM_IEEE,      MVT::f64,   Legal);
+
+  // Lower f16 conversion operations into library calls
+  setOperationAction(ISD::FP16_TO_FP,        MVT::f32,   Expand);
+  setOperationAction(ISD::FP_TO_FP16,        MVT::f32,   Expand);
+  setOperationAction(ISD::FP16_TO_FP,        MVT::f64,   Expand);
+  setOperationAction(ISD::FP_TO_FP16,        MVT::f64,   Expand);
+
+  setOperationAction(ISD::EH_RETURN, MVT::Other, Custom);
+
+  setOperationAction(ISD::VASTART,           MVT::Other, Custom);
+  setOperationAction(ISD::VAARG,             MVT::Other, Custom);
+  setOperationAction(ISD::VACOPY,            MVT::Other, Expand);
+  setOperationAction(ISD::VAEND,             MVT::Other, Expand);
+
+  // Use the default for now
+  setOperationAction(ISD::STACKSAVE,         MVT::Other, Expand);
+  setOperationAction(ISD::STACKRESTORE,      MVT::Other, Expand);
+
+  if (!Subtarget.is64Bit()) {
+    setOperationAction(ISD::ATOMIC_LOAD,     MVT::i64,   Expand);
+    setOperationAction(ISD::ATOMIC_STORE,    MVT::i64,   Expand);
+  }
+
+  if (Subtarget.is64Bit()) {
+    setLoadExtAction(ISD::SEXTLOAD, MVT::i64, MVT::i32, Custom);
+    setLoadExtAction(ISD::ZEXTLOAD, MVT::i64, MVT::i32, Custom);
+    setLoadExtAction(ISD::EXTLOAD, MVT::i64, MVT::i32, Custom);
+    setTruncStoreAction(MVT::i64, MVT::i32, Custom);
+  }
+
+  setOperationAction(ISD::TRAP, MVT::Other, Legal);
+  setOperationAction(ISD::BITREVERSE, MVT::i32, Legal);
+  setOperationAction(ISD::BITREVERSE, MVT::i64, Legal);
+
+  setTargetDAGCombine(ISD::SELECT);
+  setTargetDAGCombine(ISD::AND);
+  setTargetDAGCombine(ISD::OR);
+  setTargetDAGCombine(ISD::AssertZext);
+  setTargetDAGCombine(ISD::SHL);
+
+  if (ABI.IsLP32()) {
+    // These libcalls are not available in 32-bit.
+    setLibcallName(RTLIB::SHL_I128, nullptr);
+    setLibcallName(RTLIB::SRL_I128, nullptr);
+    setLibcallName(RTLIB::SRA_I128, nullptr);
+  }
+
+  if (!Subtarget.useSoftFloat()) {
+    addRegisterClass(MVT::f32, &LoongArch::FGR32RegClass);
+
+    // When dealing with single precision only, use libcalls
+    if (!Subtarget.isSingleFloat()) {
+      if (Subtarget.isFP64bit())
+        addRegisterClass(MVT::f64, &LoongArch::FGR64RegClass);
+    }
+  }
+
+  setOperationAction(ISD::SMUL_LOHI,          MVT::i32, Custom);
+  setOperationAction(ISD::UMUL_LOHI,          MVT::i32, Custom);
+
+  if (Subtarget.is64Bit())
+    setOperationAction(ISD::MUL,              MVT::i64, Custom);
+
+  if (Subtarget.is64Bit()) {
+    setOperationAction(ISD::SMUL_LOHI,        MVT::i64, Custom);
+    setOperationAction(ISD::UMUL_LOHI,        MVT::i64, Custom);
+    setOperationAction(ISD::SDIVREM,          MVT::i64, Custom);
+    setOperationAction(ISD::UDIVREM,          MVT::i64, Custom);
+  }
+
+  setOperationAction(ISD::INTRINSIC_WO_CHAIN, MVT::i64, Custom);
+  setOperationAction(ISD::INTRINSIC_W_CHAIN,  MVT::i64, Custom);
+
+  setOperationAction(ISD::SDIVREM, MVT::i32, Custom);
+  setOperationAction(ISD::UDIVREM, MVT::i32, Custom);
+  setOperationAction(ISD::ATOMIC_FENCE,       MVT::Other, Custom);
+  setOperationAction(ISD::LOAD,               MVT::i32, Legal);
+  setOperationAction(ISD::STORE,              MVT::i32, Custom);
+
+  setTargetDAGCombine(ISD::MUL);
+
+  setOperationAction(ISD::INTRINSIC_WO_CHAIN, MVT::Other, Custom);
+  setOperationAction(ISD::INTRINSIC_W_CHAIN, MVT::Other, Custom);
+  setOperationAction(ISD::INTRINSIC_VOID, MVT::Other, Custom);
+
+  // Replace the accumulator-based multiplies with a
+  // three register instruction.
+  setOperationAction(ISD::SMUL_LOHI, MVT::i32, Expand);
+  setOperationAction(ISD::UMUL_LOHI, MVT::i32, Expand);
+  setOperationAction(ISD::MUL, MVT::i32, Legal);
+  setOperationAction(ISD::MULHS, MVT::i32, Legal);
+  setOperationAction(ISD::MULHU, MVT::i32, Legal);
+
+  // Replace the accumulator-based division/remainder with separate
+  // three register division and remainder instructions.
+  setOperationAction(ISD::SDIVREM, MVT::i32, Expand);
+  setOperationAction(ISD::UDIVREM, MVT::i32, Expand);
+  setOperationAction(ISD::SDIV, MVT::i32, Legal);
+  setOperationAction(ISD::UDIV, MVT::i32, Legal);
+  setOperationAction(ISD::SREM, MVT::i32, Legal);
+  setOperationAction(ISD::UREM, MVT::i32, Legal);
+
+  // Replace the accumulator-based multiplies with a
+  // three register instruction.
+  setOperationAction(ISD::SMUL_LOHI, MVT::i64, Expand);
+  setOperationAction(ISD::UMUL_LOHI, MVT::i64, Expand);
+  setOperationAction(ISD::MUL, MVT::i64, Legal);
+  setOperationAction(ISD::MULHS, MVT::i64, Legal);
+  setOperationAction(ISD::MULHU, MVT::i64, Legal);
+
+  // Replace the accumulator-based division/remainder with separate
+  // three register division and remainder instructions.
+  setOperationAction(ISD::SDIVREM, MVT::i64, Expand);
+  setOperationAction(ISD::UDIVREM, MVT::i64, Expand);
+  setOperationAction(ISD::SDIV, MVT::i64, Legal);
+  setOperationAction(ISD::UDIV, MVT::i64, Legal);
+  setOperationAction(ISD::SREM, MVT::i64, Legal);
+  setOperationAction(ISD::UREM, MVT::i64, Legal);
+
+  MaxGluedStoresPerMemcpy = 4;
+
+  setMinFunctionAlignment(Subtarget.is64Bit() ? Align(8) : Align(4));
+
+  // The arguments on the stack are defined in terms of 4-byte slots on LP32
+  // and 8-byte slots on LPX32/LP64D.
+  setMinStackArgumentAlignment((ABI.IsLPX32() || ABI.IsLP64D()) ? Align(8)
+                                                               : Align(4));
+
+  setStackPointerRegisterToSaveRestore(ABI.IsLP64D() ? LoongArch::SP_64 : LoongArch::SP);
+
+  MaxStoresPerMemcpy = 16;
+
+  computeRegisterProperties(Subtarget.getRegisterInfo());
+}
+
+bool
+LoongArchTargetLowering::allowsMisalignedMemoryAccesses(
+    EVT VT, unsigned, unsigned, MachineMemOperand::Flags, bool *Fast) const {
+  if (Fast)
+    *Fast = true;
+  return true;
+}
+
+EVT LoongArchTargetLowering::getSetCCResultType(const DataLayout &, LLVMContext &,
+                                           EVT VT) const {
+  if (!VT.isVector())
+    return MVT::i32;
+  return VT.changeVectorElementTypeToInteger();
+}
+
+static LoongArch::CondCode condCodeToFCC(ISD::CondCode CC) {
+  switch (CC) {
+  default: llvm_unreachable("Unknown fp condition code!");
+  case ISD::SETEQ:
+  case ISD::SETOEQ: return LoongArch::FCOND_OEQ;
+  case ISD::SETUNE: return LoongArch::FCOND_UNE;
+  case ISD::SETLT:
+  case ISD::SETOLT: return LoongArch::FCOND_OLT;
+  case ISD::SETGT:
+  case ISD::SETOGT: return LoongArch::FCOND_OGT;
+  case ISD::SETLE:
+  case ISD::SETOLE: return LoongArch::FCOND_OLE;
+  case ISD::SETGE:
+  case ISD::SETOGE: return LoongArch::FCOND_OGE;
+  case ISD::SETULT: return LoongArch::FCOND_ULT;
+  case ISD::SETULE: return LoongArch::FCOND_ULE;
+  case ISD::SETUGT: return LoongArch::FCOND_UGT;
+  case ISD::SETUGE: return LoongArch::FCOND_UGE;
+  case ISD::SETUO:  return LoongArch::FCOND_UN;
+  case ISD::SETO:   return LoongArch::FCOND_OR;
+  case ISD::SETNE:
+  case ISD::SETONE: return LoongArch::FCOND_ONE;
+  case ISD::SETUEQ: return LoongArch::FCOND_UEQ;
+  }
+}
+
+/// This function returns true if the floating point conditional branches and
+/// conditional moves which use condition code CC should be inverted.
+static bool invertFPCondCodeUser(LoongArch::CondCode CC) {
+  if (CC >= LoongArch::FCOND_F && CC <= LoongArch::FCOND_SUNE)
+    return false;
+
+  assert((CC >= LoongArch::FCOND_T && CC <= LoongArch::FCOND_GT) &&
+         "Illegal Condition Code");
+
+  return true;
+}
+
+// Creates and returns an FPCmp node from a setcc node.
+// Returns Op if setcc is not a floating point comparison.
+static SDValue createFPCmp(SelectionDAG &DAG, const SDValue &Op) {
+  // must be a SETCC node
+  if (Op.getOpcode() != ISD::SETCC)
+    return Op;
+
+  SDValue LHS = Op.getOperand(0);
+
+  if (!LHS.getValueType().isFloatingPoint())
+    return Op;
+
+  SDValue RHS = Op.getOperand(1);
+  SDLoc DL(Op);
+
+  // Assume the 3rd operand is a CondCodeSDNode. Add code to check the type of
+  // node if necessary.
+  ISD::CondCode CC = cast<CondCodeSDNode>(Op.getOperand(2))->get();
+
+  return DAG.getNode(LoongArchISD::FPCmp, DL, MVT::Glue, LHS, RHS,
+                     DAG.getConstant(condCodeToFCC(CC), DL, MVT::i32));
+}
+
+// Creates and returns a CMovFPT/F node.
+static SDValue createCMovFP(SelectionDAG &DAG, SDValue Cond, SDValue True,
+                            SDValue False, const SDLoc &DL) {
+  ConstantSDNode *CC = cast<ConstantSDNode>(Cond.getOperand(2));
+  bool invert = invertFPCondCodeUser((LoongArch::CondCode)CC->getSExtValue());
+  SDValue FCC0 = DAG.getRegister(LoongArch::FCC0, MVT::i32);
+
+  return DAG.getNode((invert ? LoongArchISD::CMovFP_F : LoongArchISD::CMovFP_T), DL,
+                 True.getValueType(), True, FCC0, False, Cond);
+
+}
+
+static SDValue performSELECTCombine(SDNode *N, SelectionDAG &DAG,
+                                    TargetLowering::DAGCombinerInfo &DCI,
+                                    const LoongArchSubtarget &Subtarget) {
+  if (DCI.isBeforeLegalizeOps())
+    return SDValue();
+
+  SDValue SetCC = N->getOperand(0);
+
+  if ((SetCC.getOpcode() != ISD::SETCC) ||
+      !SetCC.getOperand(0).getValueType().isInteger())
+    return SDValue();
+
+  SDValue False = N->getOperand(2);
+  EVT FalseTy = False.getValueType();
+
+  if (!FalseTy.isInteger())
+    return SDValue();
+
+  ConstantSDNode *FalseC = dyn_cast<ConstantSDNode>(False);
+
+  // If the RHS (False) is 0, we swap the order of the operands
+  // of ISD::SELECT (obviously also inverting the condition) so that we can
+  // take advantage of conditional moves using the $0 register.
+  // Example:
+  //   return (a != 0) ? x : 0;
+  //     load $reg, x
+  //     movz $reg, $0, a
+  if (!FalseC)
+    return SDValue();
+
+  const SDLoc DL(N);
+
+  if (!FalseC->getZExtValue()) {
+    ISD::CondCode CC = cast<CondCodeSDNode>(SetCC.getOperand(2))->get();
+    SDValue True = N->getOperand(1);
+
+    SetCC = DAG.getSetCC(DL, SetCC.getValueType(), SetCC.getOperand(0),
+                         SetCC.getOperand(1),
+                         ISD::getSetCCInverse(CC, SetCC.getValueType()));
+
+    return DAG.getNode(ISD::SELECT, DL, FalseTy, SetCC, False, True);
+  }
+
+  // If both operands are integer constants there's a possibility that we
+  // can do some interesting optimizations.
+  SDValue True = N->getOperand(1);
+  ConstantSDNode *TrueC = dyn_cast<ConstantSDNode>(True);
+
+  if (!TrueC || !True.getValueType().isInteger())
+    return SDValue();
+
+  // We'll also ignore MVT::i64 operands as this optimizations proves
+  // to be ineffective because of the required sign extensions as the result
+  // of a SETCC operator is always MVT::i32 for non-vector types.
+  if (True.getValueType() == MVT::i64)
+    return SDValue();
+
+  int64_t Diff = TrueC->getSExtValue() - FalseC->getSExtValue();
+
+  // 1)  (a < x) ? y : y-1
+  //  slti $reg1, a, x
+  //  addiu $reg2, $reg1, y-1
+  if (Diff == 1)
+    return DAG.getNode(ISD::ADD, DL, SetCC.getValueType(), SetCC, False);
+
+  // 2)  (a < x) ? y-1 : y
+  //  slti $reg1, a, x
+  //  xor $reg1, $reg1, 1
+  //  addiu $reg2, $reg1, y-1
+  if (Diff == -1) {
+    ISD::CondCode CC = cast<CondCodeSDNode>(SetCC.getOperand(2))->get();
+    SetCC = DAG.getSetCC(DL, SetCC.getValueType(), SetCC.getOperand(0),
+                         SetCC.getOperand(1),
+                         ISD::getSetCCInverse(CC, SetCC.getValueType()));
+    return DAG.getNode(ISD::ADD, DL, SetCC.getValueType(), SetCC, True);
+  }
+
+  // Could not optimize.
+  return SDValue();
+}
+
+static SDValue performANDCombine(SDNode *N, SelectionDAG &DAG,
+                                 TargetLowering::DAGCombinerInfo &DCI,
+                                 const LoongArchSubtarget &Subtarget) {
+  if (DCI.isBeforeLegalizeOps())
+    return SDValue();
+
+  SDValue FirstOperand = N->getOperand(0);
+  unsigned FirstOperandOpc = FirstOperand.getOpcode();
+  SDValue Mask = N->getOperand(1);
+  EVT ValTy = N->getValueType(0);
+  SDLoc DL(N);
+
+  uint64_t Lsb = 0, SMLsb, SMSize;
+  ConstantSDNode *CN;
+  SDValue NewOperand;
+  unsigned Opc;
+
+  // Op's second operand must be a shifted mask.
+  if (!(CN = dyn_cast<ConstantSDNode>(Mask)) ||
+      !isShiftedMask(CN->getZExtValue(), SMLsb, SMSize))
+    return SDValue();
+
+  if (FirstOperandOpc == ISD::SRA || FirstOperandOpc == ISD::SRL) {
+    // Pattern match BSTRPICK.
+    //  $dst = and ((sra or srl) $src , lsb), (2**size - 1)
+    //  => bstrpick $dst, $src, lsb+size-1, lsb
+
+    // The second operand of the shift must be an immediate.
+    if (!(CN = dyn_cast<ConstantSDNode>(FirstOperand.getOperand(1))))
+      return SDValue();
+
+    Lsb = CN->getZExtValue();
+
+    // Return if the shifted mask does not start at bit 0 or the sum of its size
+    // and Lsb exceeds the word's size.
+    if (SMLsb != 0 || Lsb + SMSize > ValTy.getSizeInBits())
+      return SDValue();
+
+    Opc = LoongArchISD::BSTRPICK;
+    NewOperand = FirstOperand.getOperand(0);
+  } else {
+    // Pattern match BSTRPICK.
+    //  $dst = and $src, (2**size - 1) , if size > 12
+    //  => bstrpick $dst, $src, lsb+size-1, lsb , lsb = 0
+
+    // If the mask is <= 0xfff, andi can be used instead.
+    if (CN->getZExtValue() <= 0xfff)
+      return SDValue();
+    // Return if the mask doesn't start at position 0.
+    if (SMLsb)
+      return SDValue();
+
+    Opc = LoongArchISD::BSTRPICK;
+    NewOperand = FirstOperand;
+  }
+  return DAG.getNode(Opc, DL, ValTy, NewOperand,
+                     DAG.getConstant((Lsb + SMSize - 1), DL, MVT::i32),
+                     DAG.getConstant(Lsb, DL, MVT::i32));
+}
+
+static SDValue performORCombine(SDNode *N, SelectionDAG &DAG,
+                                TargetLowering::DAGCombinerInfo &DCI,
+                                const LoongArchSubtarget &Subtarget) {
+  // Pattern match BSTRINS.
+  //  $dst = or (and $src1 , mask0), (and (shl $src, lsb), mask1),
+  //  where mask1 = (2**size - 1) << lsb, mask0 = ~mask1
+  //  => bstrins $dst, $src, lsb+size-1, lsb, $src1
+  if (DCI.isBeforeLegalizeOps())
+    return SDValue();
+
+  SDValue And0 = N->getOperand(0), And1 = N->getOperand(1);
+  uint64_t SMLsb0, SMSize0, SMLsb1, SMSize1;
+  ConstantSDNode *CN, *CN1;
+
+  // See if Op's first operand matches (and $src1 , mask0).
+  if (And0.getOpcode() != ISD::AND)
+    return SDValue();
+
+  if (!(CN = dyn_cast<ConstantSDNode>(And0.getOperand(1))) ||
+      !isShiftedMask(~CN->getSExtValue(), SMLsb0, SMSize0))
+    return SDValue();
+
+  // See if Op's second operand matches (and (shl $src, lsb), mask1).
+  if (And1.getOpcode() == ISD::AND &&
+      And1.getOperand(0).getOpcode() == ISD::SHL) {
+
+    if (!(CN = dyn_cast<ConstantSDNode>(And1.getOperand(1))) ||
+        !isShiftedMask(CN->getZExtValue(), SMLsb1, SMSize1))
+      return SDValue();
+
+    // The shift masks must have the same least significant bit and size.
+    if (SMLsb0 != SMLsb1 || SMSize0 != SMSize1)
+      return SDValue();
+
+    SDValue Shl = And1.getOperand(0);
+
+    if (!(CN = dyn_cast<ConstantSDNode>(Shl.getOperand(1))))
+      return SDValue();
+
+    unsigned Shamt = CN->getZExtValue();
+
+    // Return if the shift amount and the first bit position of mask are not the
+    // same.
+    EVT ValTy = N->getValueType(0);
+    if ((Shamt != SMLsb0) || (SMLsb0 + SMSize0 > ValTy.getSizeInBits()))
+      return SDValue();
+
+    SDLoc DL(N);
+    return DAG.getNode(LoongArchISD::BSTRINS, DL, ValTy, Shl.getOperand(0),
+                       DAG.getConstant((SMLsb0 + SMSize0 - 1), DL, MVT::i32),
+                       DAG.getConstant(SMLsb0, DL, MVT::i32),
+                       And0.getOperand(0));
+  } else {
+    // Pattern match BSTRINS.
+    //  $dst = or (and $src, mask0), mask1
+    //  where mask0 = ((1 << SMSize0) -1) << SMLsb0
+    //  => bstrins $dst, $src, SMLsb0+SMSize0-1, SMLsb0
+    if (~CN->getSExtValue() == ((((int64_t)1 << SMSize0) - 1) << SMLsb0) &&
+        (SMSize0 + SMLsb0 <= 64)) {
+      // Check if AND instruction has constant as argument
+      bool isConstCase = And1.getOpcode() != ISD::AND;
+      if (And1.getOpcode() == ISD::AND) {
+        if (!(CN1 = dyn_cast<ConstantSDNode>(And1->getOperand(1))))
+          return SDValue();
+      } else {
+        if (!(CN1 = dyn_cast<ConstantSDNode>(N->getOperand(1))))
+          return SDValue();
+      }
+      // Don't generate BSTRINS if constant OR operand doesn't fit into bits
+      // cleared by constant AND operand.
+      if (CN->getSExtValue() & CN1->getSExtValue())
+        return SDValue();
+
+      SDLoc DL(N);
+      EVT ValTy = N->getOperand(0)->getValueType(0);
+      SDValue Const1;
+      SDValue SrlX;
+      if (!isConstCase) {
+        Const1 = DAG.getConstant(SMLsb0, DL, MVT::i32);
+        SrlX = DAG.getNode(ISD::SRL, DL, And1->getValueType(0), And1, Const1);
+      }
+      return DAG.getNode(
+          LoongArchISD::BSTRINS, DL, N->getValueType(0),
+          isConstCase
+              ? DAG.getConstant(CN1->getSExtValue() >> SMLsb0, DL, ValTy)
+              : SrlX,
+          DAG.getConstant(ValTy.getSizeInBits() / 8 < 8 ? (SMLsb0 + (SMSize0 & 31) - 1)
+                                                        : (SMLsb0 + SMSize0 - 1),
+                          DL, MVT::i32),
+          DAG.getConstant(SMLsb0, DL, MVT::i32),
+          And0->getOperand(0));
+
+    }
+    return SDValue();
+  }
+}
+
+static bool
+shouldTransformMulToShiftsAddsSubs(APInt C, EVT VT,
+                                   SelectionDAG &DAG,
+                                   const LoongArchSubtarget &Subtarget) {
+  // Estimate the number of operations the below transform will turn a
+  // constant multiply into. The number is approximately equal to the minimal
+  // number of powers of two that constant can be broken down to by adding
+  // or subtracting them.
+  //
+  // If we have taken more than 17[1] / 8[2] steps to attempt the
+  // optimization for a native sized value, it is more than likely that this
+  // optimization will make things worse.
+  //
+  // [1] LoongArch64 requires 11 instructions at most to materialize any constant,
+  //     multiplication requires at least 4 cycles, but another cycle (or two)
+  //     to retrieve the result from corresponding registers.
+  //
+  // [2] LoongArch32 requires 2 instructions at most to materialize any constant,
+  //     multiplication requires at least 4 cycles, but another cycle (or two)
+  //     to retrieve the result from corresponding registers.
+  //
+  // TODO:
+  // - MaxSteps needs to consider the `VT` of the constant for the current
+  //   target.
+  // - Consider to perform this optimization after type legalization.
+  //   That allows to remove a workaround for types not supported natively.
+  // - Take in account `-Os, -Oz` flags because this optimization
+  //   increases code size.
+  unsigned MaxSteps = Subtarget.isABI_LP32() ? 8 : 17;
+
+  SmallVector<APInt, 16> WorkStack(1, C);
+  unsigned Steps = 0;
+  unsigned BitWidth = C.getBitWidth();
+
+  while (!WorkStack.empty()) {
+    APInt Val = WorkStack.pop_back_val();
+
+    if (Val == 0 || Val == 1)
+      continue;
+
+    if (Steps >= MaxSteps)
+      return false;
+
+    if (Val.isPowerOf2()) {
+      ++Steps;
+      continue;
+    }
+
+    APInt Floor = APInt(BitWidth, 1) << Val.logBase2();
+    APInt Ceil = Val.isNegative() ? APInt(BitWidth, 0)
+                                  : APInt(BitWidth, 1) << C.ceilLogBase2();
+
+    if ((Val - Floor).ule(Ceil - Val)) {
+      WorkStack.push_back(Floor);
+      WorkStack.push_back(Val - Floor);
+    } else {
+      WorkStack.push_back(Ceil);
+      WorkStack.push_back(Ceil - Val);
+    }
+
+    ++Steps;
+  }
+
+  // If the value being multiplied is not supported natively, we have to pay
+  // an additional legalization cost, conservatively assume an increase in the
+  // cost of 3 instructions per step. This values for this heuristic were
+  // determined experimentally.
+  unsigned RegisterSize = DAG.getTargetLoweringInfo()
+                              .getRegisterType(*DAG.getContext(), VT)
+                              .getSizeInBits();
+  Steps *= (VT.getSizeInBits() != RegisterSize) * 3;
+  if (Steps > 27)
+    return false;
+
+  return true;
+}
+
+static SDValue genConstMult(SDValue X, APInt C, const SDLoc &DL, EVT VT,
+                            EVT ShiftTy, SelectionDAG &DAG) {
+  // Return 0.
+  if (C == 0)
+    return DAG.getConstant(0, DL, VT);
+
+  // Return x.
+  if (C == 1)
+    return X;
+
+  // If c is power of 2, return (shl x, log2(c)).
+  if (C.isPowerOf2())
+    return DAG.getNode(ISD::SHL, DL, VT, X,
+                       DAG.getConstant(C.logBase2(), DL, ShiftTy));
+
+  unsigned BitWidth = C.getBitWidth();
+  APInt Floor = APInt(BitWidth, 1) << C.logBase2();
+  APInt Ceil = C.isNegative() ? APInt(BitWidth, 0) :
+                                APInt(BitWidth, 1) << C.ceilLogBase2();
+
+  // If |c - floor_c| <= |c - ceil_c|,
+  // where floor_c = pow(2, floor(log2(c))) and ceil_c = pow(2, ceil(log2(c))),
+  // return (add constMult(x, floor_c), constMult(x, c - floor_c)).
+  if ((C - Floor).ule(Ceil - C)) {
+    SDValue Op0 = genConstMult(X, Floor, DL, VT, ShiftTy, DAG);
+    SDValue Op1 = genConstMult(X, C - Floor, DL, VT, ShiftTy, DAG);
+    return DAG.getNode(ISD::ADD, DL, VT, Op0, Op1);
+  }
+
+  // If |c - floor_c| > |c - ceil_c|,
+  // return (sub constMult(x, ceil_c), constMult(x, ceil_c - c)).
+  SDValue Op0 = genConstMult(X, Ceil, DL, VT, ShiftTy, DAG);
+  SDValue Op1 = genConstMult(X, Ceil - C, DL, VT, ShiftTy, DAG);
+  return DAG.getNode(ISD::SUB, DL, VT, Op0, Op1);
+}
+
+static SDValue performMULCombine(SDNode *N, SelectionDAG &DAG,
+                                 const TargetLowering::DAGCombinerInfo &DCI,
+                                 const LoongArchTargetLowering *TL,
+                                 const LoongArchSubtarget &Subtarget) {
+  EVT VT = N->getValueType(0);
+
+  if (ConstantSDNode *C = dyn_cast<ConstantSDNode>(N->getOperand(1)))
+    if (!VT.isVector() && shouldTransformMulToShiftsAddsSubs(
+                              C->getAPIntValue(), VT, DAG, Subtarget))
+      return genConstMult(N->getOperand(0), C->getAPIntValue(), SDLoc(N), VT,
+                          TL->getScalarShiftAmountTy(DAG.getDataLayout(), VT),
+                          DAG);
+
+  return SDValue(N, 0);
+}
+
+static SDValue performSRACombine(SDNode *N, SelectionDAG &DAG,
+                                 TargetLowering::DAGCombinerInfo &DCI,
+                                 const LoongArchSubtarget &Subtarget) {
+  return SDValue();
+}
+
+static SDValue performXORCombine(SDNode *N, SelectionDAG &DAG,
+                                 const LoongArchSubtarget &Subtarget) {
+  return SDValue();
+}
+
+SDValue  LoongArchTargetLowering::
+PerformDAGCombine(SDNode *N, DAGCombinerInfo &DCI) const {
+  SelectionDAG &DAG = DCI.DAG;
+  SDValue Val;
+
+  switch (N->getOpcode()) {
+  default: break;
+  case ISD::AND:
+    return performANDCombine(N, DAG, DCI, Subtarget);
+  case ISD::OR:
+    return performORCombine(N, DAG, DCI, Subtarget);
+  case ISD::XOR:
+    return performXORCombine(N, DAG, Subtarget);
+  case ISD::MUL:
+    return performMULCombine(N, DAG, DCI, this, Subtarget);
+  case ISD::SRA:
+    return performSRACombine(N, DAG, DCI, Subtarget);
+  case ISD::SELECT:
+    return performSELECTCombine(N, DAG, DCI, Subtarget);
+  }
+  return SDValue();
+}
+
+static SDValue LowerSUINT_TO_FP(unsigned ExtOpcode, SDValue Op, SelectionDAG &DAG) {
+
+  EVT ResTy = Op->getValueType(0);
+  SDValue Op0 = Op->getOperand(0);
+  EVT ViaTy = Op0->getValueType(0);
+  SDLoc DL(Op);
+
+  if (!ResTy.isVector()) {
+    if(ResTy.getScalarSizeInBits() == ViaTy.getScalarSizeInBits())
+        return DAG.getNode(ISD::BITCAST, DL, ResTy, Op0);
+    else if(ResTy.getScalarSizeInBits() > ViaTy.getScalarSizeInBits()) {
+        Op0 = DAG.getNode(ISD::BITCAST, DL, MVT::f32, Op0);
+        return DAG.getNode(ISD::FP_EXTEND, DL, MVT::f64, Op0);
+    } else {
+        Op0 = DAG.getNode(ISD::BITCAST, DL, MVT::f64, Op0);
+        return DAG.getNode(ISD::TRUNCATE, DL, MVT::f32, Op0);
+    }
+
+  }
+
+   return Op0;
+}
+
+bool LoongArchTargetLowering::isCheapToSpeculateCttz() const {
+  return true;
+}
+
+bool LoongArchTargetLowering::isCheapToSpeculateCtlz() const {
+  return true;
+}
+
+void
+LoongArchTargetLowering::LowerOperationWrapper(SDNode *N,
+                                          SmallVectorImpl<SDValue> &Results,
+                                          SelectionDAG &DAG) const {
+  SDValue Res = LowerOperation(SDValue(N, 0), DAG);
+
+  for (unsigned I = 0, E = Res->getNumValues(); I != E; ++I)
+    Results.push_back(Res.getValue(I));
+}
+
+void
+LoongArchTargetLowering::ReplaceNodeResults(SDNode *N,
+                                       SmallVectorImpl<SDValue> &Results,
+                                       SelectionDAG &DAG) const {
+  return LowerOperationWrapper(N, Results, DAG);
+}
+
+SDValue LoongArchTargetLowering::
+LowerOperation(SDValue Op, SelectionDAG &DAG) const
+{
+  switch (Op.getOpcode())
+  {
+  case ISD::STORE:              return lowerSTORE(Op, DAG);
+  case ISD::INTRINSIC_WO_CHAIN: return lowerINTRINSIC_WO_CHAIN(Op, DAG);
+  case ISD::INTRINSIC_W_CHAIN:  return lowerINTRINSIC_W_CHAIN(Op, DAG);
+  case ISD::INTRINSIC_VOID:     return lowerINTRINSIC_VOID(Op, DAG);
+  case ISD::UINT_TO_FP:         return lowerUINT_TO_FP(Op, DAG);
+  case ISD::SINT_TO_FP:         return lowerSINT_TO_FP(Op, DAG);
+  case ISD::FP_TO_UINT:         return lowerFP_TO_UINT(Op, DAG);
+  case ISD::FP_TO_SINT:         return lowerFP_TO_SINT(Op, DAG);
+  case ISD::BRCOND:             return lowerBRCOND(Op, DAG);
+  case ISD::ConstantPool:       return lowerConstantPool(Op, DAG);
+  case ISD::GlobalAddress:      return lowerGlobalAddress(Op, DAG);
+  case ISD::BlockAddress:       return lowerBlockAddress(Op, DAG);
+  case ISD::GlobalTLSAddress:   return lowerGlobalTLSAddress(Op, DAG);
+  case ISD::JumpTable:          return lowerJumpTable(Op, DAG);
+  case ISD::SELECT:             return lowerSELECT(Op, DAG);
+  case ISD::SETCC:              return lowerSETCC(Op, DAG);
+  case ISD::VASTART:            return lowerVASTART(Op, DAG);
+  case ISD::VAARG:              return lowerVAARG(Op, DAG);
+  case ISD::FCOPYSIGN:          return lowerFCOPYSIGN(Op, DAG);
+  case ISD::FRAMEADDR:          return lowerFRAMEADDR(Op, DAG);
+  case ISD::RETURNADDR:         return lowerRETURNADDR(Op, DAG);
+  case ISD::EH_RETURN:          return lowerEH_RETURN(Op, DAG);
+  case ISD::ATOMIC_FENCE:       return lowerATOMIC_FENCE(Op, DAG);
+  case ISD::SHL_PARTS:          return lowerShiftLeftParts(Op, DAG);
+  case ISD::SRA_PARTS:          return lowerShiftRightParts(Op, DAG, true);
+  case ISD::SRL_PARTS:          return lowerShiftRightParts(Op, DAG, false);
+  case ISD::EH_DWARF_CFA:       return lowerEH_DWARF_CFA(Op, DAG);
+  }
+  return SDValue();
+}
+
+//===----------------------------------------------------------------------===//
+//  Lower helper functions
+//===----------------------------------------------------------------------===//
+
+template <class NodeTy>
+SDValue LoongArchTargetLowering::getAddr(NodeTy *N, SelectionDAG &DAG,
+                                     bool IsLocal) const {
+  SDLoc DL(N);
+  EVT Ty = getPointerTy(DAG.getDataLayout());
+
+  if (isPositionIndependent()) {
+    SDValue Addr = getTargetNode(N, Ty, DAG, 0U);
+    if (IsLocal)
+      // Use PC-relative addressing to access the symbol.
+      return SDValue(DAG.getMachineNode(LoongArch::LoadAddrLocal, DL, Ty, Addr), 0);
+
+    // Use PC-relative addressing to access the GOT for this symbol, then load
+    // the address from the GOT.
+    return SDValue(DAG.getMachineNode(LoongArch::LoadAddrGlobal, DL, Ty, Addr), 0);
+  }
+
+  SDValue Addr = getTargetNode(N, Ty, DAG, 0U);
+  return SDValue(DAG.getMachineNode(LoongArch::LoadAddrLocal, DL, Ty, Addr), 0);
+}
+
+// addLiveIn - This helper function adds the specified physical register to the
+// MachineFunction as a live in value.  It also creates a corresponding
+// virtual register for it.
+static unsigned
+addLiveIn(MachineFunction &MF, unsigned PReg, const TargetRegisterClass *RC)
+{
+  unsigned VReg = MF.getRegInfo().createVirtualRegister(RC);
+  MF.getRegInfo().addLiveIn(PReg, VReg);
+  return VReg;
+}
+
+static MachineBasicBlock *insertDivByZeroTrap(MachineInstr &MI,
+                                              MachineBasicBlock &MBB,
+                                              const TargetInstrInfo &TII,
+                                              bool Is64Bit) {
+  if (NoZeroDivCheck)
+    return &MBB;
+
+  // Insert pseudo instruction(PseudoTEQ), will expand:
+  //   beq $divisor_reg, $zero, 8
+  //   break 7
+  MachineBasicBlock::iterator I(MI);
+  MachineInstrBuilder MIB;
+  MachineOperand &Divisor = MI.getOperand(2);
+  unsigned TeqOp = LoongArch::PseudoTEQ;
+
+  MIB = BuildMI(MBB, std::next(I), MI.getDebugLoc(), TII.get(TeqOp))
+            .addReg(Divisor.getReg(), getKillRegState(Divisor.isKill()));
+
+  // Use the 32-bit sub-register if this is a 64-bit division.
+  //if (Is64Bit)
+  //  MIB->getOperand(0).setSubReg(LoongArch::sub_32);
+
+  // Clear Divisor's kill flag.
+  Divisor.setIsKill(false);
+
+  // We would normally delete the original instruction here but in this case
+  // we only needed to inject an additional instruction rather than replace it.
+
+  return &MBB;
+}
+
+MachineBasicBlock *
+LoongArchTargetLowering::EmitInstrWithCustomInserter(MachineInstr &MI,
+                                                MachineBasicBlock *BB) const {
+  switch (MI.getOpcode()) {
+  default:
+    llvm_unreachable("Unexpected instr type to insert");
+  case LoongArch::ATOMIC_LOAD_ADD_I8:
+    return emitAtomicBinaryPartword(MI, BB, 1);
+  case LoongArch::ATOMIC_LOAD_ADD_I16:
+    return emitAtomicBinaryPartword(MI, BB, 2);
+  case LoongArch::ATOMIC_LOAD_ADD_I32:
+    return emitAtomicBinary(MI, BB);
+  case LoongArch::ATOMIC_LOAD_ADD_I64:
+    return emitAtomicBinary(MI, BB);
+
+  case LoongArch::ATOMIC_LOAD_AND_I8:
+    return emitAtomicBinaryPartword(MI, BB, 1);
+  case LoongArch::ATOMIC_LOAD_AND_I16:
+    return emitAtomicBinaryPartword(MI, BB, 2);
+  case LoongArch::ATOMIC_LOAD_AND_I32:
+    return emitAtomicBinary(MI, BB);
+  case LoongArch::ATOMIC_LOAD_AND_I64:
+    return emitAtomicBinary(MI, BB);
+
+  case LoongArch::ATOMIC_LOAD_OR_I8:
+    return emitAtomicBinaryPartword(MI, BB, 1);
+  case LoongArch::ATOMIC_LOAD_OR_I16:
+    return emitAtomicBinaryPartword(MI, BB, 2);
+  case LoongArch::ATOMIC_LOAD_OR_I32:
+    return emitAtomicBinary(MI, BB);
+  case LoongArch::ATOMIC_LOAD_OR_I64:
+    return emitAtomicBinary(MI, BB);
+
+  case LoongArch::ATOMIC_LOAD_XOR_I8:
+    return emitAtomicBinaryPartword(MI, BB, 1);
+  case LoongArch::ATOMIC_LOAD_XOR_I16:
+    return emitAtomicBinaryPartword(MI, BB, 2);
+  case LoongArch::ATOMIC_LOAD_XOR_I32:
+    return emitAtomicBinary(MI, BB);
+  case LoongArch::ATOMIC_LOAD_XOR_I64:
+    return emitAtomicBinary(MI, BB);
+
+  case LoongArch::ATOMIC_LOAD_NAND_I8:
+    return emitAtomicBinaryPartword(MI, BB, 1);
+  case LoongArch::ATOMIC_LOAD_NAND_I16:
+    return emitAtomicBinaryPartword(MI, BB, 2);
+  case LoongArch::ATOMIC_LOAD_NAND_I32:
+    return emitAtomicBinary(MI, BB);
+  case LoongArch::ATOMIC_LOAD_NAND_I64:
+    return emitAtomicBinary(MI, BB);
+
+  case LoongArch::ATOMIC_LOAD_SUB_I8:
+    return emitAtomicBinaryPartword(MI, BB, 1);
+  case LoongArch::ATOMIC_LOAD_SUB_I16:
+    return emitAtomicBinaryPartword(MI, BB, 2);
+  case LoongArch::ATOMIC_LOAD_SUB_I32:
+    return emitAtomicBinary(MI, BB);
+  case LoongArch::ATOMIC_LOAD_SUB_I64:
+    return emitAtomicBinary(MI, BB);
+
+  case LoongArch::ATOMIC_SWAP_I8:
+    return emitAtomicBinaryPartword(MI, BB, 1);
+  case LoongArch::ATOMIC_SWAP_I16:
+    return emitAtomicBinaryPartword(MI, BB, 2);
+  case LoongArch::ATOMIC_SWAP_I32:
+    return emitAtomicBinary(MI, BB);
+  case LoongArch::ATOMIC_SWAP_I64:
+    return emitAtomicBinary(MI, BB);
+
+  case LoongArch::ATOMIC_LOAD_MAX_I8:
+    return emitAtomicBinaryPartword(MI, BB, 1);
+  case LoongArch::ATOMIC_LOAD_MAX_I16:
+    return emitAtomicBinaryPartword(MI, BB, 2);
+  case LoongArch::ATOMIC_LOAD_MAX_I32:
+    return emitAtomicBinary(MI, BB);
+  case LoongArch::ATOMIC_LOAD_MAX_I64:
+    return emitAtomicBinary(MI, BB);
+
+  case LoongArch::ATOMIC_LOAD_MIN_I8:
+    return emitAtomicBinaryPartword(MI, BB, 1);
+  case LoongArch::ATOMIC_LOAD_MIN_I16:
+    return emitAtomicBinaryPartword(MI, BB, 2);
+  case LoongArch::ATOMIC_LOAD_MIN_I32:
+    return emitAtomicBinary(MI, BB);
+  case LoongArch::ATOMIC_LOAD_MIN_I64:
+    return emitAtomicBinary(MI, BB);
+
+  case LoongArch::ATOMIC_LOAD_UMAX_I8:
+    return emitAtomicBinaryPartword(MI, BB, 1);
+  case LoongArch::ATOMIC_LOAD_UMAX_I16:
+    return emitAtomicBinaryPartword(MI, BB, 2);
+  case LoongArch::ATOMIC_LOAD_UMAX_I32:
+    return emitAtomicBinary(MI, BB);
+  case LoongArch::ATOMIC_LOAD_UMAX_I64:
+    return emitAtomicBinary(MI, BB);
+
+  case LoongArch::ATOMIC_LOAD_UMIN_I8:
+    return emitAtomicBinaryPartword(MI, BB, 1);
+  case LoongArch::ATOMIC_LOAD_UMIN_I16:
+    return emitAtomicBinaryPartword(MI, BB, 2);
+  case LoongArch::ATOMIC_LOAD_UMIN_I32:
+    return emitAtomicBinary(MI, BB);
+  case LoongArch::ATOMIC_LOAD_UMIN_I64:
+    return emitAtomicBinary(MI, BB);
+
+  case LoongArch::ATOMIC_CMP_SWAP_I8:
+    return emitAtomicCmpSwapPartword(MI, BB, 1);
+  case LoongArch::ATOMIC_CMP_SWAP_I16:
+    return emitAtomicCmpSwapPartword(MI, BB, 2);
+  case LoongArch::ATOMIC_CMP_SWAP_I32:
+    return emitAtomicCmpSwap(MI, BB);
+  case LoongArch::ATOMIC_CMP_SWAP_I64:
+    return emitAtomicCmpSwap(MI, BB);
+
+  case LoongArch::PseudoSELECT_I:
+  case LoongArch::PseudoSELECT_I64:
+  case LoongArch::PseudoSELECT_S:
+  case LoongArch::PseudoSELECT_D64:
+    return emitPseudoSELECT(MI, BB, false, LoongArch::BNE32);
+
+  case LoongArch::PseudoSELECTFP_T_I:
+  case LoongArch::PseudoSELECTFP_T_I64:
+  case LoongArch::PseudoSELECTFP_T_S:
+  case LoongArch::PseudoSELECTFP_T_D64:
+    return emitPseudoSELECT(MI, BB, true, LoongArch::BCNEZ);
+
+  case LoongArch::PseudoSELECTFP_F_I:
+  case LoongArch::PseudoSELECTFP_F_I64:
+  case LoongArch::PseudoSELECTFP_F_S:
+  case LoongArch::PseudoSELECTFP_F_D64:
+    return emitPseudoSELECT(MI, BB, true, LoongArch::BCEQZ);
+  case LoongArch::DIV_W:
+  case LoongArch::DIV_WU:
+  case LoongArch::MOD_W:
+  case LoongArch::MOD_WU:
+    return insertDivByZeroTrap(MI, *BB, *Subtarget.getInstrInfo(), false);
+  case LoongArch::DIV_D:
+  case LoongArch::DIV_DU:
+  case LoongArch::MOD_D:
+  case LoongArch::MOD_DU:
+    return insertDivByZeroTrap(MI, *BB, *Subtarget.getInstrInfo(), true);
+  }
+}
+
+const TargetRegisterClass *
+LoongArchTargetLowering::getRepRegClassFor(MVT VT) const {
+  return TargetLowering::getRepRegClassFor(VT);
+}
+
+// This function also handles LoongArch::ATOMIC_SWAP_I32 (when BinOpcode == 0), and
+// LoongArch::ATOMIC_LOAD_NAND_I32 (when Nand == true)
+MachineBasicBlock *
+LoongArchTargetLowering::emitAtomicBinary(MachineInstr &MI,
+                                     MachineBasicBlock *BB) const {
+
+  MachineFunction *MF = BB->getParent();
+  MachineRegisterInfo &RegInfo = MF->getRegInfo();
+  const TargetInstrInfo *TII = Subtarget.getInstrInfo();
+  DebugLoc DL = MI.getDebugLoc();
+
+  unsigned AtomicOp;
+  switch (MI.getOpcode()) {
+  case LoongArch::ATOMIC_LOAD_ADD_I32:
+    AtomicOp = LoongArch::ATOMIC_LOAD_ADD_I32_POSTRA;
+    break;
+  case LoongArch::ATOMIC_LOAD_SUB_I32:
+    AtomicOp = LoongArch::ATOMIC_LOAD_SUB_I32_POSTRA;
+    break;
+  case LoongArch::ATOMIC_LOAD_AND_I32:
+    AtomicOp = LoongArch::ATOMIC_LOAD_AND_I32_POSTRA;
+    break;
+  case LoongArch::ATOMIC_LOAD_OR_I32:
+    AtomicOp = LoongArch::ATOMIC_LOAD_OR_I32_POSTRA;
+    break;
+  case LoongArch::ATOMIC_LOAD_XOR_I32:
+    AtomicOp = LoongArch::ATOMIC_LOAD_XOR_I32_POSTRA;
+    break;
+  case LoongArch::ATOMIC_LOAD_NAND_I32:
+    AtomicOp = LoongArch::ATOMIC_LOAD_NAND_I32_POSTRA;
+    break;
+  case LoongArch::ATOMIC_SWAP_I32:
+    AtomicOp = LoongArch::ATOMIC_SWAP_I32_POSTRA;
+    break;
+  case LoongArch::ATOMIC_LOAD_MAX_I32:
+    AtomicOp = LoongArch::ATOMIC_LOAD_MAX_I32_POSTRA;
+    break;
+  case LoongArch::ATOMIC_LOAD_MIN_I32:
+    AtomicOp = LoongArch::ATOMIC_LOAD_MIN_I32_POSTRA;
+    break;
+  case LoongArch::ATOMIC_LOAD_UMAX_I32:
+    AtomicOp = LoongArch::ATOMIC_LOAD_UMAX_I32_POSTRA;
+    break;
+  case LoongArch::ATOMIC_LOAD_UMIN_I32:
+    AtomicOp = LoongArch::ATOMIC_LOAD_UMIN_I32_POSTRA;
+    break;
+  case LoongArch::ATOMIC_LOAD_ADD_I64:
+    AtomicOp = LoongArch::ATOMIC_LOAD_ADD_I64_POSTRA;
+    break;
+  case LoongArch::ATOMIC_LOAD_SUB_I64:
+    AtomicOp = LoongArch::ATOMIC_LOAD_SUB_I64_POSTRA;
+    break;
+  case LoongArch::ATOMIC_LOAD_AND_I64:
+    AtomicOp = LoongArch::ATOMIC_LOAD_AND_I64_POSTRA;
+    break;
+  case LoongArch::ATOMIC_LOAD_OR_I64:
+    AtomicOp = LoongArch::ATOMIC_LOAD_OR_I64_POSTRA;
+    break;
+  case LoongArch::ATOMIC_LOAD_XOR_I64:
+    AtomicOp = LoongArch::ATOMIC_LOAD_XOR_I64_POSTRA;
+    break;
+  case LoongArch::ATOMIC_LOAD_NAND_I64:
+    AtomicOp = LoongArch::ATOMIC_LOAD_NAND_I64_POSTRA;
+    break;
+  case LoongArch::ATOMIC_SWAP_I64:
+    AtomicOp = LoongArch::ATOMIC_SWAP_I64_POSTRA;
+    break;
+  case LoongArch::ATOMIC_LOAD_MAX_I64:
+    AtomicOp = LoongArch::ATOMIC_LOAD_MAX_I64_POSTRA;
+    break;
+  case LoongArch::ATOMIC_LOAD_MIN_I64:
+    AtomicOp = LoongArch::ATOMIC_LOAD_MIN_I64_POSTRA;
+    break;
+  case LoongArch::ATOMIC_LOAD_UMAX_I64:
+    AtomicOp = LoongArch::ATOMIC_LOAD_UMAX_I64_POSTRA;
+    break;
+  case LoongArch::ATOMIC_LOAD_UMIN_I64:
+    AtomicOp = LoongArch::ATOMIC_LOAD_UMIN_I64_POSTRA;
+    break;
+  default:
+    llvm_unreachable("Unknown pseudo atomic for replacement!");
+  }
+
+  unsigned OldVal = MI.getOperand(0).getReg();
+  unsigned Ptr = MI.getOperand(1).getReg();
+  unsigned Incr = MI.getOperand(2).getReg();
+  unsigned Scratch = RegInfo.createVirtualRegister(RegInfo.getRegClass(OldVal));
+
+  MachineBasicBlock::iterator II(MI);
+
+  // The scratch registers here with the EarlyClobber | Define | Implicit
+  // flags is used to persuade the register allocator and the machine
+  // verifier to accept the usage of this register. This has to be a real
+  // register which has an UNDEF value but is dead after the instruction which
+  // is unique among the registers chosen for the instruction.
+
+  // The EarlyClobber flag has the semantic properties that the operand it is
+  // attached to is clobbered before the rest of the inputs are read. Hence it
+  // must be unique among the operands to the instruction.
+  // The Define flag is needed to coerce the machine verifier that an Undef
+  // value isn't a problem.
+  // The Dead flag is needed as the value in scratch isn't used by any other
+  // instruction. Kill isn't used as Dead is more precise.
+  // The implicit flag is here due to the interaction between the other flags
+  // and the machine verifier.
+
+  // For correctness purpose, a new pseudo is introduced here. We need this
+  // new pseudo, so that FastRegisterAllocator does not see an ll/sc sequence
+  // that is spread over >1 basic blocks. A register allocator which
+  // introduces (or any codegen infact) a store, can violate the expectations
+  // of the hardware.
+  //
+  // An atomic read-modify-write sequence starts with a linked load
+  // instruction and ends with a store conditional instruction. The atomic
+  // read-modify-write sequence fails if any of the following conditions
+  // occur between the execution of ll and sc:
+  //   * A coherent store is completed by another process or coherent I/O
+  //     module into the block of synchronizable physical memory containing
+  //     the word. The size and alignment of the block is
+  //     implementation-dependent.
+  //   * A coherent store is executed between an LL and SC sequence on the
+  //     same processor to the block of synchornizable physical memory
+  //     containing the word.
+  //
+
+  unsigned PtrCopy = RegInfo.createVirtualRegister(RegInfo.getRegClass(Ptr));
+  unsigned IncrCopy = RegInfo.createVirtualRegister(RegInfo.getRegClass(Incr));
+
+  if(MI.getOpcode() == LoongArch::ATOMIC_LOAD_NAND_I32
+     || MI.getOpcode() == LoongArch::ATOMIC_LOAD_NAND_I64){
+    BuildMI(*BB, II, DL, TII->get(LoongArch::DBAR)).addImm(0);
+  }
+
+  BuildMI(*BB, II, DL, TII->get(LoongArch::COPY), IncrCopy).addReg(Incr);
+  BuildMI(*BB, II, DL, TII->get(LoongArch::COPY), PtrCopy).addReg(Ptr);
+
+  BuildMI(*BB, II, DL, TII->get(AtomicOp))
+      .addReg(OldVal, RegState::Define | RegState::EarlyClobber)
+      .addReg(PtrCopy)
+      .addReg(IncrCopy)
+      .addReg(Scratch, RegState::Define | RegState::EarlyClobber |
+                           RegState::Implicit | RegState::Dead);
+
+  if(MI.getOpcode() == LoongArch::ATOMIC_LOAD_NAND_I32
+     || MI.getOpcode() == LoongArch::ATOMIC_LOAD_NAND_I64){
+    BuildMI(*BB, II, DL, TII->get(LoongArch::DBAR)).addImm(0);
+  }
+
+  MI.eraseFromParent();
+
+  return BB;
+}
+
+MachineBasicBlock *LoongArchTargetLowering::emitSignExtendToI32InReg(
+    MachineInstr &MI, MachineBasicBlock *BB, unsigned Size, unsigned DstReg,
+    unsigned SrcReg) const {
+  const TargetInstrInfo *TII = Subtarget.getInstrInfo();
+  const DebugLoc &DL = MI.getDebugLoc();
+  if (Size == 1) {
+    BuildMI(BB, DL, TII->get(LoongArch::EXT_W_B32), DstReg).addReg(SrcReg);
+    return BB;
+  }
+
+  if (Size == 2) {
+    BuildMI(BB, DL, TII->get(LoongArch::EXT_W_H32), DstReg).addReg(SrcReg);
+    return BB;
+  }
+
+  MachineFunction *MF = BB->getParent();
+  MachineRegisterInfo &RegInfo = MF->getRegInfo();
+  const TargetRegisterClass *RC = getRegClassFor(MVT::i32);
+  unsigned ScrReg = RegInfo.createVirtualRegister(RC);
+
+  assert(Size < 32);
+  int64_t ShiftImm = 32 - (Size * 8);
+
+  BuildMI(BB, DL, TII->get(LoongArch::SLLI_W), ScrReg).addReg(SrcReg).addImm(ShiftImm);
+  BuildMI(BB, DL, TII->get(LoongArch::SRAI_W), DstReg).addReg(ScrReg).addImm(ShiftImm);
+
+  return BB;
+}
+
+MachineBasicBlock *LoongArchTargetLowering::emitAtomicBinaryPartword(
+    MachineInstr &MI, MachineBasicBlock *BB, unsigned Size) const {
+  assert((Size == 1 || Size == 2) &&
+         "Unsupported size for EmitAtomicBinaryPartial.");
+
+  MachineFunction *MF = BB->getParent();
+  MachineRegisterInfo &RegInfo = MF->getRegInfo();
+  const TargetRegisterClass *RC = getRegClassFor(MVT::i32);
+  const bool ArePtrs64bit = ABI.ArePtrs64bit();
+  const TargetRegisterClass *RCp =
+    getRegClassFor(ArePtrs64bit ? MVT::i64 : MVT::i32);
+  const TargetInstrInfo *TII = Subtarget.getInstrInfo();
+  DebugLoc DL = MI.getDebugLoc();
+
+  unsigned Dest = MI.getOperand(0).getReg();
+  unsigned Ptr = MI.getOperand(1).getReg();
+  unsigned Incr = MI.getOperand(2).getReg();
+
+  unsigned AlignedAddr = RegInfo.createVirtualRegister(RCp);
+  unsigned ShiftAmt = RegInfo.createVirtualRegister(RC);
+  unsigned Mask = RegInfo.createVirtualRegister(RC);
+  unsigned Mask2 = RegInfo.createVirtualRegister(RC);
+  unsigned Incr2 = RegInfo.createVirtualRegister(RC);
+  unsigned MaskLSB2 = RegInfo.createVirtualRegister(RCp);
+  unsigned PtrLSB2 = RegInfo.createVirtualRegister(RC);
+  unsigned MaskUpper = RegInfo.createVirtualRegister(RC);
+  unsigned MaskUppest = RegInfo.createVirtualRegister(RC);
+  unsigned Scratch = RegInfo.createVirtualRegister(RC);
+  unsigned Scratch2 = RegInfo.createVirtualRegister(RC);
+  unsigned Scratch3 = RegInfo.createVirtualRegister(RC);
+
+  unsigned AtomicOp = 0;
+  switch (MI.getOpcode()) {
+  case LoongArch::ATOMIC_LOAD_NAND_I8:
+    AtomicOp = LoongArch::ATOMIC_LOAD_NAND_I8_POSTRA;
+    break;
+  case LoongArch::ATOMIC_LOAD_NAND_I16:
+    AtomicOp = LoongArch::ATOMIC_LOAD_NAND_I16_POSTRA;
+    break;
+  case LoongArch::ATOMIC_SWAP_I8:
+    AtomicOp = LoongArch::ATOMIC_SWAP_I8_POSTRA;
+    break;
+  case LoongArch::ATOMIC_SWAP_I16:
+    AtomicOp = LoongArch::ATOMIC_SWAP_I16_POSTRA;
+    break;
+  case LoongArch::ATOMIC_LOAD_MAX_I8:
+    AtomicOp = LoongArch::ATOMIC_LOAD_MAX_I8_POSTRA;
+    break;
+  case LoongArch::ATOMIC_LOAD_MAX_I16:
+    AtomicOp = LoongArch::ATOMIC_LOAD_MAX_I16_POSTRA;
+    break;
+  case LoongArch::ATOMIC_LOAD_MIN_I8:
+    AtomicOp = LoongArch::ATOMIC_LOAD_MIN_I8_POSTRA;
+    break;
+  case LoongArch::ATOMIC_LOAD_MIN_I16:
+    AtomicOp = LoongArch::ATOMIC_LOAD_MIN_I16_POSTRA;
+    break;
+  case LoongArch::ATOMIC_LOAD_UMAX_I8:
+    AtomicOp = LoongArch::ATOMIC_LOAD_UMAX_I8_POSTRA;
+    break;
+  case LoongArch::ATOMIC_LOAD_UMAX_I16:
+    AtomicOp = LoongArch::ATOMIC_LOAD_UMAX_I16_POSTRA;
+    break;
+  case LoongArch::ATOMIC_LOAD_UMIN_I8:
+    AtomicOp = LoongArch::ATOMIC_LOAD_UMIN_I8_POSTRA;
+    break;
+  case LoongArch::ATOMIC_LOAD_UMIN_I16:
+    AtomicOp = LoongArch::ATOMIC_LOAD_UMIN_I16_POSTRA;
+    break;
+  case LoongArch::ATOMIC_LOAD_ADD_I8:
+    AtomicOp = LoongArch::ATOMIC_LOAD_ADD_I8_POSTRA;
+    break;
+  case LoongArch::ATOMIC_LOAD_ADD_I16:
+    AtomicOp = LoongArch::ATOMIC_LOAD_ADD_I16_POSTRA;
+    break;
+  case LoongArch::ATOMIC_LOAD_SUB_I8:
+    AtomicOp = LoongArch::ATOMIC_LOAD_SUB_I8_POSTRA;
+    break;
+  case LoongArch::ATOMIC_LOAD_SUB_I16:
+    AtomicOp = LoongArch::ATOMIC_LOAD_SUB_I16_POSTRA;
+    break;
+  case LoongArch::ATOMIC_LOAD_AND_I8:
+    AtomicOp = LoongArch::ATOMIC_LOAD_AND_I8_POSTRA;
+    break;
+  case LoongArch::ATOMIC_LOAD_AND_I16:
+    AtomicOp = LoongArch::ATOMIC_LOAD_AND_I16_POSTRA;
+    break;
+  case LoongArch::ATOMIC_LOAD_OR_I8:
+    AtomicOp = LoongArch::ATOMIC_LOAD_OR_I8_POSTRA;
+    break;
+  case LoongArch::ATOMIC_LOAD_OR_I16:
+    AtomicOp = LoongArch::ATOMIC_LOAD_OR_I16_POSTRA;
+    break;
+  case LoongArch::ATOMIC_LOAD_XOR_I8:
+    AtomicOp = LoongArch::ATOMIC_LOAD_XOR_I8_POSTRA;
+    break;
+  case LoongArch::ATOMIC_LOAD_XOR_I16:
+    AtomicOp = LoongArch::ATOMIC_LOAD_XOR_I16_POSTRA;
+    break;
+  default:
+    llvm_unreachable("Unknown subword atomic pseudo for expansion!");
+  }
+
+  // insert new blocks after the current block
+  const BasicBlock *LLVM_BB = BB->getBasicBlock();
+  MachineBasicBlock *exitMBB = MF->CreateMachineBasicBlock(LLVM_BB);
+  MachineFunction::iterator It = ++BB->getIterator();
+  MF->insert(It, exitMBB);
+
+  // Transfer the remainder of BB and its successor edges to exitMBB.
+  exitMBB->splice(exitMBB->begin(), BB,
+                  std::next(MachineBasicBlock::iterator(MI)), BB->end());
+  exitMBB->transferSuccessorsAndUpdatePHIs(BB);
+
+  BB->addSuccessor(exitMBB, BranchProbability::getOne());
+
+  //  thisMBB:
+  //    addiu   masklsb2,$0,-4                # 0xfffffffc
+  //    and     alignedaddr,ptr,masklsb2
+  //    andi    ptrlsb2,ptr,3
+  //    sll     shiftamt,ptrlsb2,3
+  //    ori     maskupper,$0,255               # 0xff
+  //    sll     mask,maskupper,shiftamt
+  //    nor     mask2,$0,mask
+  //    sll     incr2,incr,shiftamt
+
+  BuildMI(BB, DL, TII->get(LoongArch::DBAR)).addImm(0);
+  int64_t MaskImm = (Size == 1) ? 255 : 4095;
+  BuildMI(BB, DL, TII->get(ABI.GetPtrAddiOp()), MaskLSB2)
+    .addReg(ABI.GetNullPtr()).addImm(-4);
+  BuildMI(BB, DL, TII->get(ABI.GetPtrAndOp()), AlignedAddr)
+    .addReg(Ptr).addReg(MaskLSB2);
+  BuildMI(BB, DL, TII->get(LoongArch::ANDI32), PtrLSB2)
+      .addReg(Ptr, 0, ArePtrs64bit ? LoongArch::sub_32 : 0).addImm(3);
+  BuildMI(BB, DL, TII->get(LoongArch::SLLI_W), ShiftAmt).addReg(PtrLSB2).addImm(3);
+
+  if(MaskImm==4095){
+  BuildMI(BB, DL, TII->get(LoongArch::LU12I_W32), MaskUppest).addImm(0xf);
+  BuildMI(BB, DL, TII->get(LoongArch::ORI32), MaskUpper)
+    .addReg(MaskUppest).addImm(MaskImm);
+  }
+  else{
+  BuildMI(BB, DL, TII->get(LoongArch::ORI32), MaskUpper)
+    .addReg(LoongArch::ZERO).addImm(MaskImm);
+  }
+
+  BuildMI(BB, DL, TII->get(LoongArch::SLL_W), Mask)
+    .addReg(MaskUpper).addReg(ShiftAmt);
+  BuildMI(BB, DL, TII->get(LoongArch::NOR32), Mask2).addReg(LoongArch::ZERO).addReg(Mask);
+  BuildMI(BB, DL, TII->get(LoongArch::SLL_W), Incr2).addReg(Incr).addReg(ShiftAmt);
+
+
+  // The purposes of the flags on the scratch registers is explained in
+  // emitAtomicBinary. In summary, we need a scratch register which is going to
+  // be undef, that is unique among registers chosen for the instruction.
+
+  BuildMI(BB, DL, TII->get(AtomicOp))
+      .addReg(Dest, RegState::Define | RegState::EarlyClobber)
+      .addReg(AlignedAddr)
+      .addReg(Incr2)
+      .addReg(Mask)
+      .addReg(Mask2)
+      .addReg(ShiftAmt)
+      .addReg(Scratch, RegState::EarlyClobber | RegState::Define |
+                           RegState::Dead | RegState::Implicit)
+      .addReg(Scratch2, RegState::EarlyClobber | RegState::Define |
+                            RegState::Dead | RegState::Implicit)
+      .addReg(Scratch3, RegState::EarlyClobber | RegState::Define |
+                            RegState::Dead | RegState::Implicit);
+
+  BuildMI(BB, DL, TII->get(LoongArch::DBAR)).addImm(0);
+
+  MI.eraseFromParent(); // The instruction is gone now.
+
+  return exitMBB;
+}
+
+// Lower atomic compare and swap to a pseudo instruction, taking care to
+// define a scratch register for the pseudo instruction's expansion. The
+// instruction is expanded after the register allocator as to prevent
+// the insertion of stores between the linked load and the store conditional.
+
+MachineBasicBlock *
+LoongArchTargetLowering::emitAtomicCmpSwap(MachineInstr &MI,
+                                      MachineBasicBlock *BB) const {
+  assert((MI.getOpcode() == LoongArch::ATOMIC_CMP_SWAP_I32 ||
+          MI.getOpcode() == LoongArch::ATOMIC_CMP_SWAP_I64) &&
+         "Unsupported atomic psseudo for EmitAtomicCmpSwap.");
+
+  const unsigned Size = MI.getOpcode() == LoongArch::ATOMIC_CMP_SWAP_I32 ? 4 : 8;
+
+  MachineFunction *MF = BB->getParent();
+  MachineRegisterInfo &MRI = MF->getRegInfo();
+  const TargetRegisterClass *RC = getRegClassFor(MVT::getIntegerVT(Size * 8));
+  const TargetInstrInfo *TII = Subtarget.getInstrInfo();
+  DebugLoc DL = MI.getDebugLoc();
+
+  unsigned AtomicOp = MI.getOpcode() == LoongArch::ATOMIC_CMP_SWAP_I32
+                          ? LoongArch::ATOMIC_CMP_SWAP_I32_POSTRA
+                          : LoongArch::ATOMIC_CMP_SWAP_I64_POSTRA;
+  unsigned Dest = MI.getOperand(0).getReg();
+  unsigned Ptr = MI.getOperand(1).getReg();
+  unsigned OldVal = MI.getOperand(2).getReg();
+  unsigned NewVal = MI.getOperand(3).getReg();
+
+  unsigned Scratch = MRI.createVirtualRegister(RC);
+  MachineBasicBlock::iterator II(MI);
+
+  // We need to create copies of the various registers and kill them at the
+  // atomic pseudo. If the copies are not made, when the atomic is expanded
+  // after fast register allocation, the spills will end up outside of the
+  // blocks that their values are defined in, causing livein errors.
+
+  unsigned DestCopy = MRI.createVirtualRegister(MRI.getRegClass(Dest));
+  unsigned PtrCopy = MRI.createVirtualRegister(MRI.getRegClass(Ptr));
+  unsigned OldValCopy = MRI.createVirtualRegister(MRI.getRegClass(OldVal));
+  unsigned NewValCopy = MRI.createVirtualRegister(MRI.getRegClass(NewVal));
+
+  BuildMI(*BB, II, DL, TII->get(LoongArch::DBAR)).addImm(0);
+  BuildMI(*BB, II, DL, TII->get(LoongArch::COPY), DestCopy).addReg(Dest);
+  BuildMI(*BB, II, DL, TII->get(LoongArch::COPY), PtrCopy).addReg(Ptr);
+  BuildMI(*BB, II, DL, TII->get(LoongArch::COPY), OldValCopy).addReg(OldVal);
+  BuildMI(*BB, II, DL, TII->get(LoongArch::COPY), NewValCopy).addReg(NewVal);
+
+  // The purposes of the flags on the scratch registers is explained in
+  // emitAtomicBinary. In summary, we need a scratch register which is going to
+  // be undef, that is unique among registers chosen for the instruction.
+
+  BuildMI(*BB, II, DL, TII->get(AtomicOp))
+      .addReg(Dest, RegState::Define | RegState::EarlyClobber)
+      .addReg(PtrCopy, RegState::Kill)
+      .addReg(OldValCopy, RegState::Kill)
+      .addReg(NewValCopy, RegState::Kill)
+      .addReg(Scratch, RegState::EarlyClobber | RegState::Define |
+                           RegState::Dead | RegState::Implicit);
+
+  BuildMI(*BB, II, DL, TII->get(LoongArch::DBAR)).addImm(0);
+
+  MI.eraseFromParent(); // The instruction is gone now.
+
+  return BB;
+}
+
+MachineBasicBlock *LoongArchTargetLowering::emitAtomicCmpSwapPartword(
+    MachineInstr &MI, MachineBasicBlock *BB, unsigned Size) const {
+  assert((Size == 1 || Size == 2) &&
+      "Unsupported size for EmitAtomicCmpSwapPartial.");
+
+  MachineFunction *MF = BB->getParent();
+  MachineRegisterInfo &RegInfo = MF->getRegInfo();
+  const TargetRegisterClass *RC = getRegClassFor(MVT::i32);
+  const bool ArePtrs64bit = ABI.ArePtrs64bit();
+  const TargetRegisterClass *RCp =
+    getRegClassFor(ArePtrs64bit ? MVT::i64 : MVT::i32);
+  const TargetInstrInfo *TII = Subtarget.getInstrInfo();
+  DebugLoc DL = MI.getDebugLoc();
+
+  unsigned Dest = MI.getOperand(0).getReg();
+  unsigned Ptr = MI.getOperand(1).getReg();
+  unsigned CmpVal = MI.getOperand(2).getReg();
+  unsigned NewVal = MI.getOperand(3).getReg();
+
+  unsigned AlignedAddr = RegInfo.createVirtualRegister(RCp);
+  unsigned ShiftAmt = RegInfo.createVirtualRegister(RC);
+  unsigned Mask = RegInfo.createVirtualRegister(RC);
+  unsigned Mask2 = RegInfo.createVirtualRegister(RC);
+  unsigned ShiftedCmpVal = RegInfo.createVirtualRegister(RC);
+  unsigned ShiftedNewVal = RegInfo.createVirtualRegister(RC);
+  unsigned MaskLSB2 = RegInfo.createVirtualRegister(RCp);
+  unsigned PtrLSB2 = RegInfo.createVirtualRegister(RC);
+  unsigned MaskUpper = RegInfo.createVirtualRegister(RC);
+  unsigned MaskUppest = RegInfo.createVirtualRegister(RC);
+  unsigned Mask3 = RegInfo.createVirtualRegister(RC);
+  unsigned MaskedCmpVal = RegInfo.createVirtualRegister(RC);
+  unsigned MaskedNewVal = RegInfo.createVirtualRegister(RC);
+  unsigned AtomicOp = MI.getOpcode() == LoongArch::ATOMIC_CMP_SWAP_I8
+                          ? LoongArch::ATOMIC_CMP_SWAP_I8_POSTRA
+                          : LoongArch::ATOMIC_CMP_SWAP_I16_POSTRA;
+
+  // The scratch registers here with the EarlyClobber | Define | Dead | Implicit
+  // flags are used to coerce the register allocator and the machine verifier to
+  // accept the usage of these registers.
+  // The EarlyClobber flag has the semantic properties that the operand it is
+  // attached to is clobbered before the rest of the inputs are read. Hence it
+  // must be unique among the operands to the instruction.
+  // The Define flag is needed to coerce the machine verifier that an Undef
+  // value isn't a problem.
+  // The Dead flag is needed as the value in scratch isn't used by any other
+  // instruction. Kill isn't used as Dead is more precise.
+  unsigned Scratch = RegInfo.createVirtualRegister(RC);
+  unsigned Scratch2 = RegInfo.createVirtualRegister(RC);
+
+  // insert new blocks after the current block
+  const BasicBlock *LLVM_BB = BB->getBasicBlock();
+  MachineBasicBlock *exitMBB = MF->CreateMachineBasicBlock(LLVM_BB);
+  MachineFunction::iterator It = ++BB->getIterator();
+  MF->insert(It, exitMBB);
+
+  // Transfer the remainder of BB and its successor edges to exitMBB.
+  exitMBB->splice(exitMBB->begin(), BB,
+                  std::next(MachineBasicBlock::iterator(MI)), BB->end());
+  exitMBB->transferSuccessorsAndUpdatePHIs(BB);
+
+  BB->addSuccessor(exitMBB, BranchProbability::getOne());
+
+  //  thisMBB:
+  //    addiu   masklsb2,$0,-4                # 0xfffffffc
+  //    and     alignedaddr,ptr,masklsb2
+  //    andi    ptrlsb2,ptr,3
+  //    xori    ptrlsb2,ptrlsb2,3              # Only for BE
+  //    sll     shiftamt,ptrlsb2,3
+  //    ori     maskupper,$0,255               # 0xff
+  //    sll     mask,maskupper,shiftamt
+  //    nor     mask2,$0,mask
+  //    andi    maskedcmpval,cmpval,255
+  //    sll     shiftedcmpval,maskedcmpval,shiftamt
+  //    andi    maskednewval,newval,255
+  //    sll     shiftednewval,maskednewval,shiftamt
+
+  BuildMI(BB, DL, TII->get(LoongArch::DBAR)).addImm(0);
+
+  int64_t MaskImm = (Size == 1) ? 255 : 4095;
+  BuildMI(BB, DL, TII->get(ArePtrs64bit ? LoongArch::ADDI_D : LoongArch::ADDI_W), MaskLSB2)
+    .addReg(ABI.GetNullPtr()).addImm(-4);
+  BuildMI(BB, DL, TII->get(ArePtrs64bit ? LoongArch::AND : LoongArch::AND32), AlignedAddr)
+    .addReg(Ptr).addReg(MaskLSB2);
+  BuildMI(BB, DL, TII->get(LoongArch::ANDI32), PtrLSB2)
+      .addReg(Ptr, 0, ArePtrs64bit ? LoongArch::sub_32 : 0).addImm(3);
+  BuildMI(BB, DL, TII->get(LoongArch::SLLI_W), ShiftAmt).addReg(PtrLSB2).addImm(3);
+
+  if(MaskImm==4095){
+  BuildMI(BB, DL, TII->get(LoongArch::LU12I_W32), MaskUppest).addImm(0xf);
+  BuildMI(BB, DL, TII->get(LoongArch::ORI32), MaskUpper)
+    .addReg(MaskUppest).addImm(MaskImm);
+  }
+  else{
+  BuildMI(BB, DL, TII->get(LoongArch::ORI32), MaskUpper)
+    .addReg(LoongArch::ZERO).addImm(MaskImm);
+  }
+
+  BuildMI(BB, DL, TII->get(LoongArch::SLL_W), Mask)
+    .addReg(MaskUpper).addReg(ShiftAmt);
+  BuildMI(BB, DL, TII->get(LoongArch::NOR32), Mask2).addReg(LoongArch::ZERO).addReg(Mask);
+  if(MaskImm==4095){
+    BuildMI(BB, DL, TII->get(LoongArch::ORI32), Mask3)
+    .addReg(MaskUppest).addImm(MaskImm);
+    BuildMI(BB, DL, TII->get(LoongArch::AND32), MaskedCmpVal)
+      .addReg(CmpVal).addReg(Mask3);
+    BuildMI(BB, DL, TII->get(LoongArch::SLL_W), ShiftedCmpVal)
+      .addReg(MaskedCmpVal).addReg(ShiftAmt);
+    BuildMI(BB, DL, TII->get(LoongArch::AND32), MaskedNewVal)
+      .addReg(NewVal).addReg(Mask3);
+  }
+  else{
+    BuildMI(BB, DL, TII->get(LoongArch::ANDI32), MaskedCmpVal)
+      .addReg(CmpVal).addImm(MaskImm);
+    BuildMI(BB, DL, TII->get(LoongArch::SLL_W), ShiftedCmpVal)
+      .addReg(MaskedCmpVal).addReg(ShiftAmt);
+    BuildMI(BB, DL, TII->get(LoongArch::ANDI32), MaskedNewVal)
+      .addReg(NewVal).addImm(MaskImm);
+  }
+  BuildMI(BB, DL, TII->get(LoongArch::SLL_W), ShiftedNewVal)
+    .addReg(MaskedNewVal).addReg(ShiftAmt);
+
+  // The purposes of the flags on the scratch registers are explained in
+  // emitAtomicBinary. In summary, we need a scratch register which is going to
+  // be undef, that is unique among the register chosen for the instruction.
+
+  BuildMI(BB, DL, TII->get(AtomicOp))
+      .addReg(Dest, RegState::Define | RegState::EarlyClobber)
+      .addReg(AlignedAddr)
+      .addReg(Mask)
+      .addReg(ShiftedCmpVal)
+      .addReg(Mask2)
+      .addReg(ShiftedNewVal)
+      .addReg(ShiftAmt)
+      .addReg(Scratch, RegState::EarlyClobber | RegState::Define |
+                           RegState::Dead | RegState::Implicit)
+      .addReg(Scratch2, RegState::EarlyClobber | RegState::Define |
+                            RegState::Dead | RegState::Implicit);
+
+  BuildMI(BB, DL, TII->get(LoongArch::DBAR)).addImm(0);
+
+  MI.eraseFromParent(); // The instruction is gone now.
+
+  return exitMBB;
+}
+
+SDValue LoongArchTargetLowering::lowerBRCOND(SDValue Op, SelectionDAG &DAG) const {
+  // The first operand is the chain, the second is the condition, the third is
+  // the block to branch to if the condition is true.
+  SDValue Chain = Op.getOperand(0);
+  SDValue Dest = Op.getOperand(2);
+  SDLoc DL(Op);
+
+  SDValue CondRes = createFPCmp(DAG, Op.getOperand(1));
+
+  // Return if flag is not set by a floating point comparison.
+  if (CondRes.getOpcode() != LoongArchISD::FPCmp)
+    return Op;
+
+  SDValue CCNode  = CondRes.getOperand(2);
+  LoongArch::CondCode CC =
+    (LoongArch::CondCode)cast<ConstantSDNode>(CCNode)->getZExtValue();
+  unsigned Opc = invertFPCondCodeUser(CC) ? LoongArch::BRANCH_F : LoongArch::BRANCH_T;
+  SDValue BrCode = DAG.getConstant(Opc, DL, MVT::i32);
+  SDValue FCC0 = DAG.getRegister(LoongArch::FCC0, MVT::i32);
+  return DAG.getNode(LoongArchISD::FPBrcond, DL, Op.getValueType(), Chain, BrCode,
+                     FCC0, Dest, CondRes);
+}
+
+SDValue LoongArchTargetLowering::
+lowerSELECT(SDValue Op, SelectionDAG &DAG) const
+{
+  SDValue Cond = createFPCmp(DAG, Op.getOperand(0));
+
+  // Return if flag is not set by a floating point comparison.
+  if (Cond.getOpcode() != LoongArchISD::FPCmp)
+    return Op;
+
+  return createCMovFP(DAG, Cond, Op.getOperand(1), Op.getOperand(2),
+                      SDLoc(Op));
+}
+
+SDValue LoongArchTargetLowering::lowerSETCC(SDValue Op, SelectionDAG &DAG) const {
+  SDValue Cond = createFPCmp(DAG, Op);
+
+  assert(Cond.getOpcode() == LoongArchISD::FPCmp &&
+         "Floating point operand expected.");
+
+  SDLoc DL(Op);
+  SDValue True  = DAG.getConstant(1, DL, MVT::i32);
+  SDValue False = DAG.getConstant(0, DL, MVT::i32);
+
+  return createCMovFP(DAG, Cond, True, False, DL);
+}
+
+SDValue LoongArchTargetLowering::lowerGlobalAddress(SDValue Op,
+                                               SelectionDAG &DAG) const {
+  GlobalAddressSDNode *N = cast<GlobalAddressSDNode>(Op);
+
+  const GlobalValue *GV = N->getGlobal();
+  bool IsLocal = getTargetMachine().shouldAssumeDSOLocal(*GV->getParent(), GV);
+  SDValue Addr = getAddr(N, DAG, IsLocal);
+
+  return Addr;
+}
+
+SDValue LoongArchTargetLowering::lowerBlockAddress(SDValue Op,
+                                              SelectionDAG &DAG) const {
+  BlockAddressSDNode *N = cast<BlockAddressSDNode>(Op);
+
+  return getAddr(N, DAG);
+}
+
+SDValue LoongArchTargetLowering::
+lowerGlobalTLSAddress(SDValue Op, SelectionDAG &DAG) const
+{
+  GlobalAddressSDNode *GA = cast<GlobalAddressSDNode>(Op);
+  if (DAG.getTarget().useEmulatedTLS())
+    return LowerToTLSEmulatedModel(GA, DAG);
+
+  SDLoc DL(GA);
+  const GlobalValue *GV = GA->getGlobal();
+  EVT PtrVT = getPointerTy(DAG.getDataLayout());
+
+  TLSModel::Model model = getTargetMachine().getTLSModel(GV);
+
+  if (model == TLSModel::GeneralDynamic || model == TLSModel::LocalDynamic) {
+    // General Dynamic TLS Model && Local Dynamic TLS Model
+    unsigned PtrSize = PtrVT.getSizeInBits();
+    IntegerType *PtrTy = Type::getIntNTy(*DAG.getContext(), PtrSize);
+    //  SDValue Addr = DAG.getTargetGlobalAddress(GV, DL, PtrTy, 0, 0);
+    SDValue Addr = DAG.getTargetGlobalAddress(GV, DL, PtrVT, 0, 0U);
+    SDValue Load = SDValue(DAG.getMachineNode(LoongArch::LoadAddrTLS_GD ,
+                           DL, PtrVT, Addr), 0);
+    SDValue TlsGetAddr = DAG.getExternalSymbol("__tls_get_addr", PtrVT);
+
+    ArgListTy Args;
+    ArgListEntry Entry;
+    Entry.Node = Load;
+    Entry.Ty = PtrTy;
+    Args.push_back(Entry);
+
+    TargetLowering::CallLoweringInfo CLI(DAG);
+    CLI.setDebugLoc(DL)
+       .setChain(DAG.getEntryNode())
+       .setLibCallee(CallingConv::C, PtrTy, TlsGetAddr, std::move(Args));
+    std::pair<SDValue, SDValue> CallResult = LowerCallTo(CLI);
+
+    SDValue Ret = CallResult.first;
+
+    return Ret;
+  }
+
+  SDValue Addr = DAG.getTargetGlobalAddress(GV, DL, PtrVT, 0, 0U);
+  SDValue Offset;
+  if (model == TLSModel::InitialExec) {
+    // Initial Exec TLS Model
+    Offset = SDValue(DAG.getMachineNode(LoongArch::LoadAddrTLS_IE, DL,
+                     PtrVT, Addr), 0);
+  } else {
+    // Local Exec TLS Model
+    assert(model == TLSModel::LocalExec);
+    Offset = SDValue(DAG.getMachineNode(LoongArch::LoadAddrTLS_LE, DL,
+                     PtrVT, Addr), 0);
+  }
+
+  SDValue ThreadPointer = DAG.getRegister((PtrVT == MVT::i32)
+                                          ? LoongArch::TP
+                                          : LoongArch::TP_64, PtrVT);
+  return DAG.getNode(ISD::ADD, DL, PtrVT, ThreadPointer, Offset);
+}
+
+SDValue LoongArchTargetLowering::
+lowerJumpTable(SDValue Op, SelectionDAG &DAG) const
+{
+  JumpTableSDNode *N = cast<JumpTableSDNode>(Op);
+
+  return getAddr(N, DAG);
+}
+
+SDValue LoongArchTargetLowering::
+lowerConstantPool(SDValue Op, SelectionDAG &DAG) const
+{
+  ConstantPoolSDNode *N = cast<ConstantPoolSDNode>(Op);
+
+  return getAddr(N, DAG);
+}
+
+SDValue LoongArchTargetLowering::lowerVASTART(SDValue Op, SelectionDAG &DAG) const {
+  MachineFunction &MF = DAG.getMachineFunction();
+  LoongArchFunctionInfo *FuncInfo = MF.getInfo<LoongArchFunctionInfo>();
+
+  SDLoc DL(Op);
+  SDValue FI = DAG.getFrameIndex(FuncInfo->getVarArgsFrameIndex(),
+                                 getPointerTy(MF.getDataLayout()));
+
+  // vastart just stores the address of the VarArgsFrameIndex slot into the
+  // memory location argument.
+  const Value *SV = cast<SrcValueSDNode>(Op.getOperand(2))->getValue();
+  return DAG.getStore(Op.getOperand(0), DL, FI, Op.getOperand(1),
+                      MachinePointerInfo(SV));
+}
+
+SDValue LoongArchTargetLowering::lowerVAARG(SDValue Op, SelectionDAG &DAG) const {
+  SDNode *Node = Op.getNode();
+  EVT VT = Node->getValueType(0);
+  SDValue Chain = Node->getOperand(0);
+  SDValue VAListPtr = Node->getOperand(1);
+  const Align Align =
+      llvm::MaybeAlign(Node->getConstantOperandVal(3)).valueOrOne();
+  const Value *SV = cast<SrcValueSDNode>(Node->getOperand(2))->getValue();
+  SDLoc DL(Node);
+  unsigned ArgSlotSizeInBytes = (ABI.IsLPX32() || ABI.IsLP64D()) ? 8 : 4;
+
+  SDValue VAListLoad = DAG.getLoad(getPointerTy(DAG.getDataLayout()), DL, Chain,
+                                   VAListPtr, MachinePointerInfo(SV));
+  SDValue VAList = VAListLoad;
+
+  // Re-align the pointer if necessary.
+  // It should only ever be necessary for 64-bit types on LP32 since the minimum
+  // argument alignment is the same as the maximum type alignment for LPX32/LP64D.
+  //
+  // FIXME: We currently align too often. The code generator doesn't notice
+  //        when the pointer is still aligned from the last va_arg (or pair of
+  //        va_args for the i64 on LP32 case).
+  if (Align > getMinStackArgumentAlignment()) {
+    VAList = DAG.getNode(
+        ISD::ADD, DL, VAList.getValueType(), VAList,
+        DAG.getConstant(Align.value() - 1, DL, VAList.getValueType()));
+
+    VAList = DAG.getNode(
+        ISD::AND, DL, VAList.getValueType(), VAList,
+        DAG.getConstant(-(int64_t)Align.value(), DL, VAList.getValueType()));
+  }
+
+  // Increment the pointer, VAList, to the next vaarg.
+  auto &TD = DAG.getDataLayout();
+  unsigned ArgSizeInBytes =
+      TD.getTypeAllocSize(VT.getTypeForEVT(*DAG.getContext()));
+  SDValue Tmp3 =
+      DAG.getNode(ISD::ADD, DL, VAList.getValueType(), VAList,
+                  DAG.getConstant(alignTo(ArgSizeInBytes, ArgSlotSizeInBytes),
+                                  DL, VAList.getValueType()));
+  // Store the incremented VAList to the legalized pointer
+  Chain = DAG.getStore(VAListLoad.getValue(1), DL, Tmp3, VAListPtr,
+                       MachinePointerInfo(SV));
+
+  // Load the actual argument out of the pointer VAList
+  return DAG.getLoad(VT, DL, Chain, VAList, MachinePointerInfo());
+}
+
+static SDValue lowerFCOPYSIGLPX32(SDValue Op, SelectionDAG &DAG) {
+  // TODO:
+  return SDValue();
+}
+
+static SDValue lowerFCOPYSIGLP64(SDValue Op, SelectionDAG &DAG) {
+  unsigned WidthX = Op.getOperand(0).getValueSizeInBits();
+  unsigned WidthY = Op.getOperand(1).getValueSizeInBits();
+  EVT TyX = MVT::getIntegerVT(WidthX), TyY = MVT::getIntegerVT(WidthY);
+  SDLoc DL(Op);
+
+  // Bitcast to integer nodes.
+  SDValue X = DAG.getNode(ISD::BITCAST, DL, TyX, Op.getOperand(0));
+  SDValue Y = DAG.getNode(ISD::BITCAST, DL, TyY, Op.getOperand(1));
+
+  // bstrpick  E, Y, width(Y) - 1, width(Y) - 1  ; extract bit width(Y)-1 of Y
+  // bstrins  X, E, width(X) - 1, width(X) - 1  ; insert extracted bit at bit width(X)-1 of X
+  SDValue E = DAG.getNode(LoongArchISD::BSTRPICK, DL, TyY, Y,
+                          DAG.getConstant(WidthY - 1, DL, MVT::i32), DAG.getConstant(WidthY - 1, DL, MVT::i32));
+
+  if (WidthX > WidthY)
+    E = DAG.getNode(ISD::ZERO_EXTEND, DL, TyX, E);
+  else if (WidthY > WidthX)
+    E = DAG.getNode(ISD::TRUNCATE, DL, TyX, E);
+
+  SDValue I = DAG.getNode(LoongArchISD::BSTRINS, DL, TyX, E,
+                          DAG.getConstant(WidthX - 1, DL, MVT::i32), DAG.getConstant(WidthX - 1, DL, MVT::i32),
+                          X);
+  return DAG.getNode(ISD::BITCAST, DL, Op.getOperand(0).getValueType(), I);
+}
+
+SDValue
+LoongArchTargetLowering::lowerFCOPYSIGN(SDValue Op, SelectionDAG &DAG) const {
+  if (Subtarget.is64Bit())
+    return lowerFCOPYSIGLP64(Op, DAG);
+
+  return lowerFCOPYSIGLPX32(Op, DAG);
+}
+
+SDValue LoongArchTargetLowering::
+lowerFRAMEADDR(SDValue Op, SelectionDAG &DAG) const {
+  // check the depth
+  assert((cast<ConstantSDNode>(Op.getOperand(0))->getZExtValue() == 0) &&
+         "Frame address can only be determined for current frame.");
+
+  MachineFrameInfo &MFI = DAG.getMachineFunction().getFrameInfo();
+  MFI.setFrameAddressIsTaken(true);
+  EVT VT = Op.getValueType();
+  SDLoc DL(Op);
+  SDValue FrameAddr = DAG.getCopyFromReg(
+      DAG.getEntryNode(), DL, ABI.IsLP64D() ? LoongArch::FP_64 : LoongArch::FP, VT);
+  return FrameAddr;
+}
+
+SDValue LoongArchTargetLowering::lowerRETURNADDR(SDValue Op,
+                                            SelectionDAG &DAG) const {
+  if (verifyReturnAddressArgumentIsConstant(Op, DAG))
+    return SDValue();
+
+  // check the depth
+  assert((cast<ConstantSDNode>(Op.getOperand(0))->getZExtValue() == 0) &&
+         "Return address can be determined only for current frame.");
+
+  MachineFunction &MF = DAG.getMachineFunction();
+  MachineFrameInfo &MFI = MF.getFrameInfo();
+  MVT VT = Op.getSimpleValueType();
+  unsigned RA = ABI.IsLP64D() ? LoongArch::RA_64 : LoongArch::RA;
+  MFI.setReturnAddressIsTaken(true);
+
+  // Return RA, which contains the return address. Mark it an implicit live-in.
+  unsigned Reg = MF.addLiveIn(RA, getRegClassFor(VT));
+  return DAG.getCopyFromReg(DAG.getEntryNode(), SDLoc(Op), Reg, VT);
+}
+
+// An EH_RETURN is the result of lowering llvm.eh.return which in turn is
+// generated from __builtin_eh_return (offset, handler)
+// The effect of this is to adjust the stack pointer by "offset"
+// and then branch to "handler".
+SDValue LoongArchTargetLowering::lowerEH_RETURN(SDValue Op, SelectionDAG &DAG)
+                                                                     const {
+  MachineFunction &MF = DAG.getMachineFunction();
+  LoongArchFunctionInfo *LoongArchFI = MF.getInfo<LoongArchFunctionInfo>();
+
+  LoongArchFI->setCallsEhReturn();
+  SDValue Chain     = Op.getOperand(0);
+  SDValue Offset    = Op.getOperand(1);
+  SDValue Handler   = Op.getOperand(2);
+  SDLoc DL(Op);
+  EVT Ty = ABI.IsLP64D() ? MVT::i64 : MVT::i32;
+
+  // Store stack offset in A1, store jump target in A0. Glue CopyToReg and
+  // EH_RETURN nodes, so that instructions are emitted back-to-back.
+  unsigned OffsetReg = ABI.IsLP64D() ? LoongArch::A1_64 : LoongArch::A1;
+  unsigned AddrReg = ABI.IsLP64D() ? LoongArch::A0_64 : LoongArch::A0;
+  Chain = DAG.getCopyToReg(Chain, DL, OffsetReg, Offset, SDValue());
+  Chain = DAG.getCopyToReg(Chain, DL, AddrReg, Handler, Chain.getValue(1));
+  return DAG.getNode(LoongArchISD::EH_RETURN, DL, MVT::Other, Chain,
+                     DAG.getRegister(OffsetReg, Ty),
+                     DAG.getRegister(AddrReg, getPointerTy(MF.getDataLayout())),
+                     Chain.getValue(1));
+}
+
+SDValue LoongArchTargetLowering::lowerATOMIC_FENCE(SDValue Op,
+                                              SelectionDAG &DAG) const {
+  // FIXME: Need pseudo-fence for 'singlethread' fences
+  // FIXME: Set SType for weaker fences where supported/appropriate.
+  unsigned SType = 0;
+  SDLoc DL(Op);
+  return DAG.getNode(LoongArchISD::DBAR, DL, MVT::Other, Op.getOperand(0),
+                     DAG.getConstant(SType, DL, MVT::i32));
+}
+
+SDValue LoongArchTargetLowering::lowerShiftLeftParts(SDValue Op,
+                                                SelectionDAG &DAG) const {
+  SDLoc DL(Op);
+  MVT VT = Subtarget.is64Bit() ? MVT::i64 : MVT::i32;
+
+  SDValue Lo = Op.getOperand(0), Hi = Op.getOperand(1);
+  SDValue Shamt = Op.getOperand(2);
+  // if shamt < (VT.bits):
+  //  lo = (shl lo, shamt)
+  //  hi = (or (shl hi, shamt) (srl (srl lo, 1), ~shamt))
+  // else:
+  //  lo = 0
+  //  hi = (shl lo, shamt[4:0])
+  SDValue Not = DAG.getNode(ISD::XOR, DL, MVT::i32, Shamt,
+                            DAG.getConstant(-1, DL, MVT::i32));
+  SDValue ShiftRight1Lo = DAG.getNode(ISD::SRL, DL, VT, Lo,
+                                      DAG.getConstant(1, DL, VT));
+  SDValue ShiftRightLo = DAG.getNode(ISD::SRL, DL, VT, ShiftRight1Lo, Not);
+  SDValue ShiftLeftHi = DAG.getNode(ISD::SHL, DL, VT, Hi, Shamt);
+  SDValue Or = DAG.getNode(ISD::OR, DL, VT, ShiftLeftHi, ShiftRightLo);
+  SDValue ShiftLeftLo = DAG.getNode(ISD::SHL, DL, VT, Lo, Shamt);
+  SDValue Cond = DAG.getNode(ISD::AND, DL, MVT::i32, Shamt,
+                             DAG.getConstant(VT.getSizeInBits(), DL, MVT::i32));
+  Lo = DAG.getNode(ISD::SELECT, DL, VT, Cond,
+                   DAG.getConstant(0, DL, VT), ShiftLeftLo);
+  Hi = DAG.getNode(ISD::SELECT, DL, VT, Cond, ShiftLeftLo, Or);
+
+  SDValue Ops[2] = {Lo, Hi};
+  return DAG.getMergeValues(Ops, DL);
+}
+
+SDValue LoongArchTargetLowering::lowerShiftRightParts(SDValue Op, SelectionDAG &DAG,
+                                                 bool IsSRA) const {
+  SDLoc DL(Op);
+  SDValue Lo = Op.getOperand(0), Hi = Op.getOperand(1);
+  SDValue Shamt = Op.getOperand(2);
+  MVT VT = Subtarget.is64Bit() ? MVT::i64 : MVT::i32;
+
+  // if shamt < (VT.bits):
+  //  lo = (or (shl (shl hi, 1), ~shamt) (srl lo, shamt))
+  //  if isSRA:
+  //    hi = (sra hi, shamt)
+  //  else:
+  //    hi = (srl hi, shamt)
+  // else:
+  //  if isSRA:
+  //   lo = (sra hi, shamt[4:0])
+  //   hi = (sra hi, 31)
+  //  else:
+  //   lo = (srl hi, shamt[4:0])
+  //   hi = 0
+  SDValue Not = DAG.getNode(ISD::XOR, DL, MVT::i32, Shamt,
+                            DAG.getConstant(-1, DL, MVT::i32));
+  SDValue ShiftLeft1Hi = DAG.getNode(ISD::SHL, DL, VT, Hi,
+                                     DAG.getConstant(1, DL, VT));
+  SDValue ShiftLeftHi = DAG.getNode(ISD::SHL, DL, VT, ShiftLeft1Hi, Not);
+  SDValue ShiftRightLo = DAG.getNode(ISD::SRL, DL, VT, Lo, Shamt);
+  SDValue Or = DAG.getNode(ISD::OR, DL, VT, ShiftLeftHi, ShiftRightLo);
+  SDValue ShiftRightHi = DAG.getNode(IsSRA ? ISD::SRA : ISD::SRL,
+                                     DL, VT, Hi, Shamt);
+  SDValue Cond = DAG.getNode(ISD::AND, DL, MVT::i32, Shamt,
+                             DAG.getConstant(VT.getSizeInBits(), DL, MVT::i32));
+  SDValue Ext = DAG.getNode(ISD::SRA, DL, VT, Hi,
+                            DAG.getConstant(VT.getSizeInBits() - 1, DL, VT));
+  Lo = DAG.getNode(ISD::SELECT, DL, VT, Cond, ShiftRightHi, Or);
+  Hi = DAG.getNode(ISD::SELECT, DL, VT, Cond,
+                   IsSRA ? Ext : DAG.getConstant(0, DL, VT), ShiftRightHi);
+
+  SDValue Ops[2] = {Lo, Hi};
+  return DAG.getMergeValues(Ops, DL);
+}
+
+// Lower (store (fp_to_sint $fp) $ptr) to (store (TruncIntFP $fp), $ptr).
+static SDValue lowerFP_TO_SINT_STORE(StoreSDNode *SD, SelectionDAG &DAG,
+                                     bool SingleFloat) {
+  SDValue Val = SD->getValue();
+
+  if (Val.getOpcode() != ISD::FP_TO_SINT ||
+      (Val.getValueSizeInBits() > 32 && SingleFloat))
+    return SDValue();
+
+  EVT FPTy = EVT::getFloatingPointVT(Val.getValueSizeInBits());
+  SDValue Tr = DAG.getNode(LoongArchISD::TruncIntFP, SDLoc(Val), FPTy,
+                           Val.getOperand(0));
+  return DAG.getStore(SD->getChain(), SDLoc(SD), Tr, SD->getBasePtr(),
+                      SD->getPointerInfo(), SD->getAlignment(),
+                      SD->getMemOperand()->getFlags());
+}
+
+SDValue LoongArchTargetLowering::lowerSTORE(SDValue Op, SelectionDAG &DAG) const {
+  StoreSDNode *SD = cast<StoreSDNode>(Op);
+  return lowerFP_TO_SINT_STORE(SD, DAG, Subtarget.isSingleFloat());
+}
+
+SDValue LoongArchTargetLowering::lowerINTRINSIC_WO_CHAIN(SDValue Op,
+                                                      SelectionDAG &DAG) const {
+  SDLoc DL(Op);
+  unsigned Intrinsic = cast<ConstantSDNode>(Op->getOperand(0))->getZExtValue();
+  switch (Intrinsic) {
+  default:
+    return SDValue();
+  case Intrinsic::thread_pointer: {
+    EVT PtrVT = getPointerTy(DAG.getDataLayout());
+    if (PtrVT ==  MVT::i64)
+      return DAG.getRegister(LoongArch::TP_64, MVT::i64);
+    return DAG.getRegister(LoongArch::TP, MVT::i32);
+  }
+  }
+}
+
+SDValue LoongArchTargetLowering::lowerINTRINSIC_W_CHAIN(SDValue Op,
+                                                     SelectionDAG &DAG) const {
+  return SDValue();
+}
+
+SDValue LoongArchTargetLowering::lowerINTRINSIC_VOID(SDValue Op,
+                                                  SelectionDAG &DAG) const {
+  unsigned Intr = cast<ConstantSDNode>(Op->getOperand(1))->getZExtValue();
+  switch (Intr) {
+  default:
+    return SDValue();
+  }
+}
+
+SDValue LoongArchTargetLowering::lowerUINT_TO_FP(SDValue Op, SelectionDAG &DAG) const {
+  SDLoc DL(Op);
+  EVT ResTy = Op->getValueType(0);
+  Op = LowerSUINT_TO_FP(ISD::ZERO_EXTEND_VECTOR_INREG, Op, DAG);
+  if (!ResTy.isVector())
+    return Op;
+  return DAG.getNode(ISD::UINT_TO_FP, DL, ResTy, Op);
+}
+
+SDValue LoongArchTargetLowering::lowerSINT_TO_FP(SDValue Op, SelectionDAG &DAG) const {
+  SDLoc DL(Op);
+  EVT ResTy = Op->getValueType(0);
+  Op = LowerSUINT_TO_FP(ISD::SIGN_EXTEND_VECTOR_INREG, Op, DAG);
+  if (!ResTy.isVector())
+    return Op;
+  return DAG.getNode(ISD::SINT_TO_FP, DL, ResTy, Op);
+}
+
+SDValue LoongArchTargetLowering::lowerFP_TO_UINT(SDValue Op, SelectionDAG &DAG) const {
+  return SDValue();
+}
+
+SDValue LoongArchTargetLowering::lowerFP_TO_SINT(SDValue Op, SelectionDAG &DAG) const {
+  if (Op.getValueSizeInBits() > 32 && Subtarget.isSingleFloat())
+    return SDValue();
+
+  EVT FPTy = EVT::getFloatingPointVT(Op.getValueSizeInBits());
+  SDValue Trunc = DAG.getNode(LoongArchISD::TruncIntFP, SDLoc(Op), FPTy,
+                              Op.getOperand(0));
+  return DAG.getNode(ISD::BITCAST, SDLoc(Op), Op.getValueType(), Trunc);
+}
+
+SDValue LoongArchTargetLowering::lowerEH_DWARF_CFA(SDValue Op,
+                                              SelectionDAG &DAG) const {
+
+  // Return a fixed StackObject with offset 0 which points to the old stack
+  // pointer.
+  MachineFrameInfo &MFI = DAG.getMachineFunction().getFrameInfo();
+  EVT ValTy = Op->getValueType(0);
+  int FI = MFI.CreateFixedObject(Op.getValueSizeInBits() / 8, 0, false);
+  return DAG.getFrameIndex(FI, ValTy);
+}
+
+bool LoongArchTargetLowering::isEligibleForTailCallOptimization(
+    const CCState &CCInfo, unsigned NextStackOffset,
+    const LoongArchFunctionInfo &FI) const {
+  if (!UseLoongArchTailCalls)
+    return false;
+
+  // Return false if either the callee or caller has a byval argument.
+  if (CCInfo.getInRegsParamsCount() > 0 || FI.hasByvalArg())
+    return false;
+
+  // Return true if the callee's argument area is no larger than the
+  // caller's.
+  return NextStackOffset <= FI.getIncomingArgSize();
+}
+
+//===----------------------------------------------------------------------===//
+//                      Calling Convention Implementation
+//===----------------------------------------------------------------------===//
+
+//===----------------------------------------------------------------------===//
+// TODO: Implement a generic logic using tblgen that can support this.
+// LoongArch LP32 ABI rules:
+// ---
+// i32 - Passed in A0, A1, A2, A3 and stack
+// f32 - Only passed in f32 registers if no int reg has been used yet to hold
+//       an argument. Otherwise, passed in A1, A2, A3 and stack.
+// f64 - Only passed in two aliased f32 registers if no int reg has been used
+//       yet to hold an argument. Otherwise, use A2, A3 and stack. If A1 is
+//       not used, it must be shadowed. If only A3 is available, shadow it and
+//       go to stack.
+// vXiX - Received as scalarized i32s, passed in A0 - A3 and the stack.
+// vXf32 - Passed in either a pair of registers {A0, A1}, {A2, A3} or {A0 - A3}
+//         with the remainder spilled to the stack.
+// vXf64 - Passed in either {A0, A1, A2, A3} or {A2, A3} and in both cases
+//         spilling the remainder to the stack.
+//
+//  For vararg functions, all arguments are passed in A0, A1, A2, A3 and stack.
+//===----------------------------------------------------------------------===//
+
+static bool CC_LoongArchLP32(unsigned ValNo, MVT ValVT, MVT LocVT,
+                       CCValAssign::LocInfo LocInfo, ISD::ArgFlagsTy ArgFlags,
+                       CCState &State, ArrayRef<MCPhysReg> F64Regs) {
+  static const MCPhysReg IntRegs[] = { LoongArch::A0, LoongArch::A1, LoongArch::A2, LoongArch::A3 };
+
+  const LoongArchCCState * LoongArchState = static_cast<LoongArchCCState *>(&State);
+
+  static const MCPhysReg F32Regs[] = { LoongArch::F12, LoongArch::F14 };
+
+  static const MCPhysReg FloatVectorIntRegs[] = { LoongArch::A0, LoongArch::A2 };
+
+  // Do not process byval args here.
+  if (ArgFlags.isByVal())
+    return true;
+
+
+  // Promote i8 and i16
+  if (LocVT == MVT::i8 || LocVT == MVT::i16) {
+    LocVT = MVT::i32;
+    if (ArgFlags.isSExt())
+      LocInfo = CCValAssign::SExt;
+    else if (ArgFlags.isZExt())
+      LocInfo = CCValAssign::ZExt;
+    else
+      LocInfo = CCValAssign::AExt;
+  }
+
+  unsigned Reg;
+
+  // f32 and f64 are allocated in A0, A1, A2, A3 when either of the following
+  // is true: function is vararg, argument is 3rd or higher, there is previous
+  // argument which is not f32 or f64.
+  bool AllocateFloatsInIntReg = State.isVarArg() || ValNo > 1 ||
+                                State.getFirstUnallocated(F32Regs) != ValNo;
+  Align OrigAlign = ArgFlags.getNonZeroOrigAlign();
+  bool isI64 = (ValVT == MVT::i32 && OrigAlign == Align(8));
+  bool isVectorFloat = LoongArchState->WasOriginalArgVectorFloat(ValNo);
+
+  // The LoongArch vector ABI for floats passes them in a pair of registers
+  if (ValVT == MVT::i32 && isVectorFloat) {
+    // This is the start of an vector that was scalarized into an unknown number
+    // of components. It doesn't matter how many there are. Allocate one of the
+    // notional 8 byte aligned registers which map onto the argument stack, and
+    // shadow the register lost to alignment requirements.
+    if (ArgFlags.isSplit()) {
+      Reg = State.AllocateReg(FloatVectorIntRegs);
+      if (Reg == LoongArch::A2)
+        State.AllocateReg(LoongArch::A1);
+      else if (Reg == 0)
+        State.AllocateReg(LoongArch::A3);
+    } else {
+      // If we're an intermediate component of the split, we can just attempt to
+      // allocate a register directly.
+      Reg = State.AllocateReg(IntRegs);
+    }
+  } else if (ValVT == MVT::i32 || (ValVT == MVT::f32 && AllocateFloatsInIntReg)) {
+    Reg = State.AllocateReg(IntRegs);
+    // If this is the first part of an i64 arg,
+    // the allocated register must be either A0 or A2.
+    if (isI64 && (Reg == LoongArch::A1 || Reg == LoongArch::A3))
+      Reg = State.AllocateReg(IntRegs);
+    LocVT = MVT::i32;
+  } else if (ValVT == MVT::f64 && AllocateFloatsInIntReg) {
+    // Allocate int register and shadow next int register. If first
+    // available register is LoongArch::A1 or LoongArch::A3, shadow it too.
+    Reg = State.AllocateReg(IntRegs);
+    if (Reg == LoongArch::A1 || Reg == LoongArch::A3)
+      Reg = State.AllocateReg(IntRegs);
+    State.AllocateReg(IntRegs);
+    LocVT = MVT::i32;
+  } else if (ValVT.isFloatingPoint() && !AllocateFloatsInIntReg) {
+    // we are guaranteed to find an available float register
+    if (ValVT == MVT::f32) {
+      Reg = State.AllocateReg(F32Regs);
+      // Shadow int register
+      State.AllocateReg(IntRegs);
+    } else {
+      Reg = State.AllocateReg(F64Regs);
+      // Shadow int registers
+      unsigned Reg2 = State.AllocateReg(IntRegs);
+      if (Reg2 == LoongArch::A1 || Reg2 == LoongArch::A3)
+        State.AllocateReg(IntRegs);
+      State.AllocateReg(IntRegs);
+    }
+  } else
+    llvm_unreachable("Cannot handle this ValVT.");
+
+  if (!Reg) {
+    unsigned Offset = State.AllocateStack(ValVT.getStoreSize(), OrigAlign);
+    State.addLoc(CCValAssign::getMem(ValNo, ValVT, Offset, LocVT, LocInfo));
+  } else
+    State.addLoc(CCValAssign::getReg(ValNo, ValVT, Reg, LocVT, LocInfo));
+
+  return false;
+}
+
+static bool CC_LoongArchLP32_FP32(unsigned ValNo, MVT ValVT,
+                            MVT LocVT, CCValAssign::LocInfo LocInfo,
+                            ISD::ArgFlagsTy ArgFlags, CCState &State) {
+  static const MCPhysReg F64Regs[] = {LoongArch::F0_64, LoongArch::F1_64, LoongArch::F2_64, \
+                                      LoongArch::F3_64, LoongArch::F4_64, LoongArch::F5_64, \
+                                      LoongArch::F6_64, LoongArch::F7_64 };
+
+  return CC_LoongArchLP32(ValNo, ValVT, LocVT, LocInfo, ArgFlags, State, F64Regs);
+}
+
+static bool CC_LoongArchLP32_FP64(unsigned ValNo, MVT ValVT,
+                            MVT LocVT, CCValAssign::LocInfo LocInfo,
+                            ISD::ArgFlagsTy ArgFlags, CCState &State) {
+  static const MCPhysReg F64Regs[] = {LoongArch::F0_64, LoongArch::F1_64, LoongArch::F2_64, \
+                                      LoongArch::F3_64, LoongArch::F4_64, LoongArch::F5_64, \
+                                      LoongArch::F6_64, LoongArch::F7_64 };
+
+  return CC_LoongArchLP32(ValNo, ValVT, LocVT, LocInfo, ArgFlags, State, F64Regs);
+}
+
+static bool CC_LoongArch_F128(unsigned ValNo, MVT ValVT,
+                            MVT LocVT, CCValAssign::LocInfo LocInfo,
+                            ISD::ArgFlagsTy ArgFlags, CCState &State) LLVM_ATTRIBUTE_UNUSED;
+
+static bool CC_LoongArch_F128(unsigned ValNo, MVT ValVT,
+                            MVT LocVT, CCValAssign::LocInfo LocInfo,
+                            ISD::ArgFlagsTy ArgFlags, CCState &State) {
+
+  static const MCPhysReg ArgRegs[8] = {
+      LoongArch::A0_64, LoongArch::A1_64, LoongArch::A2_64, LoongArch::A3_64,
+      LoongArch::A4_64, LoongArch::A5_64, LoongArch::A6_64, LoongArch::A7_64};
+
+  unsigned Idx = State.getFirstUnallocated(ArgRegs);
+  // Skip 'odd' register if necessary.
+  if (!ArgFlags.isSplitEnd() && Idx != array_lengthof(ArgRegs) && Idx % 2 == 1)
+    State.AllocateReg(ArgRegs);
+  return true;
+}
+
+static bool CC_LoongArchLP32(unsigned ValNo, MVT ValVT, MVT LocVT,
+                       CCValAssign::LocInfo LocInfo, ISD::ArgFlagsTy ArgFlags,
+                       CCState &State) LLVM_ATTRIBUTE_UNUSED;
+
+#include "LoongArchGenCallingConv.inc"
+
+ CCAssignFn *LoongArchTargetLowering::CCAssignFnForCall() const{
+   return CC_LoongArch;
+ }
+
+ CCAssignFn *LoongArchTargetLowering::CCAssignFnForReturn() const{
+   return RetCC_LoongArch;
+ }
+
+//===----------------------------------------------------------------------===//
+//                  Call Calling Convention Implementation
+//===----------------------------------------------------------------------===//
+SDValue LoongArchTargetLowering::passArgOnStack(SDValue StackPtr, unsigned Offset,
+                                           SDValue Chain, SDValue Arg,
+                                           const SDLoc &DL, bool IsTailCall,
+                                           SelectionDAG &DAG) const {
+  if (!IsTailCall) {
+    SDValue PtrOff =
+        DAG.getNode(ISD::ADD, DL, getPointerTy(DAG.getDataLayout()), StackPtr,
+                    DAG.getIntPtrConstant(Offset, DL));
+    return DAG.getStore(Chain, DL, Arg, PtrOff, MachinePointerInfo());
+  }
+
+  MachineFrameInfo &MFI = DAG.getMachineFunction().getFrameInfo();
+  int FI = MFI.CreateFixedObject(Arg.getValueSizeInBits() / 8, Offset, false);
+  SDValue FIN = DAG.getFrameIndex(FI, getPointerTy(DAG.getDataLayout()));
+  return DAG.getStore(Chain, DL, Arg, FIN, MachinePointerInfo(),
+                      /* Alignment = */ 0, MachineMemOperand::MOVolatile);
+}
+
+void LoongArchTargetLowering::
+getOpndList(SmallVectorImpl<SDValue> &Ops,
+            std::deque<std::pair<unsigned, SDValue>> &RegsToPass,
+            bool IsPICCall, bool GlobalOrExternal, bool InternalLinkage,
+            bool IsCallReloc, CallLoweringInfo &CLI, SDValue Callee,
+            SDValue Chain) const {
+  // Build a sequence of copy-to-reg nodes chained together with token
+  // chain and flag operands which copy the outgoing args into registers.
+  // The InFlag in necessary since all emitted instructions must be
+  // stuck together.
+  SDValue InFlag;
+
+  Ops.push_back(Callee);
+
+  for (unsigned i = 0, e = RegsToPass.size(); i != e; ++i) {
+    Chain = CLI.DAG.getCopyToReg(Chain, CLI.DL, RegsToPass[i].first,
+                                 RegsToPass[i].second, InFlag);
+    InFlag = Chain.getValue(1);
+  }
+
+  // Add argument registers to the end of the list so that they are
+  // known live into the call.
+  for (unsigned i = 0, e = RegsToPass.size(); i != e; ++i)
+    Ops.push_back(CLI.DAG.getRegister(RegsToPass[i].first,
+                                      RegsToPass[i].second.getValueType()));
+
+  // Add a register mask operand representing the call-preserved registers.
+  const TargetRegisterInfo *TRI = Subtarget.getRegisterInfo();
+  const uint32_t *Mask =
+      TRI->getCallPreservedMask(CLI.DAG.getMachineFunction(), CLI.CallConv);
+  assert(Mask && "Missing call preserved mask for calling convention");
+  Ops.push_back(CLI.DAG.getRegisterMask(Mask));
+
+  if (InFlag.getNode())
+    Ops.push_back(InFlag);
+}
+
+void LoongArchTargetLowering::AdjustInstrPostInstrSelection(MachineInstr &MI,
+                                                       SDNode *Node) const {
+  switch (MI.getOpcode()) {
+    default:
+      return;
+  }
+}
+
+/// LowerCall - functions arguments are copied from virtual regs to
+/// (physical regs)/(stack frame), CALLSEQ_START and CALLSEQ_END are emitted.
+SDValue
+LoongArchTargetLowering::LowerCall(TargetLowering::CallLoweringInfo &CLI,
+                              SmallVectorImpl<SDValue> &InVals) const {
+  SelectionDAG &DAG                     = CLI.DAG;
+  SDLoc DL                              = CLI.DL;
+  SmallVectorImpl<ISD::OutputArg> &Outs = CLI.Outs;
+  SmallVectorImpl<SDValue> &OutVals     = CLI.OutVals;
+  SmallVectorImpl<ISD::InputArg> &Ins   = CLI.Ins;
+  SDValue Chain                         = CLI.Chain;
+  SDValue Callee                        = CLI.Callee;
+  bool &IsTailCall                      = CLI.IsTailCall;
+  CallingConv::ID CallConv              = CLI.CallConv;
+  bool IsVarArg                         = CLI.IsVarArg;
+
+  MachineFunction &MF = DAG.getMachineFunction();
+  MachineFrameInfo &MFI = MF.getFrameInfo();
+  const TargetFrameLowering *TFL = Subtarget.getFrameLowering();
+  bool IsPIC = isPositionIndependent();
+
+  // Analyze operands of the call, assigning locations to each operand.
+  SmallVector<CCValAssign, 16> ArgLocs;
+  LoongArchCCState CCInfo(
+      CallConv, IsVarArg, DAG.getMachineFunction(), ArgLocs, *DAG.getContext(),
+      LoongArchCCState::getSpecialCallingConvForCallee(Callee.getNode(), Subtarget));
+
+  const ExternalSymbolSDNode *ES =
+      dyn_cast_or_null<const ExternalSymbolSDNode>(Callee.getNode());
+
+  // There is one case where CALLSEQ_START..CALLSEQ_END can be nested, which
+  // is during the lowering of a call with a byval argument which produces
+  // a call to memcpy. For the LP32 case, this causes the caller to allocate
+  // stack space for the reserved argument area for the callee, then recursively
+  // again for the memcpy call. In the NEWABI case, this doesn't occur as those
+  // ABIs mandate that the callee allocates the reserved argument area. We do
+  // still produce nested CALLSEQ_START..CALLSEQ_END with zero space though.
+  //
+  // If the callee has a byval argument and memcpy is used, we are mandated
+  // to already have produced a reserved argument area for the callee for LP32.
+  // Therefore, the reserved argument area can be reused for both calls.
+  //
+  // Other cases of calling memcpy cannot have a chain with a CALLSEQ_START
+  // present, as we have yet to hook that node onto the chain.
+  //
+  // Hence, the CALLSEQ_START and CALLSEQ_END nodes can be eliminated in this
+  // case. GCC does a similar trick, in that wherever possible, it calculates
+  // the maximum out going argument area (including the reserved area), and
+  // preallocates the stack space on entrance to the caller.
+  //
+  // FIXME: We should do the same for efficiency and space.
+
+  // Note: The check on the calling convention below must match
+  //       LoongArchABIInfo::GetCalleeAllocdArgSizeInBytes().
+  bool MemcpyInByVal = ES &&
+                       StringRef(ES->getSymbol()) == StringRef("memcpy") &&
+                       CallConv != CallingConv::Fast &&
+                       Chain.getOpcode() == ISD::CALLSEQ_START;
+
+  // Allocate the reserved argument area. It seems strange to do this from the
+  // caller side but removing it breaks the frame size calculation.
+  unsigned ReservedArgArea =
+      MemcpyInByVal ? 0 : ABI.GetCalleeAllocdArgSizeInBytes(CallConv);
+  CCInfo.AllocateStack(ReservedArgArea, Align(1));
+
+  CCInfo.AnalyzeCallOperands(Outs, CC_LoongArch, CLI.getArgs(),
+                             ES ? ES->getSymbol() : nullptr);
+
+  // Get a count of how many bytes are to be pushed on the stack.
+  unsigned NextStackOffset = CCInfo.getNextStackOffset();
+
+  // Check if it's really possible to do a tail call. Restrict it to functions
+  // that are part of this compilation unit.
+  bool InternalLinkage = false;
+  if (IsTailCall) {
+    IsTailCall = isEligibleForTailCallOptimization(
+        CCInfo, NextStackOffset, *MF.getInfo<LoongArchFunctionInfo>());
+     if (GlobalAddressSDNode *G = dyn_cast<GlobalAddressSDNode>(Callee)) {
+      InternalLinkage = G->getGlobal()->hasInternalLinkage();
+      IsTailCall &= (InternalLinkage || G->getGlobal()->hasLocalLinkage() ||
+                     G->getGlobal()->hasPrivateLinkage() ||
+                     G->getGlobal()->hasHiddenVisibility() ||
+                     G->getGlobal()->hasProtectedVisibility());
+     }
+  }
+  if (!IsTailCall && CLI.CB && CLI.CB->isMustTailCall())
+    report_fatal_error("failed to perform tail call elimination on a call "
+                       "site marked musttail");
+
+  if (IsTailCall)
+    ++NumTailCalls;
+
+  // Chain is the output chain of the last Load/Store or CopyToReg node.
+  // ByValChain is the output chain of the last Memcpy node created for copying
+  // byval arguments to the stack.
+  unsigned StackAlignment = TFL->getStackAlignment();
+  NextStackOffset = alignTo(NextStackOffset, StackAlignment);
+  SDValue NextStackOffsetVal = DAG.getIntPtrConstant(NextStackOffset, DL, true);
+
+  if (!(IsTailCall || MemcpyInByVal))
+    Chain = DAG.getCALLSEQ_START(Chain, NextStackOffset, 0, DL);
+
+  SDValue StackPtr =
+      DAG.getCopyFromReg(Chain, DL, ABI.IsLP64D() ? LoongArch::SP_64 : LoongArch::SP,
+                         getPointerTy(DAG.getDataLayout()));
+
+  std::deque<std::pair<unsigned, SDValue>> RegsToPass;
+  SmallVector<SDValue, 8> MemOpChains;
+
+  CCInfo.rewindByValRegsInfo();
+
+  // Walk the register/memloc assignments, inserting copies/loads.
+  for (unsigned i = 0, e = ArgLocs.size(); i != e; ++i) {
+    SDValue Arg = OutVals[i];
+    CCValAssign &VA = ArgLocs[i];
+    MVT ValVT = VA.getValVT(), LocVT = VA.getLocVT();
+    ISD::ArgFlagsTy Flags = Outs[i].Flags;
+    bool UseUpperBits = false;
+
+    // ByVal Arg.
+    if (Flags.isByVal()) {
+      unsigned FirstByValReg, LastByValReg;
+      unsigned ByValIdx = CCInfo.getInRegsParamsProcessed();
+      CCInfo.getInRegsParamInfo(ByValIdx, FirstByValReg, LastByValReg);
+
+      assert(Flags.getByValSize() &&
+             "ByVal args of size 0 should have been ignored by front-end.");
+      assert(ByValIdx < CCInfo.getInRegsParamsCount());
+      assert(!IsTailCall &&
+             "Do not tail-call optimize if there is a byval argument.");
+      passByValArg(Chain, DL, RegsToPass, MemOpChains, StackPtr, MFI, DAG, Arg,
+                   FirstByValReg, LastByValReg, Flags,
+                   VA);
+      CCInfo.nextInRegsParam();
+      continue;
+    }
+
+    // Promote the value if needed.
+    switch (VA.getLocInfo()) {
+    default:
+      llvm_unreachable("Unknown loc info!");
+    case CCValAssign::Full:
+      if (VA.isRegLoc()) {
+        if ((ValVT == MVT::f32 && LocVT == MVT::i32) ||
+            (ValVT == MVT::f64 && LocVT == MVT::i64) ||
+            (ValVT == MVT::i64 && LocVT == MVT::f64))
+          Arg = DAG.getNode(ISD::BITCAST, DL, LocVT, Arg);
+      }
+      break;
+    case CCValAssign::BCvt:
+      Arg = DAG.getNode(ISD::BITCAST, DL, LocVT, Arg);
+      break;
+    case CCValAssign::SExtUpper:
+      UseUpperBits = true;
+      LLVM_FALLTHROUGH;
+    case CCValAssign::SExt:
+      Arg = DAG.getNode(ISD::SIGN_EXTEND, DL, LocVT, Arg);
+      break;
+    case CCValAssign::ZExtUpper:
+      UseUpperBits = true;
+      LLVM_FALLTHROUGH;
+    case CCValAssign::ZExt:
+      Arg = DAG.getNode(ISD::ZERO_EXTEND, DL, LocVT, Arg);
+      break;
+    case CCValAssign::AExtUpper:
+      UseUpperBits = true;
+      LLVM_FALLTHROUGH;
+    case CCValAssign::AExt:
+      Arg = DAG.getNode(ISD::ANY_EXTEND, DL, LocVT, Arg);
+      break;
+    }
+
+    if (UseUpperBits) {
+      unsigned ValSizeInBits = Outs[i].ArgVT.getSizeInBits();
+      unsigned LocSizeInBits = VA.getLocVT().getSizeInBits();
+      Arg = DAG.getNode(
+          ISD::SHL, DL, VA.getLocVT(), Arg,
+          DAG.getConstant(LocSizeInBits - ValSizeInBits, DL, VA.getLocVT()));
+    }
+
+    // Arguments that can be passed on register must be kept at
+    // RegsToPass vector
+    if (VA.isRegLoc()) {
+      RegsToPass.push_back(std::make_pair(VA.getLocReg(), Arg));
+      continue;
+    }
+
+    // Register can't get to this point...
+    assert(VA.isMemLoc());
+
+    // emit ISD::STORE whichs stores the
+    // parameter value to a stack Location
+    MemOpChains.push_back(passArgOnStack(StackPtr, VA.getLocMemOffset(),
+                                         Chain, Arg, DL, IsTailCall, DAG));
+  }
+
+  // Transform all store nodes into one single node because all store
+  // nodes are independent of each other.
+  if (!MemOpChains.empty())
+    Chain = DAG.getNode(ISD::TokenFactor, DL, MVT::Other, MemOpChains);
+
+  // If the callee is a GlobalAddress/ExternalSymbol node (quite common, every
+  // direct call is) turn it into a TargetGlobalAddress/TargetExternalSymbol
+  // node so that legalize doesn't hack it.
+
+  bool GlobalOrExternal = false, IsCallReloc = false;
+
+  if (GlobalAddressSDNode *G = dyn_cast<GlobalAddressSDNode>(Callee)) {
+    Callee = DAG.getTargetGlobalAddress(G->getGlobal(), DL,
+                                        getPointerTy(DAG.getDataLayout()), 0,
+                                        LoongArchII::MO_NO_FLAG);
+    GlobalOrExternal = true;
+  }
+  else if (ExternalSymbolSDNode *S = dyn_cast<ExternalSymbolSDNode>(Callee)) {
+    const char *Sym = S->getSymbol();
+    Callee = DAG.getTargetExternalSymbol(
+        Sym, getPointerTy(DAG.getDataLayout()), LoongArchII::MO_NO_FLAG);
+
+    GlobalOrExternal = true;
+  }
+
+  SmallVector<SDValue, 8> Ops(1, Chain);
+  SDVTList NodeTys = DAG.getVTList(MVT::Other, MVT::Glue);
+
+  getOpndList(Ops, RegsToPass, IsPIC, GlobalOrExternal, InternalLinkage,
+              IsCallReloc, CLI, Callee, Chain);
+
+  if (IsTailCall) {
+    MF.getFrameInfo().setHasTailCall();
+    return DAG.getNode(LoongArchISD::TailCall, DL, MVT::Other, Ops);
+  }
+
+  Chain = DAG.getNode(LoongArchISD::JmpLink, DL, NodeTys, Ops);
+  SDValue InFlag = Chain.getValue(1);
+
+  // Create the CALLSEQ_END node in the case of where it is not a call to
+  // memcpy.
+  if (!(MemcpyInByVal)) {
+    Chain = DAG.getCALLSEQ_END(Chain, NextStackOffsetVal,
+                               DAG.getIntPtrConstant(0, DL, true), InFlag, DL);
+    InFlag = Chain.getValue(1);
+  }
+
+  // Handle result values, copying them out of physregs into vregs that we
+  // return.
+  return LowerCallResult(Chain, InFlag, CallConv, IsVarArg, Ins, DL, DAG,
+                         InVals, CLI);
+}
+
+/// LowerCallResult - Lower the result values of a call into the
+/// appropriate copies out of appropriate physical registers.
+SDValue LoongArchTargetLowering::LowerCallResult(
+    SDValue Chain, SDValue InFlag, CallingConv::ID CallConv, bool IsVarArg,
+    const SmallVectorImpl<ISD::InputArg> &Ins, const SDLoc &DL,
+    SelectionDAG &DAG, SmallVectorImpl<SDValue> &InVals,
+    TargetLowering::CallLoweringInfo &CLI) const {
+  // Assign locations to each value returned by this call.
+  SmallVector<CCValAssign, 16> RVLocs;
+  LoongArchCCState CCInfo(CallConv, IsVarArg, DAG.getMachineFunction(), RVLocs,
+                     *DAG.getContext());
+
+  const ExternalSymbolSDNode *ES =
+      dyn_cast_or_null<const ExternalSymbolSDNode>(CLI.Callee.getNode());
+  CCInfo.AnalyzeCallResult(Ins, RetCC_LoongArch, CLI.RetTy,
+                           ES ? ES->getSymbol() : nullptr);
+
+  // Copy all of the result registers out of their specified physreg.
+  for (unsigned i = 0; i != RVLocs.size(); ++i) {
+    CCValAssign &VA = RVLocs[i];
+    assert(VA.isRegLoc() && "Can only return in registers!");
+
+    SDValue Val = DAG.getCopyFromReg(Chain, DL, RVLocs[i].getLocReg(),
+                                     RVLocs[i].getLocVT(), InFlag);
+    Chain = Val.getValue(1);
+    InFlag = Val.getValue(2);
+
+    if (VA.isUpperBitsInLoc()) {
+      unsigned ValSizeInBits = Ins[i].ArgVT.getSizeInBits();
+      unsigned LocSizeInBits = VA.getLocVT().getSizeInBits();
+      unsigned Shift =
+          VA.getLocInfo() == CCValAssign::ZExtUpper ? ISD::SRL : ISD::SRA;
+      Val = DAG.getNode(
+          Shift, DL, VA.getLocVT(), Val,
+          DAG.getConstant(LocSizeInBits - ValSizeInBits, DL, VA.getLocVT()));
+    }
+
+    switch (VA.getLocInfo()) {
+    default:
+      llvm_unreachable("Unknown loc info!");
+    case CCValAssign::Full:
+      break;
+    case CCValAssign::BCvt:
+      Val = DAG.getNode(ISD::BITCAST, DL, VA.getValVT(), Val);
+      break;
+    case CCValAssign::AExt:
+    case CCValAssign::AExtUpper:
+      Val = DAG.getNode(ISD::TRUNCATE, DL, VA.getValVT(), Val);
+      break;
+    case CCValAssign::ZExt:
+    case CCValAssign::ZExtUpper:
+      Val = DAG.getNode(ISD::AssertZext, DL, VA.getLocVT(), Val,
+                        DAG.getValueType(VA.getValVT()));
+      Val = DAG.getNode(ISD::TRUNCATE, DL, VA.getValVT(), Val);
+      break;
+    case CCValAssign::SExt:
+    case CCValAssign::SExtUpper:
+      Val = DAG.getNode(ISD::AssertSext, DL, VA.getLocVT(), Val,
+                        DAG.getValueType(VA.getValVT()));
+      Val = DAG.getNode(ISD::TRUNCATE, DL, VA.getValVT(), Val);
+      break;
+    }
+
+    InVals.push_back(Val);
+  }
+
+  return Chain;
+}
+
+static SDValue UnpackFromArgumentSlot(SDValue Val, const CCValAssign &VA,
+                                      EVT ArgVT, const SDLoc &DL,
+                                      SelectionDAG &DAG) {
+  MVT LocVT = VA.getLocVT();
+  EVT ValVT = VA.getValVT();
+
+  // Shift into the upper bits if necessary.
+  switch (VA.getLocInfo()) {
+  default:
+    break;
+  case CCValAssign::AExtUpper:
+  case CCValAssign::SExtUpper:
+  case CCValAssign::ZExtUpper: {
+    unsigned ValSizeInBits = ArgVT.getSizeInBits();
+    unsigned LocSizeInBits = VA.getLocVT().getSizeInBits();
+    unsigned Opcode =
+        VA.getLocInfo() == CCValAssign::ZExtUpper ? ISD::SRL : ISD::SRA;
+    Val = DAG.getNode(
+        Opcode, DL, VA.getLocVT(), Val,
+        DAG.getConstant(LocSizeInBits - ValSizeInBits, DL, VA.getLocVT()));
+    break;
+  }
+  }
+
+  // If this is an value smaller than the argument slot size (32-bit for LP32,
+  // 64-bit for LPX32/LP64D), it has been promoted in some way to the argument slot
+  // size. Extract the value and insert any appropriate assertions regarding
+  // sign/zero extension.
+  switch (VA.getLocInfo()) {
+  default:
+    llvm_unreachable("Unknown loc info!");
+  case CCValAssign::Full:
+    break;
+  case CCValAssign::AExtUpper:
+  case CCValAssign::AExt:
+    Val = DAG.getNode(ISD::TRUNCATE, DL, ValVT, Val);
+    break;
+  case CCValAssign::SExtUpper:
+  case CCValAssign::SExt:
+    Val = DAG.getNode(ISD::AssertSext, DL, LocVT, Val, DAG.getValueType(ValVT));
+    Val = DAG.getNode(ISD::TRUNCATE, DL, ValVT, Val);
+    break;
+  case CCValAssign::ZExtUpper:
+  case CCValAssign::ZExt:
+    Val = DAG.getNode(ISD::AssertZext, DL, LocVT, Val, DAG.getValueType(ValVT));
+    Val = DAG.getNode(ISD::TRUNCATE, DL, ValVT, Val);
+    break;
+  case CCValAssign::BCvt:
+    Val = DAG.getNode(ISD::BITCAST, DL, ValVT, Val);
+    break;
+  }
+
+  return Val;
+}
+
+//===----------------------------------------------------------------------===//
+//             Formal Arguments Calling Convention Implementation
+//===----------------------------------------------------------------------===//
+/// LowerFormalArguments - transform physical registers into virtual registers
+/// and generate load operations for arguments places on the stack.
+SDValue LoongArchTargetLowering::LowerFormalArguments(
+    SDValue Chain, CallingConv::ID CallConv, bool IsVarArg,
+    const SmallVectorImpl<ISD::InputArg> &Ins, const SDLoc &DL,
+    SelectionDAG &DAG, SmallVectorImpl<SDValue> &InVals) const {
+  MachineFunction &MF = DAG.getMachineFunction();
+  MachineFrameInfo &MFI = MF.getFrameInfo();
+  LoongArchFunctionInfo *LoongArchFI = MF.getInfo<LoongArchFunctionInfo>();
+
+  LoongArchFI->setVarArgsFrameIndex(0);
+
+  // Used with vargs to acumulate store chains.
+  std::vector<SDValue> OutChains;
+
+  // Assign locations to all of the incoming arguments.
+  SmallVector<CCValAssign, 16> ArgLocs;
+  LoongArchCCState CCInfo(CallConv, IsVarArg, DAG.getMachineFunction(), ArgLocs,
+                     *DAG.getContext());
+  CCInfo.AllocateStack(ABI.GetCalleeAllocdArgSizeInBytes(CallConv), Align(1));
+  const Function &Func = DAG.getMachineFunction().getFunction();
+  Function::const_arg_iterator FuncArg = Func.arg_begin();
+
+  CCInfo.AnalyzeFormalArguments(Ins, CC_LoongArch_FixedArg);
+  LoongArchFI->setFormalArgInfo(CCInfo.getNextStackOffset(),
+                           CCInfo.getInRegsParamsCount() > 0);
+
+  unsigned CurArgIdx = 0;
+  CCInfo.rewindByValRegsInfo();
+
+  for (unsigned i = 0, e = ArgLocs.size(); i != e; ++i) {
+    CCValAssign &VA = ArgLocs[i];
+    if (Ins[i].isOrigArg()) {
+      std::advance(FuncArg, Ins[i].getOrigArgIndex() - CurArgIdx);
+      CurArgIdx = Ins[i].getOrigArgIndex();
+    }
+    EVT ValVT = VA.getValVT();
+    ISD::ArgFlagsTy Flags = Ins[i].Flags;
+    bool IsRegLoc = VA.isRegLoc();
+
+    if (Flags.isByVal()) {
+      assert(Ins[i].isOrigArg() && "Byval arguments cannot be implicit");
+      unsigned FirstByValReg, LastByValReg;
+      unsigned ByValIdx = CCInfo.getInRegsParamsProcessed();
+      CCInfo.getInRegsParamInfo(ByValIdx, FirstByValReg, LastByValReg);
+
+      assert(Flags.getByValSize() &&
+             "ByVal args of size 0 should have been ignored by front-end.");
+      assert(ByValIdx < CCInfo.getInRegsParamsCount());
+      copyByValRegs(Chain, DL, OutChains, DAG, Flags, InVals, &*FuncArg,
+                    FirstByValReg, LastByValReg, VA, CCInfo);
+      CCInfo.nextInRegsParam();
+      continue;
+    }
+
+    // Arguments stored on registers
+    if (IsRegLoc) {
+      MVT RegVT = VA.getLocVT();
+      unsigned ArgReg = VA.getLocReg();
+      const TargetRegisterClass *RC = getRegClassFor(RegVT);
+
+      // Transform the arguments stored on
+      // physical registers into virtual ones
+      unsigned Reg = addLiveIn(DAG.getMachineFunction(), ArgReg, RC);
+      SDValue ArgValue = DAG.getCopyFromReg(Chain, DL, Reg, RegVT);
+
+      ArgValue = UnpackFromArgumentSlot(ArgValue, VA, Ins[i].ArgVT, DL, DAG);
+
+      // Handle floating point arguments passed in integer registers and
+      // long double arguments passed in floating point registers.
+      if ((RegVT == MVT::i32 && ValVT == MVT::f32) ||
+          (RegVT == MVT::i64 && ValVT == MVT::f64) ||
+          (RegVT == MVT::f64 && ValVT == MVT::i64))
+        ArgValue = DAG.getNode(ISD::BITCAST, DL, ValVT, ArgValue);
+      else if (ABI.IsLP32() && RegVT == MVT::i32 &&
+               ValVT == MVT::f64) {
+        // TODO: lp32
+      }
+
+      InVals.push_back(ArgValue);
+    } else { // VA.isRegLoc()
+      MVT LocVT = VA.getLocVT();
+
+      if (ABI.IsLP32()) {
+        // We ought to be able to use LocVT directly but LP32 sets it to i32
+        // when allocating floating point values to integer registers.
+        // This shouldn't influence how we load the value into registers unless
+        // we are targeting softfloat.
+        if (VA.getValVT().isFloatingPoint() && !Subtarget.useSoftFloat())
+          LocVT = VA.getValVT();
+      }
+
+      // sanity check
+      assert(VA.isMemLoc());
+
+      // The stack pointer offset is relative to the caller stack frame.
+      int FI = MFI.CreateFixedObject(LocVT.getSizeInBits() / 8,
+                                     VA.getLocMemOffset(), true);
+
+      // Create load nodes to retrieve arguments from the stack
+      SDValue FIN = DAG.getFrameIndex(FI, getPointerTy(DAG.getDataLayout()));
+      SDValue ArgValue = DAG.getLoad(
+          LocVT, DL, Chain, FIN,
+          MachinePointerInfo::getFixedStack(DAG.getMachineFunction(), FI));
+      OutChains.push_back(ArgValue.getValue(1));
+
+      ArgValue = UnpackFromArgumentSlot(ArgValue, VA, Ins[i].ArgVT, DL, DAG);
+
+      InVals.push_back(ArgValue);
+    }
+  }
+
+  for (unsigned i = 0, e = ArgLocs.size(); i != e; ++i) {
+    // The loongarch ABIs for returning structs by value requires that we copy
+    // the sret argument into $v0 for the return. Save the argument into
+    // a virtual register so that we can access it from the return points.
+    if (Ins[i].Flags.isSRet()) {
+      unsigned Reg = LoongArchFI->getSRetReturnReg();
+      if (!Reg) {
+        Reg = MF.getRegInfo().createVirtualRegister(
+            getRegClassFor(ABI.IsLP64D() ? MVT::i64 : MVT::i32));
+        LoongArchFI->setSRetReturnReg(Reg);
+      }
+      SDValue Copy = DAG.getCopyToReg(DAG.getEntryNode(), DL, Reg, InVals[i]);
+      Chain = DAG.getNode(ISD::TokenFactor, DL, MVT::Other, Copy, Chain);
+      break;
+    }
+  }
+
+  if (IsVarArg)
+    writeVarArgRegs(OutChains, Chain, DL, DAG, CCInfo);
+
+  // All stores are grouped in one node to allow the matching between
+  // the size of Ins and InVals. This only happens when on varg functions
+  if (!OutChains.empty()) {
+    OutChains.push_back(Chain);
+    Chain = DAG.getNode(ISD::TokenFactor, DL, MVT::Other, OutChains);
+  }
+
+  return Chain;
+}
+
+//===----------------------------------------------------------------------===//
+//               Return Value Calling Convention Implementation
+//===----------------------------------------------------------------------===//
+
+bool
+LoongArchTargetLowering::CanLowerReturn(CallingConv::ID CallConv,
+                                   MachineFunction &MF, bool IsVarArg,
+                                   const SmallVectorImpl<ISD::OutputArg> &Outs,
+                                   LLVMContext &Context) const {
+  SmallVector<CCValAssign, 16> RVLocs;
+  LoongArchCCState CCInfo(CallConv, IsVarArg, MF, RVLocs, Context);
+  return CCInfo.CheckReturn(Outs, RetCC_LoongArch);
+}
+
+bool
+LoongArchTargetLowering::shouldSignExtendTypeInLibCall(EVT Type, bool IsSigned) const {
+  if ((ABI.IsLPX32() || ABI.IsLP64D()) && Type == MVT::i32)
+      return true;
+
+  return IsSigned;
+}
+
+SDValue
+LoongArchTargetLowering::LowerReturn(SDValue Chain, CallingConv::ID CallConv,
+                                bool IsVarArg,
+                                const SmallVectorImpl<ISD::OutputArg> &Outs,
+                                const SmallVectorImpl<SDValue> &OutVals,
+                                const SDLoc &DL, SelectionDAG &DAG) const {
+  // CCValAssign - represent the assignment of
+  // the return value to a location
+  SmallVector<CCValAssign, 16> RVLocs;
+  MachineFunction &MF = DAG.getMachineFunction();
+
+  // CCState - Info about the registers and stack slot.
+  LoongArchCCState CCInfo(CallConv, IsVarArg, MF, RVLocs, *DAG.getContext());
+
+  // Analyze return values.
+  CCInfo.AnalyzeReturn(Outs, RetCC_LoongArch);
+
+  SDValue Flag;
+  SmallVector<SDValue, 4> RetOps(1, Chain);
+
+  // Copy the result values into the output registers.
+  for (unsigned i = 0; i != RVLocs.size(); ++i) {
+    SDValue Val = OutVals[i];
+    CCValAssign &VA = RVLocs[i];
+    assert(VA.isRegLoc() && "Can only return in registers!");
+    bool UseUpperBits = false;
+
+    switch (VA.getLocInfo()) {
+    default:
+      llvm_unreachable("Unknown loc info!");
+    case CCValAssign::Full:
+      break;
+    case CCValAssign::BCvt:
+      Val = DAG.getNode(ISD::BITCAST, DL, VA.getLocVT(), Val);
+      break;
+    case CCValAssign::AExtUpper:
+      UseUpperBits = true;
+      LLVM_FALLTHROUGH;
+    case CCValAssign::AExt:
+      Val = DAG.getNode(ISD::ANY_EXTEND, DL, VA.getLocVT(), Val);
+      break;
+    case CCValAssign::ZExtUpper:
+      UseUpperBits = true;
+      LLVM_FALLTHROUGH;
+    case CCValAssign::ZExt:
+      Val = DAG.getNode(ISD::ZERO_EXTEND, DL, VA.getLocVT(), Val);
+      break;
+    case CCValAssign::SExtUpper:
+      UseUpperBits = true;
+      LLVM_FALLTHROUGH;
+    case CCValAssign::SExt:
+      Val = DAG.getNode(ISD::SIGN_EXTEND, DL, VA.getLocVT(), Val);
+      break;
+    }
+
+    if (UseUpperBits) {
+      unsigned ValSizeInBits = Outs[i].ArgVT.getSizeInBits();
+      unsigned LocSizeInBits = VA.getLocVT().getSizeInBits();
+      Val = DAG.getNode(
+          ISD::SHL, DL, VA.getLocVT(), Val,
+          DAG.getConstant(LocSizeInBits - ValSizeInBits, DL, VA.getLocVT()));
+    }
+
+    Chain = DAG.getCopyToReg(Chain, DL, VA.getLocReg(), Val, Flag);
+
+    // Guarantee that all emitted copies are stuck together with flags.
+    Flag = Chain.getValue(1);
+    RetOps.push_back(DAG.getRegister(VA.getLocReg(), VA.getLocVT()));
+  }
+
+  // The loongarch ABIs for returning structs by value requires that we copy
+  // the sret argument into $v0 for the return. We saved the argument into
+  // a virtual register in the entry block, so now we copy the value out
+  // and into $v0.
+  if (MF.getFunction().hasStructRetAttr()) {
+    LoongArchFunctionInfo *LoongArchFI = MF.getInfo<LoongArchFunctionInfo>();
+    unsigned Reg = LoongArchFI->getSRetReturnReg();
+
+    if (!Reg)
+      llvm_unreachable("sret virtual register not created in the entry block");
+    SDValue Val =
+        DAG.getCopyFromReg(Chain, DL, Reg, getPointerTy(DAG.getDataLayout()));
+    unsigned A0 = ABI.IsLP64D() ? LoongArch::A0_64 : LoongArch::A0;
+
+    Chain = DAG.getCopyToReg(Chain, DL, A0, Val, Flag);
+    Flag = Chain.getValue(1);
+    RetOps.push_back(DAG.getRegister(A0, getPointerTy(DAG.getDataLayout())));
+  }
+
+  RetOps[0] = Chain;  // Update chain.
+
+  // Add the flag if we have it.
+  if (Flag.getNode())
+    RetOps.push_back(Flag);
+
+  // Standard return on LoongArch is a "jr $ra"
+  return DAG.getNode(LoongArchISD::Ret, DL, MVT::Other, RetOps);
+}
+
+//===----------------------------------------------------------------------===//
+//                           LoongArch Inline Assembly Support
+//===----------------------------------------------------------------------===//
+
+/// getConstraintType - Given a constraint letter, return the type of
+/// constraint it is for this target.
+LoongArchTargetLowering::ConstraintType
+LoongArchTargetLowering::getConstraintType(StringRef Constraint) const {
+  // LoongArch specific constraints
+  // GCC config/loongarch/constraints.md
+  //
+  // 'f': Floating Point register
+  // 'G': Floating-point 0
+  // 'l': Signed 16-bit constant
+  // 'R': Memory address that can be used in a non-macro load or store
+  // "ZC" Memory address with 16-bit and 4 bytes aligned offset
+  // "ZB" Memory address with 0 offset
+
+  if (Constraint.size() == 1) {
+    switch (Constraint[0]) {
+      default : break;
+      case 'f':
+        return C_RegisterClass;
+      case 'l':
+      case 'G':
+        return C_Other;
+      case 'R':
+        return C_Memory;
+    }
+  }
+
+  if (Constraint == "ZC" || Constraint == "ZB")
+    return C_Memory;
+
+  return TargetLowering::getConstraintType(Constraint);
+}
+
+/// Examine constraint type and operand type and determine a weight value.
+/// This object must already have been set up with the operand type
+/// and the current alternative constraint selected.
+TargetLowering::ConstraintWeight
+LoongArchTargetLowering::getSingleConstraintMatchWeight(
+    AsmOperandInfo &info, const char *constraint) const {
+  ConstraintWeight weight = CW_Invalid;
+  Value *CallOperandVal = info.CallOperandVal;
+    // If we don't have a value, we can't do a match,
+    // but allow it at the lowest weight.
+  if (!CallOperandVal)
+    return CW_Default;
+  Type *type = CallOperandVal->getType();
+  // Look at the constraint type.
+  switch (*constraint) {
+  default:
+    weight = TargetLowering::getSingleConstraintMatchWeight(info, constraint);
+    break;
+  case 'f': // FPU
+    if (type->isFloatTy())
+      weight = CW_Register;
+    break;
+  case 'l': // signed 16 bit immediate
+  case 'I': // signed 12 bit immediate
+  case 'J': // integer zero
+  case 'G': // floating-point zero
+  case 'K': // unsigned 12 bit immediate
+    if (isa<ConstantInt>(CallOperandVal))
+      weight = CW_Constant;
+    break;
+  case 'm':
+  case 'R':
+    weight = CW_Memory;
+    break;
+  }
+  return weight;
+}
+
+/// This is a helper function to parse a physical register string and split it
+/// into non-numeric and numeric parts (Prefix and Reg). The first boolean flag
+/// that is returned indicates whether parsing was successful. The second flag
+/// is true if the numeric part exists.
+static std::pair<bool, bool> parsePhysicalReg(StringRef C, StringRef &Prefix,
+                                              unsigned long long &Reg) {
+  if (C.empty() || C.front() != '{' || C.back() != '}')
+    return std::make_pair(false, false);
+
+  // Search for the first numeric character.
+  StringRef::const_iterator I, B = C.begin() + 1, E = C.end() - 1;
+  I = std::find_if(B, E, isdigit);
+
+  Prefix = StringRef(B, I - B);
+
+  // The second flag is set to false if no numeric characters were found.
+  if (I == E)
+    return std::make_pair(true, false);
+
+  // Parse the numeric characters.
+  return std::make_pair(!getAsUnsignedInteger(StringRef(I, E - I), 10, Reg),
+                        true);
+}
+
+EVT LoongArchTargetLowering::getTypeForExtReturn(LLVMContext &Context, EVT VT,
+                                            ISD::NodeType) const {
+  bool Cond = !Subtarget.isABI_LP32() && VT.getSizeInBits() == 32;
+  EVT MinVT = getRegisterType(Context, Cond ? MVT::i64 : MVT::i32);
+  return VT.bitsLT(MinVT) ? MinVT : VT;
+}
+
+std::pair<unsigned, const TargetRegisterClass *> LoongArchTargetLowering::
+parseRegForInlineAsmConstraint(StringRef C, MVT VT) const {
+  const TargetRegisterInfo *TRI =
+      Subtarget.getRegisterInfo();
+  const TargetRegisterClass *RC;
+  StringRef Prefix;
+  unsigned long long Reg;
+
+  std::pair<bool, bool> R = parsePhysicalReg(C, Prefix, Reg);
+
+  if (!R.first)
+    return std::make_pair(0U, nullptr);
+
+  if (!R.second)
+    return std::make_pair(0U, nullptr);
+
+  if (Prefix == "$f") { // Parse $f0-$f31.
+    // If the size of FP registers is 64-bit or Reg is an even number, select
+    // the 64-bit register class. Otherwise, select the 32-bit register class.
+    if (VT == MVT::Other)
+      VT = (Subtarget.isFP64bit() || !(Reg % 2)) ? MVT::f64 : MVT::f32;
+
+    RC = getRegClassFor(VT);
+  }
+  else if (Prefix == "$vr") { // Parse $vr0-$vr31.
+    RC = getRegClassFor((VT == MVT::Other) ? MVT::v16i8 : VT);
+  }
+  else if (Prefix == "$xr") { // Parse $xr0-$xr31.
+    RC = getRegClassFor((VT == MVT::Other) ? MVT::v16i8 : VT);
+  }
+  else if (Prefix == "$fcc") // Parse $fcc0-$fcc7.
+    RC = TRI->getRegClass(LoongArch::FCFRRegClassID);
+  else { // Parse $r0-$r31.
+    assert(Prefix == "$r");
+    RC = getRegClassFor((VT == MVT::Other) ? MVT::i32 : VT);
+  }
+
+  assert(Reg < RC->getNumRegs());
+
+  if (RC == &LoongArch::GPR64RegClass || RC == &LoongArch::GPR32RegClass) {
+    // Sync with the GPR32/GPR64 RegisterClass in LoongArchRegisterInfo.td
+    // that just like LoongArchAsmParser.cpp
+    switch (Reg) {
+      case 0: return std::make_pair(*(RC->begin() + 0), RC); // r0
+      case 1: return std::make_pair(*(RC->begin() + 27), RC); // r1
+      case 2: return std::make_pair(*(RC->begin() + 28), RC); // r2
+      case 3: return std::make_pair(*(RC->begin() + 29), RC); // r3
+      case 4: return std::make_pair(*(RC->begin() + 1), RC); // r4
+      case 5: return std::make_pair(*(RC->begin() + 2), RC); // r5
+      case 6: return std::make_pair(*(RC->begin() + 3), RC); // r6
+      case 7: return std::make_pair(*(RC->begin() + 4), RC); // r7
+      case 8: return std::make_pair(*(RC->begin() + 5), RC); // r8
+      case 9: return std::make_pair(*(RC->begin() + 6), RC); // r9
+      case 10: return std::make_pair(*(RC->begin() + 7), RC); // r10
+      case 11: return std::make_pair(*(RC->begin() + 8), RC); // r11
+      case 12: return std::make_pair(*(RC->begin() + 9), RC); // r12
+      case 13: return std::make_pair(*(RC->begin() + 10), RC); // r13
+      case 14: return std::make_pair(*(RC->begin() + 11), RC); // r14
+      case 15: return std::make_pair(*(RC->begin() + 12), RC); // r15
+      case 16: return std::make_pair(*(RC->begin() + 13), RC); // r16
+      case 17: return std::make_pair(*(RC->begin() + 14), RC); // r17
+      case 18: return std::make_pair(*(RC->begin() + 15), RC); // r18
+      case 19: return std::make_pair(*(RC->begin() + 16), RC); // r19
+      case 20: return std::make_pair(*(RC->begin() + 17), RC); // r20
+      case 21: return std::make_pair(*(RC->begin() + 30), RC); // r21
+      case 22: return std::make_pair(*(RC->begin() + 31), RC); // r22
+      case 23: return std::make_pair(*(RC->begin() + 18), RC); // r23
+      case 24: return std::make_pair(*(RC->begin() + 19), RC); // r24
+      case 25: return std::make_pair(*(RC->begin() + 20), RC); // r25
+      case 26: return std::make_pair(*(RC->begin() + 21), RC); // r26
+      case 27: return std::make_pair(*(RC->begin() + 22), RC); // r27
+      case 28: return std::make_pair(*(RC->begin() + 23), RC); // r28
+      case 29: return std::make_pair(*(RC->begin() + 24), RC); // r29
+      case 30: return std::make_pair(*(RC->begin() + 25), RC); // r30
+      case 31: return std::make_pair(*(RC->begin() + 26), RC); // r31
+    }
+  }
+  return std::make_pair(*(RC->begin() + Reg), RC);
+}
+
+/// Given a register class constraint, like 'r', if this corresponds directly
+/// to an LLVM register class, return a register of 0 and the register class
+/// pointer.
+std::pair<unsigned, const TargetRegisterClass *>
+LoongArchTargetLowering::getRegForInlineAsmConstraint(const TargetRegisterInfo *TRI,
+                                                 StringRef Constraint,
+                                                 MVT VT) const {
+    if (Constraint.size() == 1) {
+    switch (Constraint[0]) {
+    case 'r':
+      if (VT == MVT::i32 || VT == MVT::i16 || VT == MVT::i8) {
+        return std::make_pair(0U, &LoongArch::GPR32RegClass);
+      }
+      if (VT == MVT::i64 && !Subtarget.is64Bit())
+        return std::make_pair(0U, &LoongArch::GPR32RegClass);
+      if (VT == MVT::i64 && Subtarget.is64Bit())
+        return std::make_pair(0U, &LoongArch::GPR64RegClass);
+      // This will generate an error message
+      return std::make_pair(0U, nullptr);
+    case 'f': // FPU
+      if (VT == MVT::f32)
+        return std::make_pair(0U, &LoongArch::FGR32RegClass);
+      else if (VT == MVT::f64)
+        return std::make_pair(0U, &LoongArch::FGR64RegClass);
+      break;
+    }
+  }
+
+  std::pair<unsigned, const TargetRegisterClass *> R;
+  R = parseRegForInlineAsmConstraint(Constraint, VT);
+
+  if (R.second)
+    return R;
+
+  return TargetLowering::getRegForInlineAsmConstraint(TRI, Constraint, VT);
+}
+
+/// LowerAsmOperandForConstraint - Lower the specified operand into the Ops
+/// vector.  If it is invalid, don't add anything to Ops.
+void LoongArchTargetLowering::LowerAsmOperandForConstraint(SDValue Op,
+                                                     std::string &Constraint,
+                                                     std::vector<SDValue>&Ops,
+                                                     SelectionDAG &DAG) const {
+  SDLoc DL(Op);
+  SDValue Result;
+
+  // Only support length 1 constraints for now.
+  if (Constraint.length() > 1) return;
+
+  char ConstraintLetter = Constraint[0];
+  switch (ConstraintLetter) {
+  default: break; // This will fall through to the generic implementation
+  case 'l': // Signed 16 bit constant
+    // If this fails, the parent routine will give an error
+    if (ConstantSDNode *C = dyn_cast<ConstantSDNode>(Op)) {
+      EVT Type = Op.getValueType();
+      int64_t Val = C->getSExtValue();
+      if (isInt<16>(Val)) {
+        Result = DAG.getTargetConstant(Val, DL, Type);
+        break;
+      }
+    }
+    return;
+  case 'I': // Signed 12 bit constant
+    // If this fails, the parent routine will give an error
+    if (ConstantSDNode *C = dyn_cast<ConstantSDNode>(Op)) {
+      EVT Type = Op.getValueType();
+      int64_t Val = C->getSExtValue();
+      if (isInt<12>(Val)) {
+        Result = DAG.getTargetConstant(Val, DL, Type);
+        break;
+      }
+    }
+    return;
+  case 'J': // integer zero
+    if (ConstantSDNode *C = dyn_cast<ConstantSDNode>(Op)) {
+      EVT Type = Op.getValueType();
+      int64_t Val = C->getZExtValue();
+      if (Val == 0) {
+        Result = DAG.getTargetConstant(0, DL, Type);
+        break;
+      }
+    }
+    return;
+  case 'G': // floating-point zero
+    if (ConstantFPSDNode *C = dyn_cast<ConstantFPSDNode>(Op)) {
+      if (C->isZero()) {
+        EVT Type = Op.getValueType();
+        Result = DAG.getTargetConstantFP(0, DL, Type);
+        break;
+      }
+    }
+    return;
+  case 'K': // unsigned 12 bit immediate
+    if (ConstantSDNode *C = dyn_cast<ConstantSDNode>(Op)) {
+      EVT Type = Op.getValueType();
+      uint64_t Val = (uint64_t)C->getZExtValue();
+      if (isUInt<12>(Val)) {
+        Result = DAG.getTargetConstant(Val, DL, Type);
+        break;
+      }
+    }
+    return;
+  }
+
+  if (Result.getNode()) {
+    Ops.push_back(Result);
+    return;
+  }
+
+  TargetLowering::LowerAsmOperandForConstraint(Op, Constraint, Ops, DAG);
+}
+
+bool LoongArchTargetLowering::isLegalAddressingMode(const DataLayout &DL,
+                                               const AddrMode &AM, Type *Ty,
+                                               unsigned AS, Instruction *I) const {
+  // No global is ever allowed as a base.
+  if (AM.BaseGV)
+    return false;
+
+  switch (AM.Scale) {
+  case 0: // "r+i" or just "i", depending on HasBaseReg.
+    break;
+  case 1:
+    if (!AM.HasBaseReg) // allow "r+i".
+      break;
+    return false; // disallow "r+r" or "r+r+i".
+  default:
+    return false;
+  }
+
+  return true;
+}
+
+bool
+LoongArchTargetLowering::isOffsetFoldingLegal(const GlobalAddressSDNode *GA) const {
+  // The LoongArch target isn't yet aware of offsets.
+  return false;
+}
+
+EVT LoongArchTargetLowering::getOptimalMemOpType(
+    const MemOp &Op, const AttributeList &FuncAttributes) const {
+  if (Subtarget.is64Bit())
+    return MVT::i64;
+
+  return MVT::i32;
+}
+
+/// isFPImmLegal - Returns true if the target can instruction select the
+/// specified FP immediate natively. If false, the legalizer will
+/// materialize the FP immediate as a load from a constant pool.
+bool LoongArchTargetLowering::isFPImmLegal(const APFloat &Imm, EVT VT,
+                                           bool ForCodeSize) const {
+  if (VT != MVT::f32 && VT != MVT::f64)
+    return false;
+  if (Imm.isNegZero())
+    return false;
+  return (Imm.isZero() || Imm.isExactlyValue(+1.0));
+}
+
+bool LoongArchTargetLowering::useSoftFloat() const {
+  return Subtarget.useSoftFloat();
+}
+
+void LoongArchTargetLowering::copyByValRegs(
+    SDValue Chain, const SDLoc &DL, std::vector<SDValue> &OutChains,
+    SelectionDAG &DAG, const ISD::ArgFlagsTy &Flags,
+    SmallVectorImpl<SDValue> &InVals, const Argument *FuncArg,
+    unsigned FirstReg, unsigned LastReg, const CCValAssign &VA,
+    LoongArchCCState &State) const {
+  MachineFunction &MF = DAG.getMachineFunction();
+  MachineFrameInfo &MFI = MF.getFrameInfo();
+  unsigned GPRSizeInBytes = Subtarget.getGPRSizeInBytes();
+  unsigned NumRegs = LastReg - FirstReg;
+  unsigned RegAreaSize = NumRegs * GPRSizeInBytes;
+  unsigned FrameObjSize = std::max(Flags.getByValSize(), RegAreaSize);
+  int FrameObjOffset;
+  ArrayRef<MCPhysReg> ByValArgRegs = ABI.GetByValArgRegs();
+
+  if (RegAreaSize)
+    FrameObjOffset =
+        (int)ABI.GetCalleeAllocdArgSizeInBytes(State.getCallingConv()) -
+        (int)((ByValArgRegs.size() - FirstReg) * GPRSizeInBytes);
+  else
+    FrameObjOffset = VA.getLocMemOffset();
+
+  // Create frame object.
+  EVT PtrTy = getPointerTy(DAG.getDataLayout());
+  // Make the fixed object stored to mutable so that the load instructions
+  // referencing it have their memory dependencies added.
+  // Set the frame object as isAliased which clears the underlying objects
+  // vector in ScheduleDAGInstrs::buildSchedGraph() resulting in addition of all
+  // stores as dependencies for loads referencing this fixed object.
+  int FI = MFI.CreateFixedObject(FrameObjSize, FrameObjOffset, false, true);
+  SDValue FIN = DAG.getFrameIndex(FI, PtrTy);
+  InVals.push_back(FIN);
+
+  if (!NumRegs)
+    return;
+
+  // Copy arg registers.
+  MVT RegTy = MVT::getIntegerVT(GPRSizeInBytes * 8);
+  const TargetRegisterClass *RC = getRegClassFor(RegTy);
+
+  for (unsigned I = 0; I < NumRegs; ++I) {
+    unsigned ArgReg = ByValArgRegs[FirstReg + I];
+    unsigned VReg = addLiveIn(MF, ArgReg, RC);
+    unsigned Offset = I * GPRSizeInBytes;
+    SDValue StorePtr = DAG.getNode(ISD::ADD, DL, PtrTy, FIN,
+                                   DAG.getConstant(Offset, DL, PtrTy));
+    SDValue Store = DAG.getStore(Chain, DL, DAG.getRegister(VReg, RegTy),
+                                 StorePtr, MachinePointerInfo(FuncArg, Offset));
+    OutChains.push_back(Store);
+  }
+}
+
+// Copy byVal arg to registers and stack.
+void LoongArchTargetLowering::passByValArg(
+    SDValue Chain, const SDLoc &DL,
+    std::deque<std::pair<unsigned, SDValue>> &RegsToPass,
+    SmallVectorImpl<SDValue> &MemOpChains, SDValue StackPtr,
+    MachineFrameInfo &MFI, SelectionDAG &DAG, SDValue Arg, unsigned FirstReg,
+    unsigned LastReg, const ISD::ArgFlagsTy &Flags,
+    const CCValAssign &VA) const {
+  unsigned ByValSizeInBytes = Flags.getByValSize();
+  unsigned OffsetInBytes = 0; // From beginning of struct
+  unsigned RegSizeInBytes = Subtarget.getGPRSizeInBytes();
+  Align Alignment =
+      std::min(Flags.getNonZeroByValAlign(), Align(RegSizeInBytes));
+  EVT PtrTy = getPointerTy(DAG.getDataLayout()),
+      RegTy = MVT::getIntegerVT(RegSizeInBytes * 8);
+  unsigned NumRegs = LastReg - FirstReg;
+
+  if (NumRegs) {
+    ArrayRef<MCPhysReg> ArgRegs = ABI.GetByValArgRegs();
+    bool LeftoverBytes = (NumRegs * RegSizeInBytes > ByValSizeInBytes);
+    unsigned I = 0;
+
+    // Copy words to registers.
+    for (; I < NumRegs - LeftoverBytes; ++I, OffsetInBytes += RegSizeInBytes) {
+      SDValue LoadPtr = DAG.getNode(ISD::ADD, DL, PtrTy, Arg,
+                                    DAG.getConstant(OffsetInBytes, DL, PtrTy));
+      SDValue LoadVal = DAG.getLoad(RegTy, DL, Chain, LoadPtr,
+                                    MachinePointerInfo(), Alignment);
+      MemOpChains.push_back(LoadVal.getValue(1));
+      unsigned ArgReg = ArgRegs[FirstReg + I];
+      RegsToPass.push_back(std::make_pair(ArgReg, LoadVal));
+    }
+
+    // Return if the struct has been fully copied.
+    if (ByValSizeInBytes == OffsetInBytes)
+      return;
+
+    // Copy the remainder of the byval argument with sub-word loads and shifts.
+    if (LeftoverBytes) {
+      SDValue Val;
+
+      for (unsigned LoadSizeInBytes = RegSizeInBytes / 2, TotalBytesLoaded = 0;
+           OffsetInBytes < ByValSizeInBytes; LoadSizeInBytes /= 2) {
+        unsigned RemainingSizeInBytes = ByValSizeInBytes - OffsetInBytes;
+
+        if (RemainingSizeInBytes < LoadSizeInBytes)
+          continue;
+
+        // Load subword.
+        SDValue LoadPtr = DAG.getNode(ISD::ADD, DL, PtrTy, Arg,
+                                      DAG.getConstant(OffsetInBytes, DL,
+                                                      PtrTy));
+        SDValue LoadVal = DAG.getExtLoad(
+            ISD::ZEXTLOAD, DL, RegTy, Chain, LoadPtr, MachinePointerInfo(),
+            MVT::getIntegerVT(LoadSizeInBytes * 8), Alignment);
+        MemOpChains.push_back(LoadVal.getValue(1));
+
+        // Shift the loaded value.
+        unsigned Shamt;
+
+        Shamt = TotalBytesLoaded * 8;
+
+        SDValue Shift = DAG.getNode(ISD::SHL, DL, RegTy, LoadVal,
+                                    DAG.getConstant(Shamt, DL, MVT::i32));
+
+        if (Val.getNode())
+          Val = DAG.getNode(ISD::OR, DL, RegTy, Val, Shift);
+        else
+          Val = Shift;
+
+        OffsetInBytes += LoadSizeInBytes;
+        TotalBytesLoaded += LoadSizeInBytes;
+        Alignment = std::min(Alignment, Align(LoadSizeInBytes));
+      }
+
+      unsigned ArgReg = ArgRegs[FirstReg + I];
+      RegsToPass.push_back(std::make_pair(ArgReg, Val));
+      return;
+    }
+  }
+
+  // Copy remainder of byval arg to it with memcpy.
+  unsigned MemCpySize = ByValSizeInBytes - OffsetInBytes;
+  SDValue Src = DAG.getNode(ISD::ADD, DL, PtrTy, Arg,
+                            DAG.getConstant(OffsetInBytes, DL, PtrTy));
+  SDValue Dst = DAG.getNode(ISD::ADD, DL, PtrTy, StackPtr,
+                            DAG.getIntPtrConstant(VA.getLocMemOffset(), DL));
+  Chain = DAG.getMemcpy(
+      Chain, DL, Dst, Src, DAG.getConstant(MemCpySize, DL, PtrTy),
+      Align(Alignment), /*isVolatile=*/false, /*AlwaysInline=*/false,
+      /*isTailCall=*/false, MachinePointerInfo(), MachinePointerInfo());
+  MemOpChains.push_back(Chain);
+}
+
+void LoongArchTargetLowering::writeVarArgRegs(std::vector<SDValue> &OutChains,
+                                         SDValue Chain, const SDLoc &DL,
+                                         SelectionDAG &DAG,
+                                         CCState &State) const {
+  ArrayRef<MCPhysReg> ArgRegs = ABI.GetVarArgRegs();
+  unsigned Idx = State.getFirstUnallocated(ArgRegs);
+  unsigned RegSizeInBytes = Subtarget.getGPRSizeInBytes();
+  MVT RegTy = MVT::getIntegerVT(RegSizeInBytes * 8);
+  const TargetRegisterClass *RC = getRegClassFor(RegTy);
+  MachineFunction &MF = DAG.getMachineFunction();
+  MachineFrameInfo &MFI = MF.getFrameInfo();
+  LoongArchFunctionInfo *LoongArchFI = MF.getInfo<LoongArchFunctionInfo>();
+
+  // Offset of the first variable argument from stack pointer.
+  int VaArgOffset;
+
+  if (ArgRegs.size() == Idx)
+    VaArgOffset = alignTo(State.getNextStackOffset(), RegSizeInBytes);
+  else {
+    VaArgOffset =
+        (int)ABI.GetCalleeAllocdArgSizeInBytes(State.getCallingConv()) -
+        (int)(RegSizeInBytes * (ArgRegs.size() - Idx));
+  }
+
+  // Record the frame index of the first variable argument
+  // which is a value necessary to VASTART.
+  int FI = MFI.CreateFixedObject(RegSizeInBytes, VaArgOffset, true);
+  LoongArchFI->setVarArgsFrameIndex(FI);
+
+  // Copy the integer registers that have not been used for argument passing
+  // to the argument register save area. For LP32, the save area is allocated
+  // in the caller's stack frame, while for LPX32/LP64D, it is allocated in the
+  // callee's stack frame.
+  for (unsigned I = Idx; I < ArgRegs.size();
+       ++I, VaArgOffset += RegSizeInBytes) {
+    unsigned Reg = addLiveIn(MF, ArgRegs[I], RC);
+    SDValue ArgValue = DAG.getCopyFromReg(Chain, DL, Reg, RegTy);
+    FI = MFI.CreateFixedObject(RegSizeInBytes, VaArgOffset, true);
+    SDValue PtrOff = DAG.getFrameIndex(FI, getPointerTy(DAG.getDataLayout()));
+    SDValue Store =
+        DAG.getStore(Chain, DL, ArgValue, PtrOff, MachinePointerInfo());
+    cast<StoreSDNode>(Store.getNode())->getMemOperand()->setValue(
+        (Value *)nullptr);
+    OutChains.push_back(Store);
+  }
+}
+
+void LoongArchTargetLowering::HandleByVal(CCState *State, unsigned &Size,
+                                          Align Alignment) const {
+  const TargetFrameLowering *TFL = Subtarget.getFrameLowering();
+
+  assert(Size && "Byval argument's size shouldn't be 0.");
+
+  Alignment = std::min(Alignment, TFL->getStackAlign());
+
+  unsigned FirstReg = 0;
+  unsigned NumRegs = 0;
+
+  if (State->getCallingConv() != CallingConv::Fast) {
+    unsigned RegSizeInBytes = Subtarget.getGPRSizeInBytes();
+    ArrayRef<MCPhysReg> IntArgRegs = ABI.GetByValArgRegs();
+    // FIXME: The LP32 case actually describes no shadow registers.
+    const MCPhysReg *ShadowRegs =
+        ABI.IsLP32() ? IntArgRegs.data() : LoongArch64DPRegs;
+
+    // We used to check the size as well but we can't do that anymore since
+    // CCState::HandleByVal() rounds up the size after calling this function.
+    assert(
+        Alignment >= Align(RegSizeInBytes) &&
+        "Byval argument's alignment should be a multiple of RegSizeInBytes.");
+
+    FirstReg = State->getFirstUnallocated(IntArgRegs);
+
+    // If Alignment > RegSizeInBytes, the first arg register must be even.
+    // FIXME: This condition happens to do the right thing but it's not the
+    //        right way to test it. We want to check that the stack frame offset
+    //        of the register is aligned.
+    if ((Alignment > RegSizeInBytes) && (FirstReg % 2)) {
+      State->AllocateReg(IntArgRegs[FirstReg], ShadowRegs[FirstReg]);
+      ++FirstReg;
+      //assert(true && "debug#######################################");
+    }
+
+    // Mark the registers allocated.
+    //Size = alignTo(Size, RegSizeInBytes);
+    //for (unsigned I = FirstReg; Size > 0 && (I < IntArgRegs.size());
+    //     Size -= RegSizeInBytes, ++I, ++NumRegs)
+    //  State->AllocateReg(IntArgRegs[I], ShadowRegs[I]);
+  }
+
+  State->addInRegsParamInfo(FirstReg, FirstReg + NumRegs);
+}
+
+MachineBasicBlock *LoongArchTargetLowering::emitPseudoSELECT(MachineInstr &MI,
+                                                        MachineBasicBlock *BB,
+                                                        bool isFPCmp,
+                                                        unsigned Opc) const {
+  const TargetInstrInfo *TII =
+      Subtarget.getInstrInfo();
+  DebugLoc DL = MI.getDebugLoc();
+
+  // To "insert" a SELECT instruction, we actually have to insert the
+  // diamond control-flow pattern.  The incoming instruction knows the
+  // destination vreg to set, the condition code register to branch on, the
+  // true/false values to select between, and a branch opcode to use.
+  const BasicBlock *LLVM_BB = BB->getBasicBlock();
+  MachineFunction::iterator It = ++BB->getIterator();
+
+  //  thisMBB:
+  //  ...
+  //   TrueVal = ...
+  //   setcc r1, r2, r3
+  //   bNE   r1, r0, copy1MBB
+  //   fallthrough --> copy0MBB
+  MachineBasicBlock *thisMBB  = BB;
+  MachineFunction *F = BB->getParent();
+  MachineBasicBlock *copy0MBB = F->CreateMachineBasicBlock(LLVM_BB);
+  MachineBasicBlock *sinkMBB  = F->CreateMachineBasicBlock(LLVM_BB);
+  F->insert(It, copy0MBB);
+  F->insert(It, sinkMBB);
+
+  // Transfer the remainder of BB and its successor edges to sinkMBB.
+  sinkMBB->splice(sinkMBB->begin(), BB,
+                  std::next(MachineBasicBlock::iterator(MI)), BB->end());
+  sinkMBB->transferSuccessorsAndUpdatePHIs(BB);
+
+  // Next, add the true and fallthrough blocks as its successors.
+  BB->addSuccessor(copy0MBB);
+  BB->addSuccessor(sinkMBB);
+
+  if (isFPCmp) {
+    // bc1[tf] cc, sinkMBB
+    BuildMI(BB, DL, TII->get(Opc))
+        .addReg(MI.getOperand(1).getReg())
+        .addMBB(sinkMBB);
+  } else {
+    BuildMI(BB, DL, TII->get(Opc))
+        .addReg(MI.getOperand(1).getReg())
+        .addReg(LoongArch::ZERO)
+        .addMBB(sinkMBB);
+  }
+
+  //  copy0MBB:
+  //   %FalseValue = ...
+  //   # fallthrough to sinkMBB
+  BB = copy0MBB;
+
+  // Update machine-CFG edges
+  BB->addSuccessor(sinkMBB);
+
+  //  sinkMBB:
+  //   %Result = phi [ %TrueValue, thisMBB ], [ %FalseValue, copy0MBB ]
+  //  ...
+  BB = sinkMBB;
+
+  BuildMI(*BB, BB->begin(), DL, TII->get(LoongArch::PHI), MI.getOperand(0).getReg())
+      .addReg(MI.getOperand(2).getReg())
+      .addMBB(thisMBB)
+      .addReg(MI.getOperand(3).getReg())
+      .addMBB(copy0MBB);
+
+  MI.eraseFromParent(); // The pseudo instruction is gone now.
+
+  return BB;
+}
+
+bool LoongArchTargetLowering::isFMAFasterThanFMulAndFAdd(
+    const MachineFunction &MF, EVT VT) const {
+
+  VT = VT.getScalarType();
+
+  if (!VT.isSimple())
+    return false;
+
+  switch (VT.getSimpleVT().SimpleTy) {
+  case MVT::f32:
+  case MVT::f64:
+    return true;
+  default:
+    break;
+  }
+
+  return false;
+}
diff --git a/llvm/lib/Target/LoongArch/LoongArchISelLowering.h b/llvm/lib/Target/LoongArch/LoongArchISelLowering.h
new file mode 100644
index 000000000000..299a5ea7d2c2
--- /dev/null
+++ b/llvm/lib/Target/LoongArch/LoongArchISelLowering.h
@@ -0,0 +1,433 @@
+//===- LoongArchISelLowering.h - LoongArch DAG Lowering Interface ---------*- C++ -*-===//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+//
+// This file defines the interfaces that LoongArch uses to lower LLVM code into a
+// selection DAG.
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef LLVM_LIB_TARGET_LOONGARCH_LOONGARCHISELLOWERING_H
+#define LLVM_LIB_TARGET_LOONGARCH_LOONGARCHISELLOWERING_H
+
+#include "MCTargetDesc/LoongArchABIInfo.h"
+#include "MCTargetDesc/LoongArchBaseInfo.h"
+#include "MCTargetDesc/LoongArchMCTargetDesc.h"
+#include "LoongArch.h"
+#include "llvm/CodeGen/CallingConvLower.h"
+#include "llvm/CodeGen/ISDOpcodes.h"
+#include "llvm/CodeGen/MachineMemOperand.h"
+#include "llvm/CodeGen/SelectionDAG.h"
+#include "llvm/CodeGen/SelectionDAGNodes.h"
+#include "llvm/CodeGen/TargetLowering.h"
+#include "llvm/CodeGen/ValueTypes.h"
+#include "llvm/IR/CallingConv.h"
+#include "llvm/IR/InlineAsm.h"
+#include "llvm/IR/Type.h"
+#include "llvm/Support/MachineValueType.h"
+#include "llvm/Target/TargetMachine.h"
+#include <algorithm>
+#include <cassert>
+#include <deque>
+#include <string>
+#include <utility>
+#include <vector>
+
+namespace llvm {
+
+class Argument;
+class CCState;
+class CCValAssign;
+class FastISel;
+class FunctionLoweringInfo;
+class MachineBasicBlock;
+class MachineFrameInfo;
+class MachineInstr;
+class LoongArchCCState;
+class LoongArchFunctionInfo;
+class LoongArchSubtarget;
+class LoongArchTargetMachine;
+class SelectionDAG;
+class TargetLibraryInfo;
+class TargetRegisterClass;
+
+  namespace LoongArchISD {
+
+    enum NodeType : unsigned {
+      // Start the numbering from where ISD NodeType finishes.
+      FIRST_NUMBER = ISD::BUILTIN_OP_END,
+
+      // Jump and link (call)
+      JmpLink,
+
+      // Tail call
+      TailCall,
+
+      // global address
+      GlobalAddress,
+
+      // Floating Point Branch Conditional
+      FPBrcond,
+
+      // Floating Point Compare
+      FPCmp,
+
+      // Floating Point Conditional Moves
+      CMovFP_T,
+      CMovFP_F,
+
+      // FP-to-int truncation node.
+      TruncIntFP,
+
+      // Return
+      Ret,
+
+      // error trap Return
+      ERet,
+
+      // Software Exception Return.
+      EH_RETURN,
+
+      DBAR,
+
+      BSTRPICK,
+      BSTRINS,
+    };
+
+  } // ene namespace LoongArchISD
+
+  //===--------------------------------------------------------------------===//
+  // TargetLowering Implementation
+  //===--------------------------------------------------------------------===//
+
+  class LoongArchTargetLowering : public TargetLowering  {
+  public:
+    explicit LoongArchTargetLowering(const LoongArchTargetMachine &TM,
+                                const LoongArchSubtarget &STI);
+
+    bool allowsMisalignedMemoryAccesses(
+        EVT VT, unsigned AS = 0, unsigned Align = 1,
+        MachineMemOperand::Flags Flags = MachineMemOperand::MONone,
+        bool *Fast = nullptr) const override;
+
+    MVT getScalarShiftAmountTy(const DataLayout &, EVT) const override {
+      return MVT::i32;
+    }
+
+    EVT getTypeForExtReturn(LLVMContext &Context, EVT VT,
+                            ISD::NodeType) const override;
+
+    bool isCheapToSpeculateCttz() const override;
+    bool isCheapToSpeculateCtlz() const override;
+
+    /// Return the correct alignment for the current calling convention.
+    Align getABIAlignmentForCallingConv(Type *ArgTy,
+                                        DataLayout DL) const override {
+      const Align ABIAlign = DL.getABITypeAlign(ArgTy);
+      if (ArgTy->isVectorTy())
+        return std::min(ABIAlign, Align(8));
+      return ABIAlign;
+    }
+
+    ISD::NodeType getExtendForAtomicOps() const override {
+      return ISD::SIGN_EXTEND;
+    }
+
+    void LowerOperationWrapper(SDNode *N,
+                               SmallVectorImpl<SDValue> &Results,
+                               SelectionDAG &DAG) const override;
+
+    /// LowerOperation - Provide custom lowering hooks for some operations.
+    SDValue LowerOperation(SDValue Op, SelectionDAG &DAG) const override;
+
+    bool isFMAFasterThanFMulAndFAdd(const MachineFunction &MF,
+                                    EVT VT) const override;
+
+    /// ReplaceNodeResults - Replace the results of node with an illegal result
+    /// type with new values built out of custom code.
+    ///
+    void ReplaceNodeResults(SDNode *N, SmallVectorImpl<SDValue>&Results,
+                            SelectionDAG &DAG) const override;
+
+    /// getTargetNodeName - This method returns the name of a target specific
+    //  DAG node.
+    const char *getTargetNodeName(unsigned Opcode) const override;
+
+    /// getSetCCResultType - get the ISD::SETCC result ValueType
+    EVT getSetCCResultType(const DataLayout &DL, LLVMContext &Context,
+                           EVT VT) const override;
+
+    SDValue PerformDAGCombine(SDNode *N, DAGCombinerInfo &DCI) const override;
+
+    MachineBasicBlock *
+    EmitInstrWithCustomInserter(MachineInstr &MI,
+                                MachineBasicBlock *MBB) const override;
+
+    bool isShuffleMaskLegal(ArrayRef<int> Mask, EVT VT) const override {
+      return false;
+    }
+
+    const TargetRegisterClass *getRepRegClassFor(MVT VT) const override;
+
+    void AdjustInstrPostInstrSelection(MachineInstr &MI,
+                                       SDNode *Node) const override;
+
+    void HandleByVal(CCState *, unsigned &, Align) const override;
+
+    /// If a physical register, this returns the register that receives the
+    /// exception address on entry to an EH pad.
+    Register
+    getExceptionPointerRegister(const Constant *PersonalityFn) const override {
+      return ABI.IsLP64D() ? LoongArch::A0_64 : LoongArch::A0;
+    }
+
+    /// If a physical register, this returns the register that receives the
+    /// exception typeid on entry to a landing pad.
+    Register
+    getExceptionSelectorRegister(const Constant *PersonalityFn) const override {
+      return ABI.IsLP64D() ? LoongArch::A1_64 : LoongArch::A1;
+    }
+
+    /// Returns true if a cast between SrcAS and DestAS is a noop.
+    bool isNoopAddrSpaceCast(unsigned SrcAS, unsigned DestAS) const override {
+      // Mips doesn't have any special address spaces so we just reserve
+      // the first 256 for software use (e.g. OpenCL) and treat casts
+      // between them as noops.
+      return SrcAS < 256 && DestAS < 256;
+    }
+
+    bool isJumpTableRelative() const override {
+      return getTargetMachine().isPositionIndependent();
+    }
+
+   CCAssignFn *CCAssignFnForCall() const;
+
+   CCAssignFn *CCAssignFnForReturn() const;
+
+  private:
+    template <class NodeTy>
+    SDValue getAddr(NodeTy *N, SelectionDAG &DAG, bool IsLocal = true) const;
+
+    /// This function fills Ops, which is the list of operands that will later
+    /// be used when a function call node is created. It also generates
+    /// copyToReg nodes to set up argument registers.
+    void
+    getOpndList(SmallVectorImpl<SDValue> &Ops,
+                std::deque<std::pair<unsigned, SDValue>> &RegsToPass,
+                bool IsPICCall, bool GlobalOrExternal, bool InternalLinkage,
+                bool IsCallReloc, CallLoweringInfo &CLI, SDValue Callee,
+                SDValue Chain) const;
+
+    SDValue lowerLOAD(SDValue Op, SelectionDAG &DAG) const;
+    SDValue lowerSTORE(SDValue Op, SelectionDAG &DAG) const;
+
+    // Subtarget Info
+    const LoongArchSubtarget &Subtarget;
+    // Cache the ABI from the TargetMachine, we use it everywhere.
+    const LoongArchABIInfo &ABI;
+
+    // Create a TargetGlobalAddress node.
+    SDValue getTargetNode(GlobalAddressSDNode *N, EVT Ty, SelectionDAG &DAG,
+                          unsigned Flag) const;
+
+    // Create a TargetExternalSymbol node.
+    SDValue getTargetNode(ExternalSymbolSDNode *N, EVT Ty, SelectionDAG &DAG,
+                          unsigned Flag) const;
+
+    // Create a TargetBlockAddress node.
+    SDValue getTargetNode(BlockAddressSDNode *N, EVT Ty, SelectionDAG &DAG,
+                          unsigned Flag) const;
+
+    // Create a TargetJumpTable node.
+    SDValue getTargetNode(JumpTableSDNode *N, EVT Ty, SelectionDAG &DAG,
+                          unsigned Flag) const;
+
+    // Create a TargetConstantPool node.
+    SDValue getTargetNode(ConstantPoolSDNode *N, EVT Ty, SelectionDAG &DAG,
+                          unsigned Flag) const;
+
+    // Lower Operand helpers
+    SDValue LowerCallResult(SDValue Chain, SDValue InFlag,
+                            CallingConv::ID CallConv, bool isVarArg,
+                            const SmallVectorImpl<ISD::InputArg> &Ins,
+                            const SDLoc &dl, SelectionDAG &DAG,
+                            SmallVectorImpl<SDValue> &InVals,
+                            TargetLowering::CallLoweringInfo &CLI) const;
+
+    // Lower Operand specifics
+    SDValue lowerINTRINSIC_WO_CHAIN(SDValue Op, SelectionDAG &DAG) const;
+    SDValue lowerINTRINSIC_W_CHAIN(SDValue Op, SelectionDAG &DAG) const;
+    SDValue lowerINTRINSIC_VOID(SDValue Op, SelectionDAG &DAG) const;
+    SDValue lowerUINT_TO_FP(SDValue Op, SelectionDAG &DAG) const;
+    SDValue lowerSINT_TO_FP(SDValue Op, SelectionDAG &DAG) const;
+    SDValue lowerFP_TO_UINT(SDValue Op, SelectionDAG &DAG) const;
+    SDValue lowerFP_TO_SINT(SDValue Op, SelectionDAG &DAG) const;
+    SDValue lowerBRCOND(SDValue Op, SelectionDAG &DAG) const;
+    SDValue lowerConstantPool(SDValue Op, SelectionDAG &DAG) const;
+    SDValue lowerGlobalAddress(SDValue Op, SelectionDAG &DAG) const;
+    SDValue lowerBlockAddress(SDValue Op, SelectionDAG &DAG) const;
+    SDValue lowerGlobalTLSAddress(SDValue Op, SelectionDAG &DAG) const;
+    SDValue lowerJumpTable(SDValue Op, SelectionDAG &DAG) const;
+    SDValue lowerSELECT(SDValue Op, SelectionDAG &DAG) const;
+    SDValue lowerSETCC(SDValue Op, SelectionDAG &DAG) const;
+    SDValue lowerVASTART(SDValue Op, SelectionDAG &DAG) const;
+    SDValue lowerVAARG(SDValue Op, SelectionDAG &DAG) const;
+    SDValue lowerFCOPYSIGN(SDValue Op, SelectionDAG &DAG) const;
+    SDValue lowerFABS(SDValue Op, SelectionDAG &DAG) const;
+    SDValue lowerFRAMEADDR(SDValue Op, SelectionDAG &DAG) const;
+    SDValue lowerRETURNADDR(SDValue Op, SelectionDAG &DAG) const;
+    SDValue lowerEH_RETURN(SDValue Op, SelectionDAG &DAG) const;
+    SDValue lowerATOMIC_FENCE(SDValue Op, SelectionDAG& DAG) const;
+    SDValue lowerShiftLeftParts(SDValue Op, SelectionDAG& DAG) const;
+    SDValue lowerShiftRightParts(SDValue Op, SelectionDAG& DAG,
+                                 bool IsSRA) const;
+    SDValue lowerEH_DWARF_CFA(SDValue Op, SelectionDAG &DAG) const;
+
+    /// isEligibleForTailCallOptimization - Check whether the call is eligible
+    /// for tail call optimization.
+    bool
+    isEligibleForTailCallOptimization(const CCState &CCInfo,
+                                      unsigned NextStackOffset,
+                                      const LoongArchFunctionInfo &FI) const;
+
+    /// copyByValArg - Copy argument registers which were used to pass a byval
+    /// argument to the stack. Create a stack frame object for the byval
+    /// argument.
+    void copyByValRegs(SDValue Chain, const SDLoc &DL,
+                       std::vector<SDValue> &OutChains, SelectionDAG &DAG,
+                       const ISD::ArgFlagsTy &Flags,
+                       SmallVectorImpl<SDValue> &InVals,
+                       const Argument *FuncArg, unsigned FirstReg,
+                       unsigned LastReg, const CCValAssign &VA,
+                       LoongArchCCState &State) const;
+
+    /// passByValArg - Pass a byval argument in registers or on stack.
+    void passByValArg(SDValue Chain, const SDLoc &DL,
+                      std::deque<std::pair<unsigned, SDValue>> &RegsToPass,
+                      SmallVectorImpl<SDValue> &MemOpChains, SDValue StackPtr,
+                      MachineFrameInfo &MFI, SelectionDAG &DAG, SDValue Arg,
+                      unsigned FirstReg, unsigned LastReg,
+                      const ISD::ArgFlagsTy &Flags,
+                      const CCValAssign &VA) const;
+
+    /// writeVarArgRegs - Write variable function arguments passed in registers
+    /// to the stack. Also create a stack frame object for the first variable
+    /// argument.
+    void writeVarArgRegs(std::vector<SDValue> &OutChains, SDValue Chain,
+                         const SDLoc &DL, SelectionDAG &DAG,
+                         CCState &State) const;
+
+    SDValue
+    LowerFormalArguments(SDValue Chain, CallingConv::ID CallConv, bool isVarArg,
+                         const SmallVectorImpl<ISD::InputArg> &Ins,
+                         const SDLoc &dl, SelectionDAG &DAG,
+                         SmallVectorImpl<SDValue> &InVals) const override;
+
+    SDValue passArgOnStack(SDValue StackPtr, unsigned Offset, SDValue Chain,
+                           SDValue Arg, const SDLoc &DL, bool IsTailCall,
+                           SelectionDAG &DAG) const;
+
+    SDValue LowerCall(TargetLowering::CallLoweringInfo &CLI,
+                      SmallVectorImpl<SDValue> &InVals) const override;
+
+    bool CanLowerReturn(CallingConv::ID CallConv, MachineFunction &MF,
+                        bool isVarArg,
+                        const SmallVectorImpl<ISD::OutputArg> &Outs,
+                        LLVMContext &Context) const override;
+
+    SDValue LowerReturn(SDValue Chain, CallingConv::ID CallConv, bool isVarArg,
+                        const SmallVectorImpl<ISD::OutputArg> &Outs,
+                        const SmallVectorImpl<SDValue> &OutVals,
+                        const SDLoc &dl, SelectionDAG &DAG) const override;
+
+    bool shouldSignExtendTypeInLibCall(EVT Type, bool IsSigned) const override;
+
+    // Inline asm support
+    ConstraintType getConstraintType(StringRef Constraint) const override;
+
+    /// Examine constraint string and operand type and determine a weight value.
+    /// The operand object must already have been set up with the operand type.
+    ConstraintWeight getSingleConstraintMatchWeight(
+      AsmOperandInfo &info, const char *constraint) const override;
+
+    /// This function parses registers that appear in inline-asm constraints.
+    /// It returns pair (0, 0) on failure.
+    std::pair<unsigned, const TargetRegisterClass *>
+    parseRegForInlineAsmConstraint(StringRef C, MVT VT) const;
+
+    std::pair<unsigned, const TargetRegisterClass *>
+    getRegForInlineAsmConstraint(const TargetRegisterInfo *TRI,
+                                 StringRef Constraint, MVT VT) const override;
+
+    /// LowerAsmOperandForConstraint - Lower the specified operand into the Ops
+    /// vector.  If it is invalid, don't add anything to Ops. If hasMemory is
+    /// true it means one of the asm constraint of the inline asm instruction
+    /// being processed is 'm'.
+    void LowerAsmOperandForConstraint(SDValue Op,
+                                      std::string &Constraint,
+                                      std::vector<SDValue> &Ops,
+                                      SelectionDAG &DAG) const override;
+
+    unsigned
+    getInlineAsmMemConstraint(StringRef ConstraintCode) const override {
+      if (ConstraintCode == "R")
+        return InlineAsm::Constraint_R;
+      else if (ConstraintCode == "ZC")
+        return InlineAsm::Constraint_ZC;
+      else if (ConstraintCode == "ZB")
+        return InlineAsm::Constraint_ZB;
+      return TargetLowering::getInlineAsmMemConstraint(ConstraintCode);
+    }
+
+    bool isLegalAddressingMode(const DataLayout &DL, const AddrMode &AM,
+                               Type *Ty, unsigned AS,
+                               Instruction *I = nullptr) const override;
+
+    bool isOffsetFoldingLegal(const GlobalAddressSDNode *GA) const override;
+
+    EVT getOptimalMemOpType(const MemOp &Op,
+                            const AttributeList &FuncAttributes) const override;
+
+    /// isFPImmLegal - Returns true if the target can instruction select the
+    /// specified FP immediate natively. If false, the legalizer will
+    /// materialize the FP immediate as a load from a constant pool.
+    bool isFPImmLegal(const APFloat &Imm, EVT VT,
+                      bool ForCodeSize) const override;
+
+    bool useSoftFloat() const override;
+
+    bool shouldInsertFencesForAtomic(const Instruction *I) const override {
+      return isa<LoadInst>(I) || isa<StoreInst>(I);
+    }
+
+    /// Emit a sign-extension using sll/sra, seb, or seh appropriately.
+    MachineBasicBlock *emitSignExtendToI32InReg(MachineInstr &MI,
+                                                MachineBasicBlock *BB,
+                                                unsigned Size, unsigned DstReg,
+                                                unsigned SrcRec) const;
+
+    MachineBasicBlock *emitLoadAddress(MachineInstr &MI,
+                                       MachineBasicBlock *BB) const;
+    MachineBasicBlock *emitAtomicBinary(MachineInstr &MI,
+                                        MachineBasicBlock *BB) const;
+    MachineBasicBlock *emitAtomicBinaryPartword(MachineInstr &MI,
+                                                MachineBasicBlock *BB,
+                                                unsigned Size) const;
+    MachineBasicBlock *emitAtomicCmpSwap(MachineInstr &MI,
+                                         MachineBasicBlock *BB) const;
+    MachineBasicBlock *emitAtomicCmpSwapPartword(MachineInstr &MI,
+                                                 MachineBasicBlock *BB,
+                                                 unsigned Size) const;
+    MachineBasicBlock *emitSEL_D(MachineInstr &MI, MachineBasicBlock *BB) const;
+
+    MachineBasicBlock *emitPseudoSELECT(MachineInstr &MI, MachineBasicBlock *BB,
+                                        bool isFPCmp, unsigned Opc) const;
+  };
+
+} // end namespace llvm
+
+#endif // LLVM_LIB_TARGET_LOONGARCH_LOONGARCHISELLOWERING_H
diff --git a/llvm/lib/Target/LoongArch/LoongArchInstrFormats.td b/llvm/lib/Target/LoongArch/LoongArchInstrFormats.td
new file mode 100644
index 000000000000..d75d5198bde1
--- /dev/null
+++ b/llvm/lib/Target/LoongArch/LoongArchInstrFormats.td
@@ -0,0 +1,790 @@
+//===-- LoongArchInstrFormats.td - LoongArch Instruction Formats -----*- tablegen -*-===//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+
+//===----------------------------------------------------------------------===//
+//  Describe LoongArch instructions format
+//
+//  CPU INSTRUCTION FORMATS
+//
+//  opcode  - operation code.
+//  rs      - src reg.
+//  rt      - dst reg (on a 2 regs instr) or src reg (on a 3 reg instr).
+//  rd      - dst reg, only used on 3 regs instr.
+//  shamt   - only used on shift instructions, contains the shift amount.
+//  funct   - combined with opcode field give us an operation code.
+//
+//===----------------------------------------------------------------------===//
+
+class StdArch {
+
+  bits<32> Inst;
+}
+
+// Format specifies the encoding used by the instruction.  This is part of the
+// ad-hoc solution used to emit machine instruction encodings by our machine
+// code emitter.
+class Format<bits<4> val> {
+  bits<4> Value = val;
+}
+
+def Pseudo    : Format<0>;
+def FrmR      : Format<1>;
+def FrmI      : Format<2>;
+def FrmJ      : Format<3>;
+def FrmFR     : Format<4>;
+def FrmFI     : Format<5>;
+def FrmOther  : Format<6>;
+
+// Generic LoongArch Format
+class InstLA<dag outs, dag ins, string asmstr, list<dag> pattern, Format f>
+    : Instruction
+{
+  field bits<32> Inst;
+  Format Form = f;
+
+  let Namespace = "LoongArch";
+
+  let Size = 4;
+
+  let OutOperandList = outs;
+  let InOperandList  = ins;
+  let AsmString   = asmstr;
+  let Pattern     = pattern;
+
+  //
+  // Attributes specific to LoongArch instructions...
+  //
+  bits<4> FormBits     = Form.Value;
+  bit isCTI            = 0; // Any form of Control Transfer Instruction.
+                            // Required for LoongArch
+  bit hasForbiddenSlot = 0; // Instruction has a forbidden slot.
+  bit IsPCRelativeLoad = 0; // Load instruction with implicit source register
+                            // ($pc) and with explicit offset and destination
+                            // register
+  bit hasFCCRegOperand = 0; // Instruction uses $fcc<X> register
+
+  // TSFlags layout should be kept in sync with MCTargetDesc/LoongArchBaseInfo.h.
+  let TSFlags{3-0}   = FormBits;
+  let TSFlags{4}     = isCTI;
+  let TSFlags{5}     = hasForbiddenSlot;
+  let TSFlags{6}     = IsPCRelativeLoad;
+  let TSFlags{7}     = hasFCCRegOperand;
+
+  let DecoderNamespace = "LoongArch";
+
+  field bits<32> SoftFail = 0;
+}
+
+class InstForm<dag outs, dag ins, string asmstr, list<dag> pattern,
+               Format f, string opstr = ""> :
+  InstLA<outs, ins, asmstr, pattern, f> {
+  string BaseOpcode = opstr;
+  string Arch;
+}
+
+class LoongArch_str<string opstr> {
+  string Arch;
+  string BaseOpcode = opstr;
+}
+
+//===-----------------------------------------------------------===//
+// Format instruction classes in the LoongArch
+//===-----------------------------------------------------------===//
+
+// R2 classes: 2 registers
+//
+class R2 : StdArch {
+  bits<5> rj;
+  bits<5> rd;
+
+  let Inst{9-5} = rj;
+  let Inst{4-0} = rd;
+}
+
+class R2I<bits<5> op>
+    : R2 {
+  let Inst{31-15} = 0x0;
+  let Inst{14-10} = op;
+}
+
+class R2F<bits<10> op>
+    : R2 {
+  bits<5> fj;
+  bits<5> fd;
+
+  let Inst{31-20} = 0x11;
+  let Inst{19-10} = op;
+  let Inst{9-5} = fj;
+  let Inst{4-0} = fd;
+}
+
+class MOVFI<bits<10> op>
+    : R2 {
+  bits<5> rj;
+  bits<5> fd;
+
+  let Inst{31-20} = 0x11;
+  let Inst{19-10} = op;
+  let Inst{9-5} = rj;
+  let Inst{4-0} = fd;
+}
+
+class MOVIF<bits<10> op>
+    : R2 {
+  bits<5> fj;
+  bits<5> rd;
+
+  let Inst{31-20} = 0x11;
+  let Inst{19-10} = op;
+  let Inst{9-5} = fj;
+  let Inst{4-0} = rd;
+}
+
+class R2P<bits<3> op>
+    : R2 {
+  let Inst{31-13} = 0x3240;
+  let Inst{12-10} = op;
+}
+
+class R2_CSR<bits<8> op>
+    : StdArch {
+  bits<5> rj;
+  bits<5> rd;
+  bits<14> csr;
+
+  let Inst{31-24} = op;
+  let Inst{23-10} = csr;
+  let Inst{9-5} = rj;
+  let Inst{4-0} = rd;
+}
+
+class R2_SI16<bits<6> op>
+    : StdArch {
+  bits<5> rd;
+  bits<5> rj;
+  bits<16> si16;
+
+  let Inst{31-26} = op;
+  let Inst{25-10} = si16;
+  let Inst{9-5} = rj;
+  let Inst{4-0} = rd;
+}
+
+class R2_COND<bits<2> op, bits<5> cond>
+    : StdArch {
+  bits<5> fj;
+  bits<5> fk;
+  bits<3> cd;
+
+  let Inst{31-22} = 0x30;
+  let Inst{21-20} = op;
+  let Inst{19-15} = cond;
+  let Inst{14-10} = fk;
+  let Inst{9-5} = fj;
+  let Inst{4-3} = 0b00;
+  let Inst{2-0} = cd;
+}
+
+class R2_LEVEL<bits<14> op>
+    : StdArch {
+  bits<5> rj;
+  bits<5> rd;
+  bits<8> level;
+
+  let Inst{31-18} = op;
+  let Inst{17-10} = level;
+  let Inst{9-5} = rj;
+  let Inst{4-0} = rd;
+}
+
+class IMM32<bits<6> op>
+    : StdArch {
+  let Inst{31-16} = 0x0648;
+  let Inst{15-10} = op;
+  let Inst{9-0} = 0;
+}
+
+class WAIT_FM : StdArch {
+  bits<15> hint;
+
+  let Inst{31-15} = 0xc91;
+  let Inst{14-0} = hint;
+}
+
+class R2_INVTLB : StdArch {
+  bits<5> rj;
+  bits<5> op;
+  bits<5> rk;
+
+  let Inst{31-15} = 0xc93;
+  let Inst{14-10} = rk;
+  let Inst{9-5} = rj;
+  let Inst{4-0} = op;
+}
+
+class BAR_FM<bits<1> op>
+    : StdArch {
+  bits<15> hint;
+
+  let Inst{31-16} = 0x3872;
+  let Inst{15} = op;
+  let Inst{14-0} = hint;
+}
+
+class PRELD_FM : StdArch {
+  bits<5> rj;
+  bits<5> hint;
+  bits<12> imm12;
+
+  let Inst{31-22} = 0xab;
+  let Inst{21-10} = imm12;
+  let Inst{9-5} = rj;
+  let Inst{4-0} = hint;
+}
+
+// R3 classes: 3 registers
+//
+class R3 : StdArch {
+  bits<5> rk;
+  bits<5> rj;
+  bits<5> rd;
+
+  let Inst{14-10} = rk;
+  let Inst{9-5} = rj;
+  let Inst{4-0} = rd;
+}
+
+class R3I<bits<7> op>
+    : R3 {
+  let Inst{31-22} = 0x0;
+  let Inst{21-15} = op;
+}
+
+class R3F<bits<6> op>
+    : R3 {
+  bits<5> fk;
+  bits<5> fj;
+  bits<5> fd;
+
+  let Inst{31-21} = 0x8;
+  let Inst{20-15} = op;
+  let Inst{14-10} = fk;
+  let Inst{9-5} = fj;
+  let Inst{4-0} = fd;
+}
+
+class R3MI<bits<8> op>
+    : R3 {
+  let Inst{31-23} = 0x70;
+  let Inst{22-15} = op;
+}
+
+class AM<bits<6> op> : StdArch {
+  bits<5> rk;
+  bits<17> addr; // rj + 12 bits offset 0
+  bits<5> rd;
+
+  let Inst{31-21} = 0x1c3;
+  let Inst{20-15} = op;
+  let Inst{14-10} = rk;
+  let Inst{9-5} = addr{16-12};
+  let Inst{4-0} = rd;
+}
+
+class R3MF<bits<8> op>
+    : R3 {
+  bits<5> fd;
+
+  let Inst{31-23} = 0x70;
+  let Inst{22-15} = op;
+  let Inst{4-0} = fd;
+}
+
+class R3_SA2<bits<5> op>
+    : StdArch {
+  bits<5> rk;
+  bits<5> rj;
+  bits<5> rd;
+  bits<2> sa;
+
+  let Inst{31-22} = 0x0;
+  let Inst{21-17} = op;
+  let Inst{16-15} = sa;
+  let Inst{14-10} = rk;
+  let Inst{9-5} = rj;
+  let Inst{4-0} = rd;
+}
+
+class R3_SA3 : StdArch {
+  bits<5> rk;
+  bits<5> rj;
+  bits<5> rd;
+  bits<3> sa;
+
+  let Inst{31-18} = 3;
+  let Inst{17-15} = sa;
+  let Inst{14-10} = rk;
+  let Inst{9-5} = rj;
+  let Inst{4-0} = rd;
+}
+
+// R4 classes: 4 registers
+//
+class R4MUL<bits<4> op>
+    : StdArch {
+  bits<5> fa;
+  bits<5> fk;
+  bits<5> fj;
+  bits<5> fd;
+
+  let Inst{31-24} = 0x8;
+  let Inst{23-20} = op;
+  let Inst{19-15} = fa;
+  let Inst{14-10} = fk;
+  let Inst{9-5} = fj;
+  let Inst{4-0} = fd;
+}
+
+class R4CMP<bits<2> op>
+    : StdArch {
+  bits<5> cond;
+  bits<5> fk;
+  bits<5> fj;
+  bits<3> cd;
+
+  let Inst{31-22} = 0x30;
+  let Inst{21-20} = op;
+  let Inst{19-15} = cond;
+  let Inst{14-10} = fk;
+  let Inst{9-5} = fj;
+  let Inst{4-3} = 0;
+  let Inst{2-0} = cd;
+}
+
+class R4SEL : StdArch {
+  bits<3> ca;
+  bits<5> fk;
+  bits<5> fj;
+  bits<5> fd;
+
+  let Inst{31-18} = 0x340;
+  let Inst{17-15} = ca;
+  let Inst{14-10} = fk;
+  let Inst{9-5} = fj;
+  let Inst{4-0} = fd;
+}
+
+// R2_IMM5 classes: 2registers and 1 5bit-immediate
+//
+class R2_IMM5<bits<2> op>
+    : StdArch {
+  bits<5> rj;
+  bits<5> rd;
+  bits<5> imm5;
+
+  let Inst{31-20} = 0x4;
+  let Inst{19-18} = op;
+  let Inst{17-15} = 0x1;
+  let Inst{14-10} = imm5;
+  let Inst{9-5} = rj;
+  let Inst{4-0} = rd;
+}
+
+// R2_IMM6 classes: 2registers and 1 6bit-immediate
+//
+class R2_IMM6<bits<2> op>
+    : StdArch {
+  bits<5> rj;
+  bits<5> rd;
+  bits<6> imm6;
+
+  let Inst{31-20} = 0x4;
+  let Inst{19-18} = op;
+  let Inst{17-16} = 0x1;
+  let Inst{15-10} = imm6;
+  let Inst{9-5} = rj;
+  let Inst{4-0} = rd;
+}
+
+// R2_IMM12 classes: 2 registers and 1 12bit-immediate
+//
+class LOAD_STORE<bits<4> op>
+    : StdArch {
+  bits<5> rd;
+  bits<17> addr;
+
+  let Inst{31-26} = 0xa;
+  let Inst{25-22} = op;
+  let Inst{21-10} = addr{11-0};
+  let Inst{9-5} = addr{16-12};
+  let Inst{4-0} = rd;
+}
+// for reloc
+class LOAD_STORE_RRI<bits<4> op>
+    : StdArch {
+  bits<5> rj;
+  bits<5> rd;
+  bits<12> imm12;
+
+  let Inst{31-26} = 0xa;
+  let Inst{25-22} = op;
+  let Inst{21-10} = imm12;
+  let Inst{9-5} = rj;
+  let Inst{4-0} = rd;
+}
+
+
+class R2_IMM12<bits<3> op>
+    : StdArch {
+  bits<5> rj;
+  bits<5> rd;
+  bits<12> imm12;
+
+  let Inst{31-25} = 0x1;
+  let Inst{24-22} = op;
+  let Inst{21-10} = imm12;
+  let Inst{9-5} = rj;
+  let Inst{4-0} = rd;
+}
+
+class LEA_ADDI_FM<bits<3> op>
+    : StdArch {
+  bits<5> rd;
+  bits<17> addr;
+
+  let Inst{31-25} = 0x1;
+  let Inst{24-22} = op;
+  let Inst{21-10} = addr{11-0};
+  let Inst{9-5} = addr{16-12};
+  let Inst{4-0} = rd;
+}
+
+// R2_IMM14 classes: 2 registers and 1 14bit-immediate
+//
+class LL_SC<bits<3> op>
+    : StdArch {
+  bits<5> rd;
+  bits<19> addr;
+
+  let Inst{31-27} = 4;
+  let Inst{26-24} = op;
+  let Inst{23-10} = addr{13-0};
+  let Inst{9-5} = addr{18-14};
+  let Inst{4-0} = rd;
+}
+
+// R2_IMM16 classes: 2 registers and 1 16bit-immediate
+//
+class R2_IMM16BEQ<bits<6> op>
+    : StdArch {
+  bits<5> rj;
+  bits<5> rd;
+  bits<16> offs16;
+
+  let Inst{31-26} = op;
+  let Inst{25-10} = offs16;
+  let Inst{9-5} = rj;
+  let Inst{4-0} = rd;
+}
+
+class R2_IMM16JIRL : StdArch {
+  bits<5> rj;
+  bits<5> rd;
+  bits<16> offs16;
+
+  let Inst{31-26} = 0x13;
+  let Inst{25-10} = offs16;
+  let Inst{9-5} = rj;
+  let Inst{4-0} = rd;
+}
+
+// R1_IMM21 classes: 1 registers and 1 21bit-immediate
+//
+class R1_IMM21BEQZ<bits<6> op>
+    : StdArch {
+  bits<5> rj;
+  bits<21> offs21;
+
+  let Inst{31-26} = op;
+  let Inst{25-10} = offs21{15-0};
+  let Inst{9-5} = rj;
+  let Inst{4-0} = offs21{20-16};
+}
+
+class R1_CSR<bits<13> op>
+    : StdArch {
+  bits<5> rd;
+  bits<14> csr;
+
+  let Inst{31-24} = op{7-0};
+  let Inst{23-10} = csr;
+  let Inst{9-5} = op{12-8};
+  let Inst{4-0} = rd;
+}
+
+class R1_SI20<bits<7> op>
+    : StdArch {
+  bits<5> rd;
+  bits<20> si20;
+
+  let Inst{31-25} = op;
+  let Inst{24-5} = si20;
+  let Inst{4-0} = rd;
+}
+
+class R1_CACHE : StdArch {
+  bits<5> rj;
+  bits<5> op;
+  bits<12> si12;
+
+  let Inst{31-22} = 0x18;
+  let Inst{21-10} = si12;
+  let Inst{9-5} = rj;
+  let Inst{4-0} = op;
+}
+
+class R1_SEQ<bits<14> op>
+    : StdArch {
+  bits<5> rj;
+  bits<5> offset;
+  bits<8> seq;
+
+  let Inst{31-18} = op;
+  let Inst{17-10} = seq;
+  let Inst{9-5} = rj;
+  let Inst{4-0} = 0b00000;
+}
+
+class R1_BCEQZ<bits<2> op>
+    : StdArch {
+  bits<21> offset;
+  bits<3> cj;
+
+  let Inst{31-26} = 0x12;
+  let Inst{25-10} = offset{15-0};
+  let Inst{9-8} = op;
+  let Inst{7-5} = cj;
+  let Inst{4-0} = offset{20-16};
+}
+
+// IMM26 classes: 1 26bit-immediate
+//
+class IMM26B<bits<6> op>
+    : StdArch {
+  bits<26> offs26;
+
+  let Inst{31-26} = op;
+  let Inst{25-10} = offs26{15-0};
+  let Inst{9-0} = offs26{25-16};
+}
+
+// LoongArch Pseudo Instructions Format
+class LoongArchPseudo<dag outs, dag ins, list<dag> pattern> :
+  InstLA<outs, ins, "", pattern, Pseudo> {
+  let isCodeGenOnly = 1;
+  let isPseudo = 1;
+}
+
+// Pseudo-instructions for alternate assembly syntax (never used by codegen).
+// These are aliases that require C++ handling to convert to the target
+// instruction, while InstAliases can be handled directly by tblgen.
+class LoongArchAsmPseudoInst<dag outs, dag ins, string asmstr>:
+  InstLA<outs, ins, asmstr, [], Pseudo> {
+  let isPseudo = 1;
+  let Pattern = [];
+}
+
+//
+// Misc instruction classes
+class ASSERT<bits<2> op>
+    : StdArch {
+  bits<5> rk;
+  bits<5> rj;
+
+  let Inst{31-17} = 0x0;
+  let Inst{16-15} = op;
+  let Inst{14-10} = rk;
+  let Inst{9-5} = rj;
+  let Inst{4-0} = 0x0;
+}
+
+class CODE15<bits<7> op>
+    : StdArch {
+  bits<15> Code;
+
+  let Inst{31-22} = 0x0;
+  let Inst{21-15} = op;
+  let Inst{14-0} = Code;
+}
+
+class INSERT_BIT32<bits<1> op>
+    : StdArch {
+  bits<5> msbw;
+  bits<5> lsbw;
+  bits<5> rj;
+  bits<5> rd;
+
+  let Inst{31-21} = 0x3;
+  let Inst{20-16} = msbw;
+  let Inst{15} = op;
+  let Inst{14-10} = lsbw;
+  let Inst{9-5} = rj;
+  let Inst{4-0} = rd;
+}
+
+class INSERT_BIT64<bits<1> op>
+    : StdArch {
+  bits<6> msbd;
+  bits<6> lsbd;
+  bits<5> rj;
+  bits<5> rd;
+
+  let Inst{31-23} = 0x1;
+  let Inst{22} = op;
+  let Inst{21-16} = msbd;
+  let Inst{15-10} = lsbd;
+  let Inst{9-5} = rj;
+  let Inst{4-0} = rd;
+}
+
+class MOVGPR2FCSR: StdArch {
+  bits<5> fcsr;
+  bits<5> rj;
+
+  let Inst{31-10} = 0x4530;
+  let Inst{9-5} = rj;
+  let Inst{4-0} = fcsr;
+}
+
+class MOVFCSR2GPR: StdArch {
+  bits<5> fcsr;
+  bits<5> rd;
+
+  let Inst{31-10} = 0x4532;
+  let Inst{9-5} = fcsr;
+  let Inst{4-0} = rd;
+}
+
+class MOVFGR2FCFR: StdArch {
+  bits<3> cd;
+  bits<5> fj;
+
+  let Inst{31-10} = 0x4534;
+  let Inst{9-5} = fj;
+  let Inst{4-3} = 0;
+  let Inst{2-0} = cd;
+}
+
+class MOVFCFR2FGR: StdArch {
+  bits<3> cj;
+  bits<5> fd;
+
+  let Inst{31-10} = 0x4535;
+  let Inst{9-8} = 0;
+  let Inst{7-5} = cj;
+  let Inst{4-0} = fd;
+}
+
+class MOVGPR2FCFR: StdArch {
+  bits<3> cd;
+  bits<5> rj;
+
+  let Inst{31-10} = 0x4536;
+  let Inst{9-5} = rj;
+  let Inst{4-3} = 0;
+  let Inst{2-0} = cd;
+}
+
+class MOVFCFR2GPR: StdArch {
+  bits<3> cj;
+  bits<5> rd;
+
+  let Inst{31-10} = 0x4537;
+  let Inst{9-8} = 0;
+  let Inst{7-5} = cj;
+  let Inst{4-0} = rd;
+}
+
+class LoongArchInst : InstLA<(outs), (ins), "", [], FrmOther> {
+}
+class JMP_OFFS_2R<bits<6> op> : LoongArchInst {
+  bits<5>  rs;
+  bits<5>  rd;
+  bits<16> offset;
+
+  bits<32> Inst;
+
+  let Inst{31-26} = op;
+  let Inst{25-10} = offset;
+  let Inst{9-5} = rs;
+  let Inst{4-0}  = rd;
+}
+
+class FJ<bits<6> op> : StdArch
+{
+  bits<26> target;
+
+  let Inst{31-26} = op;
+  let Inst{25-10}  = target{15-0};
+  let Inst{9-0}  = target{25-16};
+}
+
+class LUI_FM : StdArch {
+  bits<5> rt;
+  bits<16> imm16;
+
+  let Inst{31-26} = 0xf;
+  let Inst{25-21} = 0;
+  let Inst{20-16} = rt;
+  let Inst{15-0}  = imm16;
+}
+
+class  R2_IMM12M_STD<bits<4> op> : StdArch {
+  bits<5> rj;
+  bits<5> rd;
+  bits<12> imm12;
+
+  let Inst{31-26} = 0xa;
+  let Inst{25-22} = op;
+  let Inst{21-10} = imm12;
+  let Inst{9-5} = rj;
+  let Inst{4-0} = rd;
+}
+
+class LLD_2R<bits<3> Code> : LoongArchInst {
+  bits<5> rd;
+  bits<19> addr;
+  bits<5> rj = addr{18-14};
+  bits<14> offset = addr{13-0};
+
+  bits<32> Inst;
+
+  let Inst{31-27} = 0x4;
+  let Inst{26-24} = Code;
+  let Inst{23-10} = offset;
+  let Inst{9-5} = rj;
+  let Inst{4-0} = rd;
+}
+
+class CEQS_FM<bits<2> op> {
+  bits<5> fj;
+  bits<5> fk;
+  bits<3> cd;
+  bits<5> cond;
+
+  bits<32> Inst;
+
+  let Inst{31-22} = 0x30;
+  let Inst{21-20} = op;
+  let Inst{19-15} = cond;
+  let Inst{14-10} = fk;
+  let Inst{9-5} = fj;
+  let Inst{4-3} = 0b00;
+  let Inst{2-0} = cd;
+}
+
diff --git a/llvm/lib/Target/LoongArch/LoongArchInstrInfo.cpp b/llvm/lib/Target/LoongArch/LoongArchInstrInfo.cpp
new file mode 100644
index 000000000000..b56a8d594a8f
--- /dev/null
+++ b/llvm/lib/Target/LoongArch/LoongArchInstrInfo.cpp
@@ -0,0 +1,990 @@
+//===- LoongArchInstrInfo.cpp - LoongArch Instruction Information -------------------===//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+//
+// This file contains the LoongArch implementation of the TargetInstrInfo class.
+//
+//===----------------------------------------------------------------------===//
+
+#include "LoongArchInstrInfo.h"
+#include "MCTargetDesc/LoongArchBaseInfo.h"
+#include "MCTargetDesc/LoongArchMCTargetDesc.h"
+#include "LoongArchAnalyzeImmediate.h"
+#include "LoongArchSubtarget.h"
+#include "llvm/ADT/SmallVector.h"
+#include "llvm/CodeGen/MachineBasicBlock.h"
+#include "llvm/CodeGen/MachineFrameInfo.h"
+#include "llvm/CodeGen/MachineFunction.h"
+#include "llvm/CodeGen/MachineInstr.h"
+#include "llvm/CodeGen/MachineInstrBuilder.h"
+#include "llvm/CodeGen/MachineOperand.h"
+#include "llvm/CodeGen/RegisterScavenging.h"
+#include "llvm/CodeGen/TargetOpcodes.h"
+#include "llvm/CodeGen/TargetSubtargetInfo.h"
+#include "llvm/IR/DebugLoc.h"
+#include "llvm/MC/MCInstrDesc.h"
+#include "llvm/Target/TargetMachine.h"
+#include <cassert>
+
+using namespace llvm;
+
+#define GET_INSTRINFO_CTOR_DTOR
+#include "LoongArchGenInstrInfo.inc"
+
+// Pin the vtable to this file.
+void LoongArchInstrInfo::anchor() {}
+LoongArchInstrInfo::LoongArchInstrInfo(const LoongArchSubtarget &STI)
+    : LoongArchGenInstrInfo(LoongArch::ADJCALLSTACKDOWN,
+                            LoongArch::ADJCALLSTACKUP),
+    RI(), Subtarget(STI) {}
+
+const LoongArchRegisterInfo &LoongArchInstrInfo::getRegisterInfo() const {
+  return RI;
+}
+
+/// isLoadFromStackSlot - If the specified machine instruction is a direct
+/// load from a stack slot, return the virtual or physical register number of
+/// the destination along with the FrameIndex of the loaded stack slot.  If
+/// not, return 0.  This predicate must return 0 if the instruction has
+/// any side effects other than loading from the stack slot.
+unsigned LoongArchInstrInfo::isLoadFromStackSlot(const MachineInstr &MI,
+                                                 int &FrameIndex) const {
+  unsigned Opc = MI.getOpcode();
+  if ((Opc == LoongArch::LD_W)   || (Opc == LoongArch::LD_D) ||
+      (Opc == LoongArch::FLD_S) || (Opc == LoongArch::FLD_D)) {
+    if ((MI.getOperand(1).isFI()) &&  // is a stack slot
+        (MI.getOperand(2).isImm()) && // the imm is zero
+        (isZeroImm(MI.getOperand(2)))) {
+      FrameIndex = MI.getOperand(1).getIndex();
+      return MI.getOperand(0).getReg();
+    }
+  }
+  return 0;
+}
+
+/// isStoreToStackSlot - If the specified machine instruction is a direct
+/// store to a stack slot, return the virtual or physical register number of
+/// the source reg along with the FrameIndex of the loaded stack slot.  If
+/// not, return 0.  This predicate must return 0 if the instruction has
+/// any side effects other than storing to the stack slot.
+unsigned LoongArchInstrInfo::isStoreToStackSlot(const MachineInstr &MI,
+                                                int &FrameIndex) const {
+  unsigned Opc = MI.getOpcode();
+  if ((Opc == LoongArch::ST_D) || (Opc == LoongArch::ST_W) ||
+      (Opc == LoongArch::FST_S) ||(Opc == LoongArch::FST_D)) {
+    if ((MI.getOperand(1).isFI()) &&  // is a stack slot
+        (MI.getOperand(2).isImm()) && // the imm is zero
+        (isZeroImm(MI.getOperand(2)))) {
+      FrameIndex = MI.getOperand(1).getIndex();
+      return MI.getOperand(0).getReg();
+    }
+  }
+  return 0;
+}
+
+void LoongArchInstrInfo::copyPhysReg(MachineBasicBlock &MBB,
+                                     MachineBasicBlock::iterator I,
+                                     const DebugLoc &DL, MCRegister DestReg,
+                                     MCRegister SrcReg, bool KillSrc) const {
+  unsigned Opc = 0, ZeroReg = 0;
+  if (LoongArch::GPR32RegClass.contains(DestReg)) { // Copy to CPU Reg.
+    if (LoongArch::GPR32RegClass.contains(SrcReg)) {
+      Opc = LoongArch::OR32, ZeroReg = LoongArch::ZERO;
+    }
+    else if (LoongArch::FGR32RegClass.contains(SrcReg))
+      Opc = LoongArch::MOVFR2GR_S;
+    else if (LoongArch::FCFRRegClass.contains(SrcReg))
+      Opc = LoongArch::MOVCF2GR;
+  }
+  else if (LoongArch::GPR32RegClass.contains(SrcReg)) { // Copy from CPU Reg.
+    if (LoongArch::FGR32RegClass.contains(DestReg))
+      Opc = LoongArch::MOVGR2FR_W;
+    else if (LoongArch::FCFRRegClass.contains(DestReg))
+      Opc = LoongArch::MOVGR2CF;
+  }
+  else if (LoongArch::FGR32RegClass.contains(DestReg, SrcReg))
+    Opc = LoongArch::FMOV_S;
+  else if (LoongArch::FGR64RegClass.contains(DestReg, SrcReg))
+    Opc = LoongArch::FMOV_D;
+  else if (LoongArch::GPR64RegClass.contains(DestReg)) { // Copy to CPU64 Reg.
+    if (LoongArch::GPR64RegClass.contains(SrcReg))
+      Opc = LoongArch::OR, ZeroReg = LoongArch::ZERO_64;
+    else if (LoongArch::FGR64RegClass.contains(SrcReg))
+      Opc = LoongArch::MOVFR2GR_D;
+    else if (LoongArch::FCFRRegClass.contains(SrcReg))
+      Opc = LoongArch::MOVCF2GR;
+  }
+  else if (LoongArch::GPR64RegClass.contains(SrcReg)) { // Copy from CPU64 Reg.
+    if (LoongArch::FGR64RegClass.contains(DestReg))
+      Opc = LoongArch::MOVGR2FR_D;
+    else if (LoongArch::FCFRRegClass.contains(DestReg))
+      Opc = LoongArch::MOVGR2CF;
+  }
+  else if (LoongArch::FGR32RegClass.contains(DestReg)) // Copy to FGR32 Reg
+      Opc = LoongArch::MOVCF2FR;
+  else if (LoongArch::FGR32RegClass.contains(SrcReg))  // Copy from FGR32 Reg
+      Opc = LoongArch::MOVFR2CF;
+  else if (LoongArch::FGR64RegClass.contains(DestReg)) // Copy to FGR64 Reg
+      Opc = LoongArch::MOVCF2FR;
+  else if (LoongArch::FGR64RegClass.contains(SrcReg))  // Copy from FGR64 Reg
+      Opc = LoongArch::MOVFR2CF;
+
+  assert(Opc && "Cannot copy registers");
+
+  MachineInstrBuilder MIB = BuildMI(MBB, I, DL, get(Opc));
+
+  if (DestReg)
+    MIB.addReg(DestReg, RegState::Define);
+
+  if (SrcReg)
+    MIB.addReg(SrcReg, getKillRegState(KillSrc));
+
+  if (ZeroReg)
+    MIB.addReg(ZeroReg);
+}
+
+static bool isORCopyInst(const MachineInstr &MI) {
+  switch (MI.getOpcode()) {
+  default:
+    break;
+  case LoongArch::OR:
+    if (MI.getOperand(2).getReg() == LoongArch::ZERO_64)
+      return true;
+    break;
+  case LoongArch::OR32:
+    if (MI.getOperand(2).getReg() == LoongArch::ZERO)
+      return true;
+    break;
+  }
+  return false;
+}
+
+/// We check for the common case of 'or', as it's LoongArch' preferred instruction
+/// for GPRs but we have to check the operands to ensure that is the case.
+/// Other move instructions for LoongArch are directly identifiable.
+Optional<DestSourcePair>
+LoongArchInstrInfo::isCopyInstrImpl(const MachineInstr &MI) const {
+  if (MI.isMoveReg() || isORCopyInst(MI)) {
+    return DestSourcePair{MI.getOperand(0), MI.getOperand(1)};
+  }
+  return None;
+}
+
+void LoongArchInstrInfo::
+storeRegToStack(MachineBasicBlock &MBB, MachineBasicBlock::iterator I,
+                Register SrcReg, bool isKill, int FI,
+                const TargetRegisterClass *RC, const TargetRegisterInfo *TRI,
+                int64_t Offset) const {
+  DebugLoc DL;
+  MachineMemOperand *MMO = GetMemOperand(MBB, FI, MachineMemOperand::MOStore);
+
+  unsigned Opc = 0;
+  if (LoongArch::GPR32RegClass.hasSubClassEq(RC))
+    Opc = LoongArch::ST_W;
+  else if (LoongArch::GPR64RegClass.hasSubClassEq(RC))
+    Opc = LoongArch::ST_D;
+  else if (LoongArch::FGR64RegClass.hasSubClassEq(RC))
+    Opc = LoongArch::FST_D;
+  else if (LoongArch::FGR32RegClass.hasSubClassEq(RC))
+    Opc = LoongArch::FST_S;
+
+  assert(Opc && "Register class not handled!");
+  BuildMI(MBB, I, DL, get(Opc)).addReg(SrcReg, getKillRegState(isKill))
+    .addFrameIndex(FI).addImm(Offset).addMemOperand(MMO);
+}
+
+void LoongArchInstrInfo::
+loadRegFromStack(MachineBasicBlock &MBB, MachineBasicBlock::iterator I,
+                 Register DestReg, int FI, const TargetRegisterClass *RC,
+                 const TargetRegisterInfo *TRI, int64_t Offset) const {
+  DebugLoc DL;
+  if (I != MBB.end()) DL = I->getDebugLoc();
+  MachineMemOperand *MMO = GetMemOperand(MBB, FI, MachineMemOperand::MOLoad);
+  unsigned Opc = 0;
+
+  if (LoongArch::GPR32RegClass.hasSubClassEq(RC))
+    Opc = LoongArch::LD_W;
+  else if (LoongArch::GPR64RegClass.hasSubClassEq(RC))
+    Opc = LoongArch::LD_D;
+  else if (LoongArch::FGR32RegClass.hasSubClassEq(RC))
+    Opc = LoongArch::FLD_S;
+  else if (LoongArch::FGR64RegClass.hasSubClassEq(RC))
+    Opc = LoongArch::FLD_D;
+
+  assert(Opc && "Register class not handled!");
+
+  BuildMI(MBB, I, DL, get(Opc), DestReg)
+      .addFrameIndex(FI)
+      .addImm(Offset)
+      .addMemOperand(MMO);
+}
+
+bool LoongArchInstrInfo::expandPostRAPseudo(MachineInstr &MI) const {
+  MachineBasicBlock &MBB = *MI.getParent();
+  switch (MI.getDesc().getOpcode()) {
+  default:
+    return false;
+  case LoongArch::RetRA:
+    expandRetRA(MBB, MI);
+    break;
+  case LoongArch::ERet:
+    expandERet(MBB, MI);
+    break;
+  case LoongArch::PseudoFFINT_S_W:
+    expandCvtFPInt(MBB, MI, LoongArch::FFINT_S_W, LoongArch::MOVGR2FR_W, false);
+    break;
+  case LoongArch::PseudoFFINT_S_L:
+    expandCvtFPInt(MBB, MI, LoongArch::FFINT_S_L, LoongArch::MOVGR2FR_D, true);
+    break;
+  case LoongArch::PseudoFFINT_D_W:
+    expandCvtFPInt(MBB, MI, LoongArch::FFINT_D_W, LoongArch::MOVGR2FR_W, true);
+    break;
+  case LoongArch::PseudoFFINT_D_L:
+    expandCvtFPInt(MBB, MI, LoongArch::FFINT_D_L, LoongArch::MOVGR2FR_D, true);
+    break;
+  case LoongArch::LoongArcheh_return32:
+  case LoongArch::LoongArcheh_return64:
+    expandEhReturn(MBB, MI);
+    break;
+  }
+
+  MBB.erase(MI);
+  return true;
+}
+
+/// getOppositeBranchOpc - Return the inverse of the specified
+/// opcode, e.g. turning BEQ to BNE.
+unsigned LoongArchInstrInfo::getOppositeBranchOpc(unsigned Opc) const {
+  switch (Opc) {
+  default:                 llvm_unreachable("Illegal opcode!");
+  case LoongArch::BEQ32:   return LoongArch::BNE32;
+  case LoongArch::BEQ:     return LoongArch::BNE;
+  case LoongArch::BNE32:   return LoongArch::BEQ32;
+  case LoongArch::BNE:     return LoongArch::BEQ;
+  case LoongArch::BEQZ32:  return LoongArch::BNEZ32;
+  case LoongArch::BEQZ:    return LoongArch::BNEZ;
+  case LoongArch::BNEZ32:  return LoongArch::BEQZ32;
+  case LoongArch::BNEZ:    return LoongArch::BEQZ;
+  case LoongArch::BCEQZ:   return LoongArch::BCNEZ;
+  case LoongArch::BCNEZ:   return LoongArch::BCEQZ;
+  case LoongArch::BLT32:   return LoongArch::BGE32;
+  case LoongArch::BLT:     return LoongArch::BGE;
+  case LoongArch::BGE32:   return LoongArch::BLT32;
+  case LoongArch::BGE:     return LoongArch::BLT;
+  case LoongArch::BLTU32:  return LoongArch::BGEU32;
+  case LoongArch::BLTU:    return LoongArch::BGEU;
+  case LoongArch::BGEU32:  return LoongArch::BLTU32;
+  case LoongArch::BGEU:    return LoongArch::BLTU;
+  }
+}
+
+/// Adjust SP by Amount bytes.
+void LoongArchInstrInfo::adjustStackPtr(unsigned SP, int64_t Amount,
+                                          MachineBasicBlock &MBB,
+                                          MachineBasicBlock::iterator I) const {
+  LoongArchABIInfo ABI = Subtarget.getABI();
+  DebugLoc DL;
+  unsigned ADDI = ABI.GetPtrAddiOp();
+
+  if (Amount == 0)
+    return;
+
+  if (isInt<12>(Amount)) {
+    // addi sp, sp, amount
+    BuildMI(MBB, I, DL, get(ADDI), SP).addReg(SP).addImm(Amount)
+      .setMIFlag(MachineInstr::FrameSetup);
+  } else {
+    // For numbers which are not 12bit integers we synthesize Amount inline
+    // then add or subtract it from sp.
+    unsigned Opc = ABI.GetPtrAddOp();
+    if (Amount < 0) {
+      Opc = ABI.GetPtrSubOp();
+      Amount = -Amount;
+    }
+    unsigned Reg = loadImmediate(Amount, MBB, I, DL, nullptr);
+    BuildMI(MBB, I, DL, get(Opc), SP).addReg(SP).addReg(Reg, RegState::Kill)
+      .setMIFlag(MachineInstr::FrameSetup);
+  }
+}
+
+/// This function generates the sequence of instructions needed to get the
+/// result of adding register REG and immediate IMM.
+unsigned LoongArchInstrInfo::loadImmediate(int64_t Imm, MachineBasicBlock &MBB,
+                                             MachineBasicBlock::iterator II,
+                                             const DebugLoc &DL,
+                                             unsigned *NewImm) const {
+  LoongArchAnalyzeImmediate AnalyzeImm;
+  const LoongArchSubtarget &STI = Subtarget;
+  MachineRegisterInfo &RegInfo = MBB.getParent()->getRegInfo();
+  unsigned Size = STI.isABI_LP64D() ? 64 : 32;
+  unsigned ZEROReg = STI.isABI_LP64D() ? LoongArch::ZERO_64 : LoongArch::ZERO;
+  const TargetRegisterClass *RC = STI.isABI_LP64D() ?
+    &LoongArch::GPR64RegClass : &LoongArch::GPR32RegClass;
+  bool LastInstrIsADDI = NewImm;
+
+  const LoongArchAnalyzeImmediate::InstSeq &Seq =
+    AnalyzeImm.Analyze(Imm, Size, LastInstrIsADDI);
+  LoongArchAnalyzeImmediate::InstSeq::const_iterator Inst = Seq.begin();
+
+  assert(Seq.size() && (!LastInstrIsADDI || (Seq.size() > 1)));
+
+  unsigned Reg = RegInfo.createVirtualRegister(RC);
+
+  BuildMI(MBB, II, DL, get(Inst->Opc), Reg).addReg(ZEROReg)
+    .addImm(SignExtend64<12>(Inst->ImmOpnd));
+
+  // Build the remaining instructions in Seq.
+  for (++Inst; Inst != Seq.end() - LastInstrIsADDI; ++Inst)
+    BuildMI(MBB, II, DL, get(Inst->Opc), Reg).addReg(Reg, RegState::Kill)
+      .addImm(SignExtend64<12>(Inst->ImmOpnd));
+
+  if (LastInstrIsADDI)
+    *NewImm = Inst->ImmOpnd;
+
+  return Reg;
+}
+
+unsigned LoongArchInstrInfo::getAnalyzableBrOpc(unsigned Opc) const {
+    return (Opc == LoongArch::B      || Opc == LoongArch::B32      ||
+            Opc == LoongArch::BEQZ   || Opc == LoongArch::BEQZ32   ||
+            Opc == LoongArch::BNEZ   || Opc == LoongArch::BNEZ32   ||
+            Opc == LoongArch::BCEQZ ||
+            Opc == LoongArch::BCNEZ ||
+            Opc == LoongArch::BEQ    || Opc == LoongArch::BEQ32    ||
+            Opc == LoongArch::BNE    || Opc == LoongArch::BNE32    ||
+            Opc == LoongArch::BLT    || Opc == LoongArch::BLT32    ||
+            Opc == LoongArch::BGE    || Opc == LoongArch::BGE32    ||
+            Opc == LoongArch::BLTU   || Opc == LoongArch::BLTU32   ||
+            Opc == LoongArch::BGEU   || Opc == LoongArch::BGEU32) ? Opc : 0;
+}
+
+void LoongArchInstrInfo::expandRetRA(MachineBasicBlock &MBB,
+                                  MachineBasicBlock::iterator I) const {
+
+  MachineInstrBuilder MIB;
+
+  if (Subtarget.is64Bit())
+    MIB = BuildMI(MBB, I, I->getDebugLoc(), get(LoongArch::PseudoReturn64))
+              .addReg(LoongArch::RA_64, RegState::Undef);
+  else
+    MIB = BuildMI(MBB, I, I->getDebugLoc(), get(LoongArch::PseudoReturn))
+              .addReg(LoongArch::RA, RegState::Undef);
+
+  // Retain any imp-use flags.
+  for (auto & MO : I->operands()) {
+    if (MO.isImplicit())
+      MIB.add(MO);
+  }
+}
+
+void LoongArchInstrInfo::expandERet(MachineBasicBlock &MBB,
+                                 MachineBasicBlock::iterator I) const {
+    BuildMI(MBB, I, I->getDebugLoc(), get(LoongArch::ERTN));
+}
+
+std::pair<bool, bool>
+LoongArchInstrInfo::compareOpndSize(unsigned Opc,
+                                 const MachineFunction &MF) const {
+  const MCInstrDesc &Desc = get(Opc);
+  assert(Desc.NumOperands == 2 && "Unary instruction expected.");
+  const LoongArchRegisterInfo *RI = &getRegisterInfo();
+  unsigned DstRegSize = RI->getRegSizeInBits(*getRegClass(Desc, 0, RI, MF));
+  unsigned SrcRegSize = RI->getRegSizeInBits(*getRegClass(Desc, 1, RI, MF));
+
+  return std::make_pair(DstRegSize > SrcRegSize, DstRegSize < SrcRegSize);
+}
+
+void LoongArchInstrInfo::expandCvtFPInt(MachineBasicBlock &MBB,
+                                     MachineBasicBlock::iterator I,
+                                     unsigned CvtOpc, unsigned MovOpc,
+                                     bool IsI64) const {
+  const MCInstrDesc &CvtDesc = get(CvtOpc), &MovDesc = get(MovOpc);
+  const MachineOperand &Dst = I->getOperand(0), &Src = I->getOperand(1);
+  unsigned DstReg = Dst.getReg(), SrcReg = Src.getReg(), TmpReg = DstReg;
+  unsigned KillSrc =  getKillRegState(Src.isKill());
+  DebugLoc DL = I->getDebugLoc();
+  bool DstIsLarger, SrcIsLarger;
+
+  std::tie(DstIsLarger, SrcIsLarger) =
+      compareOpndSize(CvtOpc, *MBB.getParent());
+
+  if (DstIsLarger)
+    TmpReg = getRegisterInfo().getSubReg(DstReg, LoongArch::sub_lo);
+
+  if (SrcIsLarger)
+    DstReg = getRegisterInfo().getSubReg(DstReg, LoongArch::sub_lo);
+
+  BuildMI(MBB, I, DL, MovDesc, TmpReg).addReg(SrcReg, KillSrc);
+  BuildMI(MBB, I, DL, CvtDesc, DstReg).addReg(TmpReg, RegState::Kill);
+}
+
+void LoongArchInstrInfo::expandEhReturn(MachineBasicBlock &MBB,
+                                     MachineBasicBlock::iterator I) const {
+  // This pseudo instruction is generated as part of the lowering of
+  // ISD::EH_RETURN. We convert it to a stack increment by OffsetReg, and
+  // indirect jump to TargetReg
+  LoongArchABIInfo ABI = Subtarget.getABI();
+  unsigned ADD = ABI.GetPtrAddOp();
+  unsigned SP = Subtarget.is64Bit() ? LoongArch::SP_64 : LoongArch::SP;
+  unsigned RA = Subtarget.is64Bit() ? LoongArch::RA_64 : LoongArch::RA;
+  unsigned T8 = Subtarget.is64Bit() ? LoongArch::T8_64 : LoongArch::T8;
+  unsigned ZERO = Subtarget.is64Bit() ? LoongArch::ZERO_64 : LoongArch::ZERO;
+  unsigned OffsetReg = I->getOperand(0).getReg();
+  unsigned TargetReg = I->getOperand(1).getReg();
+
+  // add $ra, $v0, $zero
+  // add $sp, $sp, $v1
+  // jr   $ra (via RetRA)
+  const TargetMachine &TM = MBB.getParent()->getTarget();
+  if (TM.isPositionIndependent())
+    BuildMI(MBB, I, I->getDebugLoc(), get(ADD), T8)
+        .addReg(TargetReg)
+        .addReg(ZERO);
+  BuildMI(MBB, I, I->getDebugLoc(), get(ADD), RA)
+      .addReg(TargetReg)
+      .addReg(ZERO);
+  BuildMI(MBB, I, I->getDebugLoc(), get(ADD), SP).addReg(SP).addReg(OffsetReg);
+  expandRetRA(MBB, I);
+}
+
+
+bool LoongArchInstrInfo::isZeroImm(const MachineOperand &op) const {
+  return op.isImm() && op.getImm() == 0;
+}
+
+/// insertNoop - If data hazard condition is found insert the target nop
+/// instruction.
+// FIXME: This appears to be dead code.
+void LoongArchInstrInfo::
+insertNoop(MachineBasicBlock &MBB, MachineBasicBlock::iterator MI) const
+{
+  DebugLoc DL;
+  BuildMI(MBB, MI, DL, get(LoongArch::NOP));
+}
+
+MachineMemOperand *
+LoongArchInstrInfo::GetMemOperand(MachineBasicBlock &MBB, int FI,
+                             MachineMemOperand::Flags Flags) const {
+  MachineFunction &MF = *MBB.getParent();
+  MachineFrameInfo &MFI = MF.getFrameInfo();
+
+  return MF.getMachineMemOperand(MachinePointerInfo::getFixedStack(MF, FI),
+                                 Flags, MFI.getObjectSize(FI),
+                                 MFI.getObjectAlign(FI));
+}
+
+//===----------------------------------------------------------------------===//
+// Branch Analysis
+//===----------------------------------------------------------------------===//
+
+void LoongArchInstrInfo::AnalyzeCondBr(const MachineInstr *Inst, unsigned Opc,
+                                  MachineBasicBlock *&BB,
+                                  SmallVectorImpl<MachineOperand> &Cond) const {
+  assert(getAnalyzableBrOpc(Opc) && "Not an analyzable branch");
+  int NumOp = Inst->getNumExplicitOperands();
+
+  // for both int and fp branches, the last explicit operand is the
+  // MBB.
+  BB = Inst->getOperand(NumOp-1).getMBB();
+  Cond.push_back(MachineOperand::CreateImm(Opc));
+
+  for (int i = 0; i < NumOp-1; i++)
+    Cond.push_back(Inst->getOperand(i));
+}
+
+bool LoongArchInstrInfo::analyzeBranch(MachineBasicBlock &MBB,
+                                  MachineBasicBlock *&TBB,
+                                  MachineBasicBlock *&FBB,
+                                  SmallVectorImpl<MachineOperand> &Cond,
+                                  bool AllowModify) const {
+  SmallVector<MachineInstr*, 2> BranchInstrs;
+  BranchType BT = analyzeBranch(MBB, TBB, FBB, Cond, AllowModify, BranchInstrs);
+
+  return (BT == BT_None) || (BT == BT_Indirect);
+}
+
+MachineInstr *
+LoongArchInstrInfo::BuildCondBr(MachineBasicBlock &MBB, MachineBasicBlock *TBB,
+                                const DebugLoc &DL,
+                                ArrayRef<MachineOperand> Cond) const {
+  unsigned Opc = Cond[0].getImm();
+  const MCInstrDesc &MCID = get(Opc);
+  MachineInstrBuilder MIB = BuildMI(&MBB, DL, MCID);
+
+  for (unsigned i = 1; i < Cond.size(); ++i) {
+    assert((Cond[i].isImm() || Cond[i].isReg()) &&
+           "Cannot copy operand for conditional branch!");
+    MIB.add(Cond[i]);
+  }
+  MIB.addMBB(TBB);
+  return MIB.getInstr();
+}
+
+unsigned LoongArchInstrInfo::insertBranch(MachineBasicBlock &MBB,
+                                          MachineBasicBlock *TBB,
+                                          MachineBasicBlock *FBB,
+                                          ArrayRef<MachineOperand> Cond,
+                                          const DebugLoc &DL,
+                                          int *BytesAdded) const {
+  unsigned UncondBrOpc = LoongArch::B;
+  // Shouldn't be a fall through.
+  assert(TBB && "insertBranch must not be told to insert a fallthrough");
+  if (BytesAdded)
+    *BytesAdded = 0;
+
+  // # of condition operands:
+  //  Unconditional branches: 0
+  //  Floating point branches: 1 (opc)
+  //  Int BranchZero: 2 (opc, reg)
+  //  Int Branch: 3 (opc, reg0, reg1)
+  assert((Cond.size() <= 3) &&
+         "# of LoongArch branch conditions must be <= 3!");
+
+  // Two-way Conditional branch.
+  if (FBB) {
+    MachineInstr &MI1 = *BuildCondBr(MBB, TBB, DL, Cond);
+    if (BytesAdded)
+      *BytesAdded += getInstSizeInBytes(MI1);
+    MachineInstr &MI2 = *BuildMI(&MBB, DL, get(UncondBrOpc)).addMBB(FBB);
+    if (BytesAdded)
+      *BytesAdded += getInstSizeInBytes(MI2);
+    return 2;
+  }
+
+  // One way branch.
+  // Unconditional branch.
+  if (Cond.empty()) {
+    MachineInstr &MI = *BuildMI(&MBB, DL, get(UncondBrOpc)).addMBB(TBB);
+    if (BytesAdded)
+      *BytesAdded += getInstSizeInBytes(MI);
+  }
+  else {// Conditional branch.
+    MachineInstr &MI = *BuildCondBr(MBB, TBB, DL, Cond);
+    if (BytesAdded)
+      *BytesAdded += getInstSizeInBytes(MI);
+  }
+  return 1;
+}
+
+unsigned LoongArchInstrInfo::insertIndirectBranch(MachineBasicBlock &MBB,
+                                                  MachineBasicBlock &DestBB,
+                                                  const DebugLoc &DL,
+                                                  int64_t BrOffset,
+                                                  RegScavenger *RS) const {
+  assert(RS && "RegScavenger required for long branching");
+  assert(MBB.empty() &&
+         "new block should be inserted for expanding unconditional branch");
+  assert(MBB.pred_size() == 1);
+
+  MachineFunction *MF = MBB.getParent();
+  MachineRegisterInfo &MRI = MF->getRegInfo();
+  const LoongArchSubtarget &Subtarget = MF->getSubtarget<LoongArchSubtarget>();
+  bool is64 = Subtarget.isABI_LP64D();
+  const TargetRegisterClass *RC =
+    is64 ? &LoongArch::GPR64RegClass : &LoongArch::GPR32RegClass;
+
+  if (!is64 && !isInt<32>(BrOffset))
+    report_fatal_error(
+        "Branch offsets outside of the signed 32-bit range not supported");
+
+  unsigned ScratchReg = MRI.createVirtualRegister(RC);
+  unsigned ZeroReg = is64 ? LoongArch::ZERO_64 : LoongArch::ZERO;
+  auto II = MBB.end();
+
+  MachineInstr &Pcaddu12iMI =
+      *BuildMI(MBB, II, DL, get(LoongArch::LONG_BRANCH_PCADDU12I), ScratchReg)
+          .addMBB(&DestBB, LoongArchII::MO_PCREL_HI);
+  BuildMI(MBB, II, DL, get(LoongArch::LONG_BRANCH_ADDID2Op), ScratchReg)
+      .addReg(ScratchReg)
+      .addMBB(&DestBB, LoongArchII::MO_PCREL_LO);
+  BuildMI(MBB, II, DL, get(LoongArch::JIRL))
+      .addReg(ZeroReg)
+      .addReg(ScratchReg, RegState::Kill)
+      .addImm(0);
+  RS->enterBasicBlockEnd(MBB);
+  unsigned Scav = RS->scavengeRegisterBackwards(
+      *RC, MachineBasicBlock::iterator(Pcaddu12iMI), false, 0);
+  MRI.replaceRegWith(ScratchReg, Scav);
+  MRI.clearVirtRegs();
+  RS->setRegUsed(Scav);
+
+  return 12;
+}
+
+unsigned LoongArchInstrInfo::removeBranch(MachineBasicBlock &MBB,
+                                          int *BytesRemoved) const {
+  if (BytesRemoved)
+    *BytesRemoved = 0;
+
+  MachineBasicBlock::reverse_iterator I = MBB.rbegin(), REnd = MBB.rend();
+  unsigned removed = 0;
+
+  // Up to 2 branches are removed.
+  // Note that indirect branches are not removed.
+  while (I != REnd && removed < 2) {
+    // Skip past debug instructions.
+    if (I->isDebugInstr()) {
+      ++I;
+      continue;
+    }
+    if (!getAnalyzableBrOpc(I->getOpcode()))
+      break;
+    // Remove the branch.
+    I->eraseFromParent();
+    if (BytesRemoved)
+      *BytesRemoved += getInstSizeInBytes(*I);
+    I = MBB.rbegin();
+    ++removed;
+  }
+
+  return removed;
+}
+
+/// reverseBranchCondition - Return the inverse opcode of the
+/// specified Branch instruction.
+bool LoongArchInstrInfo::reverseBranchCondition(
+    SmallVectorImpl<MachineOperand> &Cond) const {
+  assert( (Cond.size() && Cond.size() <= 3) &&
+          "Invalid LoongArch branch condition!");
+  Cond[0].setImm(getOppositeBranchOpc(Cond[0].getImm()));
+  return false;
+}
+
+LoongArchInstrInfo::BranchType LoongArchInstrInfo::analyzeBranch(
+    MachineBasicBlock &MBB, MachineBasicBlock *&TBB, MachineBasicBlock *&FBB,
+    SmallVectorImpl<MachineOperand> &Cond, bool AllowModify,
+    SmallVectorImpl<MachineInstr *> &BranchInstrs) const {
+  MachineBasicBlock::reverse_iterator I = MBB.rbegin(), REnd = MBB.rend();
+
+  // Skip all the debug instructions.
+  while (I != REnd && I->isDebugInstr())
+    ++I;
+
+  if (I == REnd || !isUnpredicatedTerminator(*I)) {
+    // This block ends with no branches (it just falls through to its succ).
+    // Leave TBB/FBB null.
+    TBB = FBB = nullptr;
+    return BT_NoBranch;
+  }
+
+  MachineInstr *LastInst = &*I;
+  unsigned LastOpc = LastInst->getOpcode();
+  BranchInstrs.push_back(LastInst);
+
+  // Not an analyzable branch (e.g., indirect jump).
+  if (!getAnalyzableBrOpc(LastOpc))
+    return LastInst->isIndirectBranch() ? BT_Indirect : BT_None;
+
+  // Get the second to last instruction in the block.
+  unsigned SecondLastOpc = 0;
+  MachineInstr *SecondLastInst = nullptr;
+
+  // Skip past any debug instruction to see if the second last actual
+  // is a branch.
+  ++I;
+  while (I != REnd && I->isDebugInstr())
+    ++I;
+
+  if (I != REnd) {
+    SecondLastInst = &*I;
+    SecondLastOpc = getAnalyzableBrOpc(SecondLastInst->getOpcode());
+
+    // Not an analyzable branch (must be an indirect jump).
+    if (isUnpredicatedTerminator(*SecondLastInst) && !SecondLastOpc)
+      return BT_None;
+  }
+
+  // If there is only one terminator instruction, process it.
+  if (!SecondLastOpc) {
+    // Unconditional branch.
+    if (LastInst->isUnconditionalBranch()) {
+      TBB = LastInst->getOperand(0).getMBB();
+      return BT_Uncond;
+    }
+
+    // Conditional branch
+    AnalyzeCondBr(LastInst, LastOpc, TBB, Cond);
+    return BT_Cond;
+  }
+
+  // If we reached here, there are two branches.
+  // If there are three terminators, we don't know what sort of block this is.
+  if (++I != REnd && isUnpredicatedTerminator(*I))
+    return BT_None;
+
+  BranchInstrs.insert(BranchInstrs.begin(), SecondLastInst);
+
+  // If second to last instruction is an unconditional branch,
+  // analyze it and remove the last instruction.
+  if (SecondLastInst->isUnconditionalBranch()) {
+    // Return if the last instruction cannot be removed.
+    if (!AllowModify)
+      return BT_None;
+
+    TBB = SecondLastInst->getOperand(0).getMBB();
+    LastInst->eraseFromParent();
+    BranchInstrs.pop_back();
+    return BT_Uncond;
+  }
+
+  // Conditional branch followed by an unconditional branch.
+  // The last one must be unconditional.
+  if (!LastInst->isUnconditionalBranch())
+    return BT_None;
+
+  AnalyzeCondBr(SecondLastInst, SecondLastOpc, TBB, Cond);
+  FBB = LastInst->getOperand(0).getMBB();
+
+  return BT_CondUncond;
+}
+
+MachineBasicBlock *
+LoongArchInstrInfo::getBranchDestBlock(const MachineInstr &MI) const {
+  assert(MI.getDesc().isBranch() && "Unexpected opcode!");
+  // The branch target is always the last operand.
+  int NumOp = MI.getNumExplicitOperands();
+  return MI.getOperand(NumOp - 1).getMBB();
+}
+
+bool LoongArchInstrInfo::isBranchOffsetInRange(unsigned BranchOpc, int64_t BrOffset) const {
+/*
+      	switch (BranchOpc) {
+  case LoongArch::B:
+  case LoongArch::BAL:
+  case LoongArch::BAL_BR:
+  case LoongArch::BC1F:
+  case LoongArch::BC1FL:
+  case LoongArch::BC1T:
+  case LoongArch::BC1TL:
+  case LoongArch::BEQ:     case LoongArch::BEQ64:
+  case LoongArch::BEQL:
+  case LoongArch::BGEZ:    case LoongArch::BGEZ64:
+  case LoongArch::BGEZL:
+  case LoongArch::BGEZAL:
+  case LoongArch::BGEZALL:
+  case LoongArch::BGTZ:    case LoongArch::BGTZ64:
+  case LoongArch::BGTZL:
+  case LoongArch::BLEZ:    case LoongArch::BLEZ64:
+  case LoongArch::BLEZL:
+  case LoongArch::BLTZ:    case LoongArch::BLTZ64:
+  case LoongArch::BLTZL:
+  case LoongArch::BLTZAL:
+  case LoongArch::BLTZALL:
+  case LoongArch::BNE:     case LoongArch::BNE64:
+  case LoongArch::BNEL:
+    return isInt<18>(BrOffset);
+
+  case LoongArch::BC1EQZ:
+  case LoongArch::BC1NEZ:
+  case LoongArch::BC2EQZ:
+  case LoongArch::BC2NEZ:
+  case LoongArch::BEQC:   case LoongArch::BEQC64:
+  case LoongArch::BNEC:   case LoongArch::BNEC64:
+  case LoongArch::BGEC:   case LoongArch::BGEC64:
+  case LoongArch::BGEUC:  case LoongArch::BGEUC64:
+  case LoongArch::BGEZC:  case LoongArch::BGEZC64:
+  case LoongArch::BGTZC:  case LoongArch::BGTZC64:
+  case LoongArch::BLEZC:  case LoongArch::BLEZC64:
+  case LoongArch::BLTC:   case LoongArch::BLTC64:
+  case LoongArch::BLTUC:  case LoongArch::BLTUC64:
+  case LoongArch::BLTZC:  case LoongArch::BLTZC64:
+  case LoongArch::BNVC:
+  case LoongArch::BOVC:
+  case LoongArch::BGEZALC:
+  case LoongArch::BEQZALC:
+  case LoongArch::BGTZALC:
+  case LoongArch::BLEZALC:
+  case LoongArch::BLTZALC:
+  case LoongArch::BNEZALC:
+    return isInt<18>(BrOffset);
+
+  case LoongArch::BEQZC:  case LoongArch::BEQZC64:
+  case LoongArch::BNEZC:  case LoongArch::BNEZC64:
+    return isInt<23>(BrOffset);
+  }
+    */
+  switch (BranchOpc) {
+  case LoongArch::B: case LoongArch::B32:
+    return isInt<28>(BrOffset);
+
+  case LoongArch::BEQZ: case LoongArch::BEQZ32:
+  case LoongArch::BNEZ: case LoongArch::BNEZ32:
+  case LoongArch::BCEQZ:
+  case LoongArch::BCNEZ:
+    return isInt<23>(BrOffset);
+
+  case LoongArch::BEQ: case LoongArch::BEQ32:
+  case LoongArch::BNE: case LoongArch::BNE32:
+  case LoongArch::BLT: case LoongArch::BLT32:
+  case LoongArch::BGE: case LoongArch::BGE32:
+  case LoongArch::BLTU: case LoongArch::BLTU32:
+  case LoongArch::BGEU: case LoongArch::BGEU32:
+    return isInt<18>(BrOffset);
+  }
+
+  llvm_unreachable("Unknown branch instruction!");
+}
+
+
+/// Predicate for distingushing between control transfer instructions and all
+/// other instructions for handling forbidden slots. Consider inline assembly
+/// as unsafe as well.
+bool LoongArchInstrInfo::SafeInForbiddenSlot(const MachineInstr &MI) const {
+  if (MI.isInlineAsm())
+    return false;
+
+  return (MI.getDesc().TSFlags & LoongArchII::IsCTI) == 0;
+}
+
+/// Predicate for distingushing instructions that have forbidden slots.
+bool LoongArchInstrInfo::HasForbiddenSlot(const MachineInstr &MI) const {
+  return (MI.getDesc().TSFlags & LoongArchII::HasForbiddenSlot) != 0;
+}
+
+/// Return the number of bytes of code the specified instruction may be.
+unsigned LoongArchInstrInfo::getInstSizeInBytes(const MachineInstr &MI) const {
+  switch (MI.getOpcode()) {
+  default:
+    return MI.getDesc().getSize();
+  case  TargetOpcode::INLINEASM: {       // Inline Asm: Variable size.
+    const MachineFunction *MF = MI.getParent()->getParent();
+    const char *AsmStr = MI.getOperand(0).getSymbolName();
+    return getInlineAsmLength(AsmStr, *MF->getTarget().getMCAsmInfo());
+  }
+  }
+}
+
+MachineInstrBuilder
+LoongArchInstrInfo::genInstrWithNewOpc(unsigned NewOpc,
+                                  MachineBasicBlock::iterator I) const {
+  MachineInstrBuilder MIB;
+
+  int ZeroOperandPosition = -1;
+  bool BranchWithZeroOperand = false;
+  if (I->isBranch() && !I->isPseudo()) {
+    auto TRI = I->getParent()->getParent()->getSubtarget().getRegisterInfo();
+    ZeroOperandPosition = I->findRegisterUseOperandIdx(LoongArch::ZERO, false, TRI);
+    BranchWithZeroOperand = ZeroOperandPosition != -1;
+  }
+
+  MIB = BuildMI(*I->getParent(), I, I->getDebugLoc(), get(NewOpc));
+
+  if (NewOpc == LoongArch::JIRL) {
+    MIB->RemoveOperand(0);
+    for (unsigned J = 0, E = I->getDesc().getNumOperands(); J < E; ++J) {
+      MIB.add(I->getOperand(J));
+    }
+    MIB.addImm(0);
+  } else {
+    for (unsigned J = 0, E = I->getDesc().getNumOperands(); J < E; ++J) {
+      if (BranchWithZeroOperand && (unsigned)ZeroOperandPosition == J)
+        continue;
+
+      MIB.add(I->getOperand(J));
+    }
+  }
+
+  MIB.copyImplicitOps(*I);
+  MIB.cloneMemRefs(*I);
+  return MIB;
+}
+
+bool LoongArchInstrInfo::findCommutedOpIndices(const MachineInstr &MI,
+                                               unsigned &SrcOpIdx1,
+                                               unsigned &SrcOpIdx2) const {
+  assert(!MI.isBundle() &&
+         "TargetInstrInfo::findCommutedOpIndices() can't handle bundles");
+
+  const MCInstrDesc &MCID = MI.getDesc();
+  if (!MCID.isCommutable())
+    return false;
+
+  return TargetInstrInfo::findCommutedOpIndices(MI, SrcOpIdx1, SrcOpIdx2);
+}
+
+// bstrins, bstrpick have the following constraints:
+// 0 <= lsb <= msb <= High
+static bool verifyBstrInstruction(const MachineInstr &MI, StringRef &ErrInfo,
+                                  const int64_t High) {
+  MachineOperand MOMsb = MI.getOperand(2);
+  if (!MOMsb.isImm()) {
+    ErrInfo = "Msb operand is not an immediate!";
+    return false;
+  }
+  MachineOperand MOLsb = MI.getOperand(3);
+  if (!MOLsb.isImm()) {
+    ErrInfo = "Lsb operand is not an immediate!";
+    return false;
+  }
+
+  int64_t Lsb = MOLsb.getImm();
+  if (!((0 <= Lsb) && (Lsb <= High))) {
+    ErrInfo = "Lsb operand is out of range!";
+    return false;
+  }
+
+  int64_t Msb = MOMsb.getImm();
+  if (!((0 <= Msb) && (Msb <= High))) {
+    ErrInfo = "Msb operand is out of range!";
+    return false;
+  }
+
+  if (!(Lsb <= Msb)) {
+    ErrInfo = "Lsb operand is not less than or equal to msb operand!";
+    return false;
+  }
+
+  return true;
+}
+
+//  Perform target specific instruction verification.
+bool LoongArchInstrInfo::verifyInstruction(const MachineInstr &MI,
+                                      StringRef &ErrInfo) const {
+  // Verify that bstrins and bstrpick instructions are well formed.
+   switch (MI.getOpcode()) {
+    case LoongArch::BSTRINS_W:
+    case LoongArch::BSTRPICK_W:
+      return verifyBstrInstruction(MI, ErrInfo, 31);
+    case LoongArch::BSTRINS_D:
+    case LoongArch::BSTRPICK_D:
+      return verifyBstrInstruction(MI, ErrInfo, 63);
+    default:
+      return true;
+  }
+
+  return true;
+}
+
+std::pair<unsigned, unsigned>
+LoongArchInstrInfo::decomposeMachineOperandsTargetFlags(unsigned TF) const {
+  return std::make_pair(TF, 0u);
+}
+
+ArrayRef<std::pair<unsigned, const char*>>
+LoongArchInstrInfo::getSerializableDirectMachineOperandTargetFlags() const {
+ using namespace LoongArchII;
+
+ static const std::pair<unsigned, const char*> Flags[] = {
+    {MO_PCREL_HI,        "larch-pcrel-hi"},
+    {MO_PCREL_LO,        "larch-pcrel-lo"},
+    {MO_TLSGD_HI,        "larch-tlsgd-hi"},
+    {MO_TLSGD_LO,        "larch-tlsgd-lo"},
+    {MO_TLSIE_HI,        "larch-tlsie-hi"},
+    {MO_TLSIE_LO,        "larch-tlsie-lo"},
+    {MO_TLSLE_HI,        "larch-tlsle-hi"},
+    {MO_TLSLE_LO,        "larch-tlsle-lo"},
+    {MO_ABS_HI,          "larch-abs-hi"},
+    {MO_ABS_LO,          "larch-abs-lo"},
+    {MO_ABS_HIGHER,      "larch-abs-higher"},
+    {MO_ABS_HIGHEST,     "larch-abs-highest"},
+    {MO_GOT_HI,          "larch-got-hi"},
+    {MO_GOT_LO,          "larch-got-lo"},
+    {MO_CALL_HI,         "larch-call-hi"},
+    {MO_CALL_LO,         "larch-call-lo"}
+  };
+  return makeArrayRef(Flags);
+}
diff --git a/llvm/lib/Target/LoongArch/LoongArchInstrInfo.h b/llvm/lib/Target/LoongArch/LoongArchInstrInfo.h
new file mode 100644
index 000000000000..cae20d5f60e7
--- /dev/null
+++ b/llvm/lib/Target/LoongArch/LoongArchInstrInfo.h
@@ -0,0 +1,244 @@
+//===- LoongArchInstrInfo.h - LoongArch Instruction Information -----------*- C++ -*-===//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+//
+// This file contains the LoongArch implementation of the TargetInstrInfo class.
+//
+// FIXME: We need to override TargetInstrInfo::getInlineAsmLength method in
+// order for LoongArchLongBranch pass to work correctly when the code has inline
+// assembly.  The returned value doesn't have to be the asm instruction's exact
+// size in bytes; LoongArchLongBranch only expects it to be the correct upper bound.
+//===----------------------------------------------------------------------===//
+
+#ifndef LLVM_LIB_TARGET_LOONGARCH_LOONGARCHINSTRINFO_H
+#define LLVM_LIB_TARGET_LOONGARCH_LOONGARCHINSTRINFO_H
+
+#include "MCTargetDesc/LoongArchMCTargetDesc.h"
+#include "LoongArch.h"
+#include "LoongArchRegisterInfo.h"
+#include "llvm/ADT/ArrayRef.h"
+#include "llvm/CodeGen/MachineBasicBlock.h"
+#include "llvm/CodeGen/MachineInstrBuilder.h"
+#include "llvm/CodeGen/MachineMemOperand.h"
+#include "llvm/CodeGen/TargetInstrInfo.h"
+#include <cstdint>
+
+#define GET_INSTRINFO_HEADER
+#include "LoongArchGenInstrInfo.inc"
+
+namespace llvm {
+
+class MachineInstr;
+class MachineOperand;
+class LoongArchSubtarget;
+class TargetRegisterClass;
+class TargetRegisterInfo;
+
+class LoongArchInstrInfo : public LoongArchGenInstrInfo {
+  virtual void anchor();
+  const LoongArchRegisterInfo RI;
+  const LoongArchSubtarget &Subtarget;
+
+public:
+  enum BranchType {
+    BT_None,       // Couldn't analyze branch.
+    BT_NoBranch,   // No branches found.
+    BT_Uncond,     // One unconditional branch.
+    BT_Cond,       // One conditional branch.
+    BT_CondUncond, // A conditional branch followed by an unconditional branch.
+    BT_Indirect    // One indirct branch.
+  };
+
+  explicit LoongArchInstrInfo(const LoongArchSubtarget &STI);
+
+  /// isLoadFromStackSlot - If the specified machine instruction is a direct
+  /// load from a stack slot, return the virtual or physical register number of
+  /// the destination along with the FrameIndex of the loaded stack slot.  If
+  /// not, return 0.  This predicate must return 0 if the instruction has
+  /// any side effects other than loading from the stack slot.
+  unsigned isLoadFromStackSlot(const MachineInstr &MI,
+                               int &FrameIndex) const override;
+
+  /// isStoreToStackSlot - If the specified machine instruction is a direct
+  /// store to a stack slot, return the virtual or physical register number of
+  /// the source reg along with the FrameIndex of the loaded stack slot.  If
+  /// not, return 0.  This predicate must return 0 if the instruction has
+  /// any side effects other than storing to the stack slot.
+  unsigned isStoreToStackSlot(const MachineInstr &MI,
+                              int &FrameIndex) const override;
+
+  void copyPhysReg(MachineBasicBlock &MBB, MachineBasicBlock::iterator I,
+                   const DebugLoc &DL, MCRegister DestReg, MCRegister SrcReg,
+                   bool KillSrc) const override;
+
+  /// Branch Analysis
+  bool analyzeBranch(MachineBasicBlock &MBB, MachineBasicBlock *&TBB,
+                     MachineBasicBlock *&FBB,
+                     SmallVectorImpl<MachineOperand> &Cond,
+                     bool AllowModify) const override;
+
+  unsigned removeBranch(MachineBasicBlock &MBB,
+                        int *BytesRemoved = nullptr) const override;
+
+  unsigned insertBranch(MachineBasicBlock &MBB, MachineBasicBlock *TBB,
+                        MachineBasicBlock *FBB, ArrayRef<MachineOperand> Cond,
+                        const DebugLoc &DL,
+                        int *BytesAdded = nullptr) const override;
+
+  unsigned insertIndirectBranch(MachineBasicBlock &MBB,
+                                MachineBasicBlock &NewDestBB,
+                                const DebugLoc &DL, int64_t BrOffset,
+                                RegScavenger *RS = nullptr) const override;
+  bool
+  reverseBranchCondition(SmallVectorImpl<MachineOperand> &Cond) const override;
+
+  BranchType analyzeBranch(MachineBasicBlock &MBB, MachineBasicBlock *&TBB,
+                           MachineBasicBlock *&FBB,
+                           SmallVectorImpl<MachineOperand> &Cond,
+                           bool AllowModify,
+                           SmallVectorImpl<MachineInstr *> &BranchInstrs) const;
+
+  /// Get the block that branch instruction jumps to.
+  MachineBasicBlock *getBranchDestBlock(const MachineInstr &MI) const override;
+
+  /// Determine if the branch target is in range.
+  bool isBranchOffsetInRange(unsigned BranchOpc,
+                             int64_t BrOffset) const override;
+
+  /// Predicate to determine if an instruction can go in a forbidden slot.
+  bool SafeInForbiddenSlot(const MachineInstr &MI) const;
+
+  /// Predicate to determine if an instruction has a forbidden slot.
+  bool HasForbiddenSlot(const MachineInstr &MI) const;
+
+  /// Insert nop instruction when hazard condition is found
+  void insertNoop(MachineBasicBlock &MBB,
+                  MachineBasicBlock::iterator MI) const override;
+
+  /// getRegisterInfo - TargetInstrInfo is a superset of MRegister info.  As
+  /// such, whenever a client has an instance of instruction info, it should
+  /// always be able to get register info as well (through this method).
+  const LoongArchRegisterInfo &getRegisterInfo() const;
+
+  bool expandPostRAPseudo(MachineInstr &MI) const override;
+
+  unsigned getOppositeBranchOpc(unsigned Opc) const;
+
+  /// Emit a series of instructions to load an immediate. If NewImm is a
+  /// non-NULL parameter, the last instruction is not emitted, but instead
+  /// its immediate operand is returned in NewImm.
+  unsigned loadImmediate(int64_t Imm, MachineBasicBlock &MBB,
+                         MachineBasicBlock::iterator II, const DebugLoc &DL,
+                         unsigned *NewImm) const;
+
+  /// Return the number of bytes of code the specified instruction may be.
+  unsigned getInstSizeInBytes(const MachineInstr &MI) const override;
+
+  void storeRegToStackSlot(MachineBasicBlock &MBB,
+                           MachineBasicBlock::iterator MBBI,
+                           Register SrcReg, bool isKill, int FrameIndex,
+                           const TargetRegisterClass *RC,
+                           const TargetRegisterInfo *TRI) const override {
+    storeRegToStack(MBB, MBBI, SrcReg, isKill, FrameIndex, RC, TRI, 0);
+  }
+
+  void loadRegFromStackSlot(MachineBasicBlock &MBB,
+                            MachineBasicBlock::iterator MBBI,
+                            Register DestReg, int FrameIndex,
+                            const TargetRegisterClass *RC,
+                            const TargetRegisterInfo *TRI) const override {
+    loadRegFromStack(MBB, MBBI, DestReg, FrameIndex, RC, TRI, 0);
+  }
+
+  void storeRegToStack(MachineBasicBlock &MBB,
+                       MachineBasicBlock::iterator MI,
+                       Register SrcReg, bool isKill, int FrameIndex,
+                       const TargetRegisterClass *RC,
+                       const TargetRegisterInfo *TRI,
+                       int64_t Offset) const;
+
+  void loadRegFromStack(MachineBasicBlock &MBB,
+                        MachineBasicBlock::iterator MI,
+                        Register DestReg, int FrameIndex,
+                        const TargetRegisterClass *RC,
+                        const TargetRegisterInfo *TRI,
+                        int64_t Offset) const;
+
+  /// Adjust SP by Amount bytes.
+  void adjustStackPtr(unsigned SP, int64_t Amount,
+                      MachineBasicBlock &MBB,
+                      MachineBasicBlock::iterator I) const;
+
+  /// Create an instruction which has the same operands and memory operands
+  /// as MI but has a new opcode.
+  MachineInstrBuilder genInstrWithNewOpc(unsigned NewOpc,
+                                         MachineBasicBlock::iterator I) const;
+
+  bool findCommutedOpIndices(const MachineInstr &MI, unsigned &SrcOpIdx1,
+                             unsigned &SrcOpIdx2) const override;
+
+  /// Perform target specific instruction verification.
+  bool verifyInstruction(const MachineInstr &MI,
+                         StringRef &ErrInfo) const override;
+
+  std::pair<unsigned, unsigned>
+  decomposeMachineOperandsTargetFlags(unsigned TF) const override;
+
+  ArrayRef<std::pair<unsigned, const char *>>
+  getSerializableDirectMachineOperandTargetFlags() const override;
+
+protected:
+  /// If the specific machine instruction is a instruction that moves/copies
+  /// value from one register to another register return true along with
+  /// @Source machine operand and @Destination machine operand.
+  Optional<DestSourcePair>
+  isCopyInstrImpl(const MachineInstr &MI) const override;
+
+private:
+
+  bool isZeroImm(const MachineOperand &op) const;
+
+  MachineMemOperand *GetMemOperand(MachineBasicBlock &MBB, int FI,
+                                   MachineMemOperand::Flags Flags) const;
+
+  unsigned getAnalyzableBrOpc(unsigned Opc) const;
+
+  void AnalyzeCondBr(const MachineInstr *Inst, unsigned Opc,
+                     MachineBasicBlock *&BB,
+                     SmallVectorImpl<MachineOperand> &Cond) const;
+
+  MachineInstr *
+  BuildCondBr(MachineBasicBlock &MBB, MachineBasicBlock *TBB,
+              const DebugLoc &DL, ArrayRef<MachineOperand> Cond) const;
+
+  void expandRetRA(MachineBasicBlock &MBB, MachineBasicBlock::iterator I) const;
+
+  void expandERet(MachineBasicBlock &MBB, MachineBasicBlock::iterator I) const;
+
+  std::pair<bool, bool> compareOpndSize(unsigned Opc,
+                                        const MachineFunction &MF) const;
+
+  /// Expand pseudo Int-to-FP conversion instructions.
+  ///
+  /// For example, the following pseudo instruction
+  ///  PseudoCVT_D32_W D2, A5
+  /// gets expanded into these two instructions:
+  ///  MTC1 F4, A5
+  ///  CVT_D32_W D2, F4
+  ///
+  /// We do this expansion post-RA to avoid inserting a floating point copy
+  /// instruction between MTC1 and CVT_D32_W.
+  void expandCvtFPInt(MachineBasicBlock &MBB, MachineBasicBlock::iterator I,
+                      unsigned CvtOpc, unsigned MovOpc, bool IsI64) const;
+
+  void expandEhReturn(MachineBasicBlock &MBB,
+                      MachineBasicBlock::iterator I) const;
+};
+
+} // end namespace llvm
+
+#endif // LLVM_LIB_TARGET_LOONGARCH_LOONGARCHINSTRINFO_H
diff --git a/llvm/lib/Target/LoongArch/LoongArchInstrInfo.td b/llvm/lib/Target/LoongArch/LoongArchInstrInfo.td
new file mode 100644
index 000000000000..84a09a361693
--- /dev/null
+++ b/llvm/lib/Target/LoongArch/LoongArchInstrInfo.td
@@ -0,0 +1,1770 @@
+//===- LoongArchInstrInfo.td - Target Description for LoongArch Target -*- tablegen -*-=//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+//
+// This file contains the LoongArch implementation of the TargetInstrInfo class.
+//
+//===----------------------------------------------------------------------===//
+include "LoongArchInstrFormats.td"
+
+def SDT_Bstrpick : SDTypeProfile<1, 3, [SDTCisInt<0>, SDTCisSameAs<0, 1>,
+                                   SDTCisVT<2, i32>, SDTCisSameAs<2, 3>]>;
+def SDT_Bstrins : SDTypeProfile<1, 4, [SDTCisInt<0>, SDTCisSameAs<0, 1>,
+                                   SDTCisVT<2, i32>, SDTCisSameAs<2, 3>,
+                                   SDTCisSameAs<0, 4>]>;
+
+def LoongArchBstrpick :  SDNode<"LoongArchISD::BSTRPICK", SDT_Bstrpick>;
+
+def LoongArchBstrins :  SDNode<"LoongArchISD::BSTRINS", SDT_Bstrins>;
+
+def SDT_DBAR             : SDTypeProfile<0, 1, [SDTCisVT<0, i32>]>;
+def LoongArchDBAR : SDNode<"LoongArchISD::DBAR", SDT_DBAR, [SDNPHasChain,SDNPSideEffect]>;
+
+def SDT_LoongArchEHRET : SDTypeProfile<0, 2, [SDTCisInt<0>, SDTCisPtrTy<1>]>;
+
+def LoongArchehret : SDNode<"LoongArchISD::EH_RETURN", SDT_LoongArchEHRET,
+                      [SDNPHasChain, SDNPOptInGlue, SDNPVariadic]>;
+
+//===---------------------------------------------------------------------===/
+// Operand, Complex Patterns and Transformations Definitions.
+//===---------------------------------------------------------------------===/
+
+def assertzext_lt_i32 : PatFrag<(ops node:$src), (assertzext node:$src), [{
+  return cast<VTSDNode>(N->getOperand(1))->getVT().bitsLT(MVT::i32);
+}]>;
+
+def immz : PatLeaf<(imm), [{ return N->getSExtValue() == 0; }]>;
+def immZExt12 : PatLeaf<(imm), [{ return isUInt<12>(N->getZExtValue()); }]>;
+def immSExt12 : PatLeaf<(imm), [{ return isInt<12>(N->getSExtValue()); }]>;
+def immSExt13 : PatLeaf<(imm), [{ return isInt<13>(N->getSExtValue()); }]>;
+
+def immZExt2Alsl : ImmLeaf<i32, [{return isUInt<2>(Imm - 1);}]>;
+//class ImmAsmOperand<int Low, int High> : AsmOperandClass {
+//  let RenderMethod = "addImmOperands";
+//  let PredicateMethod = "isImmediate<" # Low # "," # High # ">";
+//  let DiagnosticString = "operand must be an immediate in the range [" # Low # "," # High # "]";
+//}
+//
+//def Imm8AsmOperand: ImmAsmOperand<8,8> { let Name = "Imm8"; }
+//def imm8 : Operand<i64>, ImmLeaf<i64, [{ return Imm == 8; }]> {
+//  let ParserMatchClass = Imm8AsmOperand;
+//}
+
+class SImmOperand<int width> : AsmOperandClass {
+  let Name = "SImm" # width;
+  let DiagnosticType = "InvalidSImm" # width;
+  let RenderMethod = "addImmOperands";
+  let PredicateMethod = "isSImm<" # width # ">";
+}
+
+def SImm2Operand : SImmOperand<2>;
+def simm2 : Operand<i64>, ImmLeaf<i64, [{ return Imm >= -2 && Imm < 2; }]> {
+  let ParserMatchClass = SImm2Operand;
+  let DecoderMethod = "DecodeSImmWithOffsetAndScale<2>";
+}
+def SImm3Operand : SImmOperand<3>;
+def simm3 : Operand<i64>, ImmLeaf<i64, [{ return Imm >= -4 && Imm < 4; }]> {
+  let ParserMatchClass = SImm3Operand;
+  let DecoderMethod = "DecodeSImmWithOffsetAndScale<3>";
+}
+
+def SImm5Operand : SImmOperand<5>;
+def simm5 : Operand<i64>, ImmLeaf<i64, [{ return Imm >= -16 && Imm < 16; }]> {
+  let ParserMatchClass = SImm5Operand;
+  let DecoderMethod = "DecodeSImmWithOffsetAndScale<5>";
+}
+
+def simm5_32 : Operand<i32>, ImmLeaf<i32, [{ return Imm >= -16 && Imm < 16; }]> {
+  let ParserMatchClass = SImm5Operand;
+  let DecoderMethod = "DecodeSImmWithOffsetAndScale<5>";
+}
+
+def SImm8Operand : SImmOperand<8>;
+def simm8 : Operand<i64>, ImmLeaf<i64, [{ return Imm >= -128 && Imm < 128; }]> {
+  let ParserMatchClass = SImm8Operand;
+  let DecoderMethod = "DecodeSImmWithOffsetAndScale<8>";
+}
+def simm8_32 : Operand<i32>, ImmLeaf<i32, [{ return Imm >= -128 && Imm < 128; }]> {
+  let ParserMatchClass = SImm8Operand;
+  let DecoderMethod = "DecodeSImmWithOffsetAndScale<8>";
+}
+
+def SImm12Operand : SImmOperand<12>;
+def simm12 : Operand<i64>, ImmLeaf<i64, [{ return Imm >= -2048 && Imm < 2048; }]> {
+  let ParserMatchClass = SImm12Operand;
+  let DecoderMethod = "DecodeSImmWithOffsetAndScale<12>";
+}
+def simm12_32 : Operand<i32>, ImmLeaf<i32, [{ return Imm >= -2048 && Imm < 2048; }]> {
+  let ParserMatchClass = SImm12Operand;
+  let DecoderMethod = "DecodeSImmWithOffsetAndScale<12>";
+}
+
+def SImm14Operand : SImmOperand<14>;
+def simm14 : Operand<i64>, ImmLeaf<i64, [{ return Imm >= -8192 && Imm < 8192; }]> {
+  let ParserMatchClass = SImm14Operand;
+  let DecoderMethod = "DecodeSImmWithOffsetAndScale<14>";
+}
+
+def SImm15Operand : SImmOperand<15>;
+def simm15 : Operand<i64>, ImmLeaf<i64, [{ return Imm >= -16384 && Imm < 16384; }]> {
+  let ParserMatchClass = SImm15Operand;
+  let DecoderMethod = "DecodeSImmWithOffsetAndScale<15>";
+}
+
+def SImm16Operand : SImmOperand<16>;
+def simm16 : Operand<i64>, ImmLeaf<i64, [{ return Imm >= -32768 && Imm < 32768; }]> {
+  let ParserMatchClass = SImm16Operand;
+  let DecoderMethod = "DecodeSImmWithOffsetAndScale<16>";
+}
+
+def SImm20Operand : SImmOperand<20>;
+def simm20 : Operand<i64>, ImmLeaf<i64, [{ return Imm >= -524288 && Imm < 524288; }]> {
+  let ParserMatchClass = SImm20Operand;
+  let DecoderMethod = "DecodeSImmWithOffsetAndScale<20>";
+}
+def simm20_32 : Operand<i32>, ImmLeaf<i32, [{ return Imm >= -524288 && Imm < 524288; }]> {
+  let ParserMatchClass = SImm20Operand;
+  let DecoderMethod = "DecodeSImmWithOffsetAndScale<20>";
+}
+
+def SImm21Operand : SImmOperand<21>;
+def simm21 : Operand<i64>, ImmLeaf<i64, [{ return Imm >= -1048576 && Imm < 1048576; }]> {
+  let ParserMatchClass = SImm21Operand;
+  let DecoderMethod = "DecodeSImmWithOffsetAndScale<21>";
+}
+
+def SImm26Operand : SImmOperand<26>;
+def simm26 : Operand<i64>, ImmLeaf<i64, [{ return Imm >= -33554432 && Imm < 33554432; }]> {
+  let ParserMatchClass = SImm26Operand;
+  let DecoderMethod = "DecodeSImmWithOffsetAndScale<26>";
+}
+
+def UImm2Operand : AsmOperandClass {
+  let Name = "UImm2";
+  let RenderMethod = "addUImmOperands<2>";
+  let PredicateMethod = "isUImm<2>";
+  let DiagnosticType = "InvalidImm0_3";
+}
+
+def UImm3Operand : AsmOperandClass {
+  let Name = "UImm3";
+  let RenderMethod = "addUImmOperands<3>";
+  let PredicateMethod = "isUImm<3>";
+  let DiagnosticType = "InvalidImm0_7";
+}
+
+def UImm5Operand : AsmOperandClass {
+  let Name = "UImm5";
+  let RenderMethod = "addUImmOperands<5>";
+  let PredicateMethod = "isUImm<5>";
+  let DiagnosticType = "InvalidImm0_31";
+}
+
+def uimm2 : Operand<i32>, ImmLeaf<i32, [{ return Imm >= 0 && Imm < 4; }]> {
+  let PrintMethod = "printUImm<2>";
+  let ParserMatchClass = UImm2Operand;
+}
+
+def uimm3 : Operand<i32>, ImmLeaf<i32, [{ return Imm >= 0 && Imm < 8; }]> {
+  let PrintMethod = "printUImm<3>";
+  let ParserMatchClass = UImm3Operand;
+}
+
+def uimm5 : Operand<i32>, ImmLeaf<i32, [{ return Imm >= 0 && Imm < 32; }]> {
+  let PrintMethod = "printUImm<5>";
+  let ParserMatchClass = UImm5Operand;
+}
+
+def UImm6Operand : AsmOperandClass {
+  let Name = "UImm6";
+  let RenderMethod = "addUImmOperands<16>";
+  let PredicateMethod = "isUImm<6>";
+  let DiagnosticType = "InvalidImm0_63";
+}
+def uimm6 : Operand<i32>, ImmLeaf<i32, [{ return Imm >= 0 && Imm < 64; }]> {
+  let PrintMethod = "printUImm<6>";
+  let ParserMatchClass = UImm6Operand;
+}
+
+def UImm12Operand : AsmOperandClass {
+  let Name = "UImm12";
+  let RenderMethod = "addUImmOperands<12>";
+  let PredicateMethod = "isUImm<12>";
+  let DiagnosticType = "InvalidImm0_4095";
+}
+def uimm12 : Operand<i64>, ImmLeaf<i64, [{ return Imm >= 0 && Imm < 4096; }]> {
+  let PrintMethod = "printUImm<12>";
+  let ParserMatchClass = UImm12Operand;
+}
+def uimm12_32 : Operand<i32>, ImmLeaf<i32, [{ return Imm >= 0 && Imm < 4096; }]> {
+  let PrintMethod = "printUImm<12>";
+  let ParserMatchClass = UImm12Operand;
+}
+
+def UImm15Operand : AsmOperandClass {
+  let Name = "UImm15";
+  let RenderMethod = "addUImmOperands<15>";
+  let PredicateMethod = "isUImm<15>";
+  let DiagnosticType = "InvalidImm0_32767";
+}
+def uimm15 : Operand<i64>, ImmLeaf<i64, [{ return Imm >= 0 && Imm < 32768; }]> {
+  let PrintMethod = "printUImm<15>";
+  let ParserMatchClass = UImm15Operand;
+}
+
+def UImm14Operand : AsmOperandClass {
+  let Name = "UImm14";
+  let RenderMethod = "addUImmOperands<14>";
+  let PredicateMethod = "isUImm<14>";
+  let DiagnosticType = "InvalidImm0_16383";
+}
+def uimm14 : Operand<i64>, ImmLeaf<i64, [{ return Imm >= 0 && Imm < 16384; }]> {
+  let PrintMethod = "printUImm<14>";
+  let ParserMatchClass = UImm14Operand;
+}
+def uimm14_32 : Operand<i32>, ImmLeaf<i32, [{ return Imm >= 0 && Imm < 16384; }]> {
+  let PrintMethod = "printUImm<14>";
+  let ParserMatchClass = UImm14Operand;
+}
+
+def UImm8Operand : AsmOperandClass {
+  let Name = "UImm8";
+  let RenderMethod = "addUImmOperands<8>";
+  let PredicateMethod = "isUImm<8>";
+  let DiagnosticType = "InvalidImm0_255";
+}
+def uimm8_64 : Operand<i64>, ImmLeaf<i64, [{ return Imm >= 0 && Imm < 256; }]> {
+  let PrintMethod = "printUImm<8>";
+  let ParserMatchClass = UImm8Operand;
+}
+
+// Transformation Function - get the higher 20 bits.
+def HI20 : SDNodeXForm<imm, [{
+  return getImm(N, (N->getZExtValue() >> 12) & 0xFFFFF);
+}]>;
+
+// Transformation Function - get the lower 12 bits.
+def LO12 : SDNodeXForm<imm, [{
+  return getImm(N, N->getZExtValue() & 0xFFF);
+}]>;
+
+def LU12IORIPred  : PatLeaf<(imm), [{
+  int64_t SVal = N->getSExtValue();
+  return isInt<32>(SVal) && (SVal & 0xfff);
+}]>;
+
+def ORi12Pred  : PatLeaf<(imm), [{
+  return isUInt<12>(N->getZExtValue()) && !isInt<12>(N->getSExtValue());
+}], LO12>;
+
+def LU12IPred : PatLeaf<(imm), [{
+  int64_t Val = N->getSExtValue();
+  return !isInt<12>(Val) && isInt<32>(Val) && !(Val & 0xfff);
+}]>;
+
+def addr :
+ComplexPattern<iPTR, 2, "selectIntAddr", [frameindex]>;
+
+def addrDefault :
+ComplexPattern<iPTR, 2, "selectAddrDefault", [frameindex]>;
+
+def addrRegImm :
+ComplexPattern<iPTR, 2, "selectAddrRegImm", [frameindex]>;
+
+def addrimm14lsl2 : ComplexPattern<iPTR, 2, "selectIntAddrSImm14Lsl2",
+                                   [frameindex]>;
+
+class ConstantUImmAsmOperandClass<int Bits, list<AsmOperandClass> Supers = [],
+                                  int Offset = 0> : AsmOperandClass {
+  let Name = "ConstantUImm" # Bits # "_" # Offset;
+  let RenderMethod = "addConstantUImmOperands<" # Bits # ", " # Offset # ">";
+  let PredicateMethod = "isConstantUImm<" # Bits # ", " # Offset # ">";
+  let SuperClasses = Supers;
+  let DiagnosticType = "UImm" # Bits # "_" # Offset;
+}
+class SImmAsmOperandClass<int Bits, list<AsmOperandClass> Supers = []>
+    : AsmOperandClass {
+  let Name = "SImm" # Bits;
+  let RenderMethod = "addSImmOperands<" # Bits # ">";
+  let PredicateMethod = "isSImm<" # Bits # ">";
+  let SuperClasses = Supers;
+  let DiagnosticType = "SImm" # Bits;
+}
+class UImmAnyAsmOperandClass<int Bits, list<AsmOperandClass> Supers = []>
+    : AsmOperandClass {
+  let Name = "ImmAny";
+  let RenderMethod = "addConstantUImmOperands<32>";
+  let PredicateMethod = "isSImm<" # Bits # ">";
+  let SuperClasses = Supers;
+  let DiagnosticType = "ImmAny";
+}
+
+def UImm32CoercedAsmOperandClass : UImmAnyAsmOperandClass<33, []> {
+  let Name = "UImm32_Coerced";
+  let DiagnosticType = "UImm32_Coerced";
+}
+def SImm32RelaxedAsmOperandClass
+    : SImmAsmOperandClass<32, [UImm32CoercedAsmOperandClass]> {
+  let Name = "SImm32_Relaxed";
+  let PredicateMethod = "isAnyImm<33>";
+  let DiagnosticType = "SImm32_Relaxed";
+}
+def SImm32AsmOperandClass
+    : SImmAsmOperandClass<32, [SImm32RelaxedAsmOperandClass]>;
+def ConstantUImm26AsmOperandClass
+    : ConstantUImmAsmOperandClass<26, [SImm32AsmOperandClass]>;
+
+def ConstantUImm20AsmOperandClass
+    : ConstantUImmAsmOperandClass<20, [ConstantUImm26AsmOperandClass]>;
+
+def ConstantUImm2Plus1AsmOperandClass
+    : ConstantUImmAsmOperandClass<2, [ConstantUImm20AsmOperandClass], 1>;
+
+class UImmAsmOperandClass<int Bits, list<AsmOperandClass> Supers = []>
+    : AsmOperandClass {
+  let Name = "UImm" # Bits;
+  let RenderMethod = "addUImmOperands<" # Bits # ">";
+  let PredicateMethod = "isUImm<" # Bits # ">";
+  let SuperClasses = Supers;
+  let DiagnosticType = "UImm" # Bits;
+}
+
+def UImm16RelaxedAsmOperandClass
+    : UImmAsmOperandClass<16, [ConstantUImm20AsmOperandClass]> {
+  let Name = "UImm16_Relaxed";
+  let PredicateMethod = "isAnyImm<16>";
+  let DiagnosticType = "UImm16_Relaxed";
+}
+
+def ConstantSImm14Lsl2AsmOperandClass : AsmOperandClass {
+  let Name = "SImm14Lsl2";
+  let RenderMethod = "addImmOperands";
+  let PredicateMethod = "isScaledSImm<14, 2>";
+  let SuperClasses = [UImm16RelaxedAsmOperandClass];
+  let DiagnosticType = "SImm14_Lsl2";
+}
+
+foreach I = {2} in
+  def simm14_lsl # I : Operand<i64> {
+    let DecoderMethod = "DecodeSImmWithOffsetAndScale<14, " # I # ">";
+    let ParserMatchClass =
+        !cast<AsmOperandClass>("ConstantSImm14Lsl" # I # "AsmOperandClass");
+  }
+
+def uimm16_64_relaxed : Operand<i64> {
+  let PrintMethod = "printUImm<16>";
+  let ParserMatchClass =
+      !cast<AsmOperandClass>("UImm16RelaxedAsmOperandClass");
+}
+
+def uimm2_plus1 : Operand<i32> {
+  let PrintMethod = "printUImm<2, 1>";
+  let EncoderMethod = "getUImmWithOffsetEncoding<2, 1>";
+  let DecoderMethod = "DecodeUImmWithOffset<2, 1>";
+  let ParserMatchClass = ConstantUImm2Plus1AsmOperandClass;
+}
+
+// like simm32 but coerces simm32 to uimm32.
+def uimm32_coerced : Operand<i32> {
+  let ParserMatchClass = !cast<AsmOperandClass>("UImm32CoercedAsmOperandClass");
+}
+
+def imm64: Operand<i64>;
+
+def LoongArchMemAsmOperand : AsmOperandClass {
+  let Name = "Mem";
+  let ParserMethod = "parseMemOperand";
+}
+
+def LoongArchMemSimm14AsmOperand : AsmOperandClass {
+  let Name = "MemOffsetSimm14";
+  let SuperClasses = [LoongArchMemAsmOperand];
+  let RenderMethod = "addMemOperands";
+  let ParserMethod = "parseMemOperand";
+  let PredicateMethod = "isMemWithSimmOffset<14>";
+  let DiagnosticType = "MemSImm14";
+}
+
+foreach I = {2} in
+  def LoongArchMemSimm14Lsl # I # AsmOperand : AsmOperandClass {
+    let Name = "MemOffsetSimm14_" # I;
+    let SuperClasses = [LoongArchMemAsmOperand];
+    let RenderMethod = "addMemOperands";
+    let ParserMethod = "parseMemOperand";
+    let PredicateMethod = "isMemWithSimmOffset<14, " # I # ">";
+    let DiagnosticType = "MemSImm14Lsl" # I;
+  }
+
+def LoongArchMemSimmPtrAsmOperand : AsmOperandClass {
+  let Name = "MemOffsetSimmPtr";
+  let SuperClasses = [LoongArchMemAsmOperand];
+  let RenderMethod = "addMemOperands";
+  let ParserMethod = "parseMemOperand";
+  let PredicateMethod = "isMemWithPtrSizeOffset";
+  let DiagnosticType = "MemSImmPtr";
+}
+
+class mem_generic : Operand<iPTR> {
+  let PrintMethod = "printMemOperand";
+  let MIOperandInfo = (ops ptr_rc, simm12);
+  let EncoderMethod = "getMemEncoding";
+  let ParserMatchClass = LoongArchMemAsmOperand;
+  let OperandType = "OPERAND_MEMORY";
+}
+
+// Address operand
+def mem : mem_generic;
+def mem_simmptr : mem_generic {
+    let ParserMatchClass = LoongArchMemSimmPtrAsmOperand;
+}
+
+foreach I = {2} in
+  def mem_simm14_lsl # I : mem_generic {
+    let MIOperandInfo = (ops ptr_rc, !cast<Operand>("simm14_lsl" # I));
+    let EncoderMethod = "getSimm14MemEncoding<" # I  # ">";
+    let ParserMatchClass =
+            !cast<AsmOperandClass>("LoongArchMemSimm14Lsl" # I # "AsmOperand");
+  }
+
+def mem_ea : Operand<iPTR> {
+  let PrintMethod = "printMemOperandEA";
+  let MIOperandInfo = (ops ptr_rc, simm12);
+  let EncoderMethod = "getMemEncoding";
+  let OperandType = "OPERAND_MEMORY";
+}
+
+def LoongArchJumpTargetAsmOperand : AsmOperandClass {
+  let Name = "JumpTarget";
+  let ParserMethod = "parseJumpTarget";
+  let PredicateMethod = "isImm";
+  let RenderMethod = "addImmOperands";
+}
+
+def jmptarget   : Operand<OtherVT> {
+  let EncoderMethod = "getJumpTargetOpValue";
+  let ParserMatchClass = LoongArchJumpTargetAsmOperand;
+}
+
+def brtarget    : Operand<OtherVT> {
+  let EncoderMethod = "getBranchTargetOpValue";
+  let OperandType = "OPERAND_PCREL";
+  let DecoderMethod = "DecodeBranchTarget";
+  let ParserMatchClass = LoongArchJumpTargetAsmOperand;
+}
+
+def calltarget  : Operand<iPTR> {
+  let EncoderMethod = "getJumpTargetOpValue";
+  let ParserMatchClass = LoongArchJumpTargetAsmOperand;
+}
+
+//
+//SDNode
+//
+def IsGP64bit    :    Predicate<"Subtarget->is64Bit()">,
+    AssemblerPredicate<(all_of Feature64Bit)>;
+def IsGP32bit    :    Predicate<"!Subtarget->is64Bit()">,
+    AssemblerPredicate<(all_of (not Feature64Bit))>;
+def SDT_LoongArchCallSeqStart : SDCallSeqStart<[SDTCisVT<0, i32>, SDTCisVT<1, i32>]>;
+def SDT_LoongArchCallSeqEnd   : SDCallSeqEnd<[SDTCisVT<0, i32>, SDTCisVT<1, i32>]>;
+
+def LoongArchRet : SDNode<"LoongArchISD::Ret", SDTNone,
+                     [SDNPHasChain, SDNPOptInGlue, SDNPVariadic]>;
+def LoongArchERet : SDNode<"LoongArchISD::ERet", SDTNone,
+                      [SDNPHasChain, SDNPOptInGlue, SDNPSideEffect]>;
+
+def callseq_start : SDNode<"ISD::CALLSEQ_START", SDT_LoongArchCallSeqStart,
+                           [SDNPHasChain, SDNPSideEffect, SDNPOutGlue]>;
+def callseq_end   : SDNode<"ISD::CALLSEQ_END", SDT_LoongArchCallSeqEnd,
+                           [SDNPHasChain, SDNPSideEffect,
+                            SDNPOptInGlue, SDNPOutGlue]>;
+def LoongArchAddress : SDNode<"LoongArchISD::GlobalAddress", SDTIntUnaryOp>;
+
+// Return RA.
+let isReturn=1, isTerminator=1, isBarrier=1, hasCtrlDep=1, isCTI=1 in {
+  def RetRA : LoongArchPseudo<(outs), (ins), [(LoongArchRet)]>;
+
+  let hasSideEffects=1 in
+  def ERet : LoongArchPseudo<(outs), (ins), [(LoongArchERet)]>;
+}
+
+let Defs = [SP], Uses = [SP], hasSideEffects = 1 in {
+def ADJCALLSTACKDOWN : LoongArchPseudo<(outs), (ins i32imm:$amt1, i32imm:$amt2),
+                                  [(callseq_start timm:$amt1, timm:$amt2)]>;
+def ADJCALLSTACKUP   : LoongArchPseudo<(outs), (ins i32imm:$amt1, i32imm:$amt2),
+                                  [(callseq_end timm:$amt1, timm:$amt2)]>;
+}
+
+class LoongArchPat<dag pattern, dag result> : Pat<pattern, result>, PredicateControl;
+
+def SDT_LoongArchJmpLink      : SDTypeProfile<0, 1, [SDTCisVT<0, iPTR>]>;
+
+def LoongArchJmpLink : SDNode<"LoongArchISD::JmpLink",SDT_LoongArchJmpLink,
+                         [SDNPHasChain, SDNPOutGlue, SDNPOptInGlue,
+                          SDNPVariadic]>;
+
+def LoongArchTailCall : SDNode<"LoongArchISD::TailCall", SDT_LoongArchJmpLink,
+                          [SDNPHasChain, SDNPOptInGlue, SDNPVariadic]>;
+
+class GPR_32 { list<Predicate> GPRPredicates = [IsGP32bit]; }
+class GPR_64 { list<Predicate> GPRPredicates = [IsGP64bit]; }
+
+//===---------------------------------------------------------------------===/
+// Instruction Class Templates
+//===---------------------------------------------------------------------===/
+///R2
+class Int_Reg2<string opstr, RegisterOperand RO,
+               SDPatternOperator OpNode= null_frag>
+    : InstForm<(outs RO:$rd), (ins RO:$rj),
+          !strconcat(opstr, "\t$rd, $rj"),
+          [(set RO:$rd, (OpNode RO:$rj))],
+          FrmR, opstr>;
+
+class Int_Reg2_Iocsrrd<string opstr, RegisterOperand RD, RegisterOperand RS,
+               SDPatternOperator OpNode= null_frag>
+    : InstForm<(outs RD:$rd), (ins RS:$rj),
+          !strconcat(opstr, "\t$rd, $rj"),
+          [(set RD:$rd, (OpNode RS:$rj))],
+          FrmR, opstr>;
+
+class Int_Reg2_Rdtime<string opstr, RegisterOperand RO,
+               SDPatternOperator OpNode= null_frag>
+    : InstForm<(outs RO:$rd, RO:$rj), (ins),
+          !strconcat(opstr, "\t$rd, $rj"),
+          [(set (OpNode RO:$rd, RO:$rj))],
+          FrmR, opstr>;
+
+class Int_Reg2_Iocsrwr<string opstr, RegisterOperand RD, RegisterOperand RS,
+               SDPatternOperator OpNode= null_frag>
+    : InstForm<(outs), (ins RD:$rd, RS:$rj),
+          !strconcat(opstr, "\t$rd, $rj"),
+          [(set (OpNode RD:$rd, RS:$rj))],
+          FrmR, opstr>;
+
+class Float_Reg2<string opstr, RegisterOperand RO,
+                 SDPatternOperator OpNode= null_frag>
+    : InstForm<(outs RO:$fd), (ins RO:$fj),
+          !strconcat(opstr, "\t$fd, $fj"),
+          [(set RO:$fd, (OpNode RO:$fj))],
+          FrmFR, opstr>;
+
+class Count1<string opstr, RegisterOperand RO,
+               SDPatternOperator OpNode= null_frag>
+    : InstForm<(outs RO:$rd), (ins RO:$rj),
+          !strconcat(opstr, "\t$rd, $rj"),
+          [(set RO:$rd, (OpNode (not RO:$rj)))],
+          FrmR, opstr>;
+
+class SignExtInReg<string opstr, RegisterOperand RO,
+                   ValueType vt>
+    : InstForm<(outs RO:$rd), (ins RO:$rj), !strconcat(opstr, "\t$rd, $rj"),
+         [(set RO:$rd, (sext_inreg RO:$rj, vt))], FrmR, opstr>;
+
+///R3
+class Int_Reg3<string opstr, RegisterOperand RO,
+               SDPatternOperator OpNode= null_frag>
+    : InstForm<(outs RO:$rd), (ins RO:$rj, RO:$rk),
+               !strconcat(opstr, "\t$rd, $rj, $rk"),
+               [(set RO:$rd, (OpNode RO:$rj, RO:$rk))],
+               FrmR, opstr>;
+
+class Int_Reg3_Crc<string opstr, RegisterOperand RD, RegisterOperand RS,
+               SDPatternOperator OpNode= null_frag>
+    : InstForm<(outs RS:$rd), (ins RD:$rj, RS:$rk),
+               !strconcat(opstr, "\t$rd, $rj, $rk"),
+               [(set RS:$rd, (OpNode RD:$rj, RS:$rk))],
+               FrmR, opstr>;
+
+class SetCC_R<string opstr, RegisterOperand RO,
+              SDPatternOperator OpNode= null_frag>
+    : InstForm<(outs GPR32Opnd:$rd), (ins RO:$rj, RO:$rk),
+               !strconcat(opstr, "\t$rd, $rj, $rk"),
+               [(set GPR32Opnd:$rd, (OpNode RO:$rj, RO:$rk))],
+               FrmR, opstr>;
+
+class SetCC_I<string opstr, RegisterOperand RO, Operand ImmOpnd,
+              SDPatternOperator OpNode= null_frag>
+    : InstForm<(outs GPR32Opnd:$rd), (ins RO:$rj, ImmOpnd:$imm12),
+               !strconcat(opstr, "\t$rd, $rj, $imm12"),
+               [(set GPR32Opnd:$rd, (OpNode RO:$rj, ImmOpnd:$imm12))],
+               FrmR, opstr>;
+
+class ATOMIC<string opstr, RegisterOperand RD, DAGOperand MO,
+             SDPatternOperator OpNode= null_frag,
+             ComplexPattern Addr = addr>
+    : InstForm<(outs RD:$rd), (ins RD:$rk, MO:$addr),
+               !strconcat(opstr, "\t$rd, $rk, $addr"),
+               [(set RD:$rd, (OpNode RD:$rk, Addr:$addr))],
+               FrmR, opstr> {
+    let DecoderMethod = "DecodeMem";
+    let canFoldAsLoad = 1;
+    string BaseOpcode = opstr;
+    let mayLoad = 1;
+    let mayStore = 1;
+    let Constraints = "@earlyclobber $rd";
+}
+
+class Nor<string opstr, RegisterOperand RO>
+    : InstForm<(outs RO:$rd), (ins RO:$rj, RO:$rk),
+          !strconcat(opstr, "\t$rd, $rj, $rk"),
+          [(set RO:$rd, (not (or RO:$rj, RO:$rk)))],
+          FrmR, opstr>;
+
+class Shift_Var<string opstr, RegisterOperand RO,
+                SDPatternOperator OpNode= null_frag>
+    : InstForm<(outs RO:$rd), (ins RO:$rj, GPR32Opnd:$rk),
+          !strconcat(opstr, "\t$rd, $rj, $rk"),
+          [(set RO:$rd, (OpNode RO:$rj, GPR32Opnd:$rk))],
+          FrmR, opstr>;
+
+class Float_Reg3<string opstr, RegisterOperand RO,
+                 SDPatternOperator OpNode= null_frag>
+    : InstForm<(outs RO:$fd), (ins RO:$fj, RO:$fk),
+          !strconcat(opstr, "\t$fd, $fj, $fk"),
+          [(set RO:$fd, (OpNode RO:$fj, RO:$fk))],
+          FrmR, opstr>;
+
+class Float_Reg3_MA<string opstr, RegisterOperand RO,
+                 SDPatternOperator OpNode= null_frag>
+    : InstForm<(outs RO:$fd), (ins RO:$fj, RO:$fk),
+          !strconcat(opstr, "\t$fd, $fj, $fk"),
+          [(set RO:$fd, (OpNode (fabs RO:$fj), (fabs RO:$fk)))],
+          FrmR, opstr>;
+
+class Float_Int_Reg3<string opstr, RegisterOperand RD, RegisterOperand RS,
+                     SDPatternOperator OpNode= null_frag>
+    : InstForm<(outs RD:$fd), (ins RS:$rj, RS:$rk),
+          !strconcat(opstr, "\t$fd, $rj, $rk"),
+          [(set RS:$fd, (OpNode RS:$rj, RS:$rk))],
+          FrmR, opstr>;
+
+///R4
+class Mul_Reg4<string opstr, RegisterOperand RO,
+               SDPatternOperator OpNode= null_frag>
+    : InstForm<(outs RO:$fd), (ins RO:$fj, RO:$fk, RO:$fa),
+          !strconcat(opstr, "\t$fd, $fj, $fk, $fa"),
+          [],
+          FrmFR, opstr>;
+
+class NMul_Reg4<string opstr, RegisterOperand RO,
+               SDPatternOperator OpNode= null_frag>
+    : InstForm<(outs RO:$fd), (ins RO:$fj, RO:$fk, RO:$fa),
+          !strconcat(opstr, "\t$fd, $fj, $fk, $fa"),
+          [],
+          FrmFR, opstr>;
+
+///R2_IMM5
+class Shift_Imm32<string opstr, RegisterOperand RO,
+             SDPatternOperator OpNode= null_frag>
+    : InstForm<(outs RO:$rd), (ins RO:$rj, uimm5:$imm5),
+          !strconcat(opstr, "\t$rd, $rj, $imm5"),
+          [(set RO:$rd, (OpNode RO:$rj, uimm5:$imm5))],
+          FrmR, opstr>;
+
+///R2_IMM6
+class Shift_Imm64<string opstr, RegisterOperand RO,
+             SDPatternOperator OpNode= null_frag>
+    : InstForm<(outs RO:$rd), (ins RO:$rj, uimm6:$imm6),
+          !strconcat(opstr, "\t$rd, $rj, $imm6"),
+          [(set RO:$rd, (OpNode RO:$rj, uimm6:$imm6))],
+          FrmR, opstr>;
+
+///LOAD_STORE
+class FLd<string opstr, RegisterOperand RD,DAGOperand MO,
+          SDPatternOperator OpNode= null_frag>
+    : InstForm<(outs RD:$rd), (ins MO:$addr),
+               !strconcat(opstr, "\t$rd, $addr"),
+               [(set RD:$rd, (OpNode addrDefault:$addr))],
+               FrmR, opstr> {
+    let DecoderMethod = "DecodeFMem";
+    let mayLoad = 1;
+}
+
+class Ld<string opstr, RegisterOperand RD, DAGOperand MO,
+         SDPatternOperator OpNode= null_frag,
+         ComplexPattern Addr = addr>
+    : InstForm<(outs RD:$rd), (ins MO:$addr),
+               !strconcat(opstr, "\t$rd, $addr"),
+               [(set RD:$rd, (OpNode Addr:$addr))],
+               FrmR, opstr> {
+    let DecoderMethod = "DecodeMem";
+    let canFoldAsLoad = 1;
+    string BaseOpcode = opstr;
+    let mayLoad = 1;
+}
+
+class FSt<string opstr, RegisterOperand RD, DAGOperand MO,
+          SDPatternOperator OpNode= null_frag>
+    : InstForm<(outs), (ins RD:$rd, MO:$addr),
+               !strconcat(opstr, "\t$rd, $addr"),
+               [(OpNode RD:$rd, addrDefault:$addr)],
+               FrmR, opstr> {
+    let DecoderMethod = "DecodeFMem";
+    let mayStore = 1;
+}
+
+class St<string opstr, RegisterOperand RS, DAGOperand MO,
+         SDPatternOperator OpNode= null_frag>
+    : InstForm<(outs), (ins RS:$rd, MO:$addr),
+               !strconcat(opstr, "\t$rd, $addr"),
+               [(OpNode RS:$rd, addr:$addr)],
+               FrmR, opstr> {
+    let DecoderMethod = "DecodeMem";
+    string BaseOpcode = opstr;
+    let mayStore = 1;
+}
+
+/// R2_IMM12
+class Int_Reg2_Imm12<string opstr, RegisterOperand RO, Operand ImmOpnd,
+                     SDPatternOperator OpNode= null_frag>
+    : InstForm<(outs RO:$rd), (ins RO:$rj, ImmOpnd:$imm12),
+               !strconcat(opstr, "\t$rd, $rj, $imm12"),
+               [(set RO:$rd, (OpNode RO:$rj, ImmOpnd:$imm12))],
+               FrmR, opstr>;
+class RELOC_rrii<string opstr, RegisterOperand RO, Operand ImmOpnd,
+                     SDPatternOperator OpNode= null_frag>
+    : InstForm<(outs RO:$rd), (ins RO:$rj, ImmOpnd:$imm12, ImmOpnd:$i12),
+               !strconcat(opstr, "\t$rd, $rj, $imm12"),
+               [(set RO:$rd, (OpNode RO:$rj, ImmOpnd:$imm12, ImmOpnd:$i12))],
+               FrmR, opstr>;
+
+///R2_IMM14
+class LdPtr<string opstr, RegisterOperand RO>
+    : InstForm<(outs RO:$rd), (ins mem_simm14_lsl2:$addr),
+               !strconcat(opstr, "\t$rd, $addr"),
+               [], FrmI, opstr>{
+    let DecoderMethod = "DecodeMemSimm14";
+    let canFoldAsLoad = 1;
+    string BaseOpcode = opstr;
+    let mayLoad = 1;
+}
+
+class StPtr<string opstr, RegisterOperand RO>
+    : InstForm<(outs), (ins RO:$rd, mem_simm14_lsl2:$addr),
+               !strconcat(opstr, "\t$rd, $addr"),
+               [], FrmI, opstr> {
+    let DecoderMethod = "DecodeMemSimm14";
+    string BaseOpcode = opstr;
+    let mayStore = 1;
+}
+
+///R2_IMM16
+class FJirl<string opstr, DAGOperand opnd,
+           RegisterOperand RO>
+    : InstForm<(outs), (ins RO:$rd, RO:$rj, opnd:$offs16),
+          !strconcat(opstr, "\t$rd, $rj, $offs16"),
+          [], FrmJ, opstr>;
+
+class Beq<string opstr, DAGOperand opnd, PatFrag cond_op,
+          RegisterOperand RO>
+    : InstForm<(outs), (ins RO:$rj, RO:$rd, opnd:$offs16),
+               !strconcat(opstr, "\t$rj, $rd, $offs16"),
+               [(brcond (i32 (cond_op RO:$rj, RO:$rd)), bb:$offs16)],
+               FrmI, opstr> {
+    let isBranch = 1;
+    let isTerminator = 1;
+    bit isCTI = 1;
+}
+
+///R1_IMM21
+class Beqz<string opstr, DAGOperand opnd, PatFrag cond_op,
+           RegisterOperand RO>
+    : InstForm<(outs), (ins RO:$rj, opnd:$offs21),
+          !strconcat(opstr, "\t$rj, $offs21"),
+          [(brcond (i32 (cond_op RO:$rj, 0)), bb:$offs21)],
+          FrmI, opstr> {
+    let isBranch = 1;
+    let isTerminator = 1;
+    bit isCTI = 1;
+}
+
+///IMM26
+class JumpFB<DAGOperand opnd, string opstr, SDPatternOperator operator,
+             SDPatternOperator targetoperator> :
+    InstForm<(outs), (ins opnd:$offset26), !strconcat(opstr, "\t$offset26"),
+           [(operator targetoperator:$offset26)], FrmJ, opstr> {
+    let isBranch = 1;
+    let isTerminator=1;
+    let isBarrier=1;
+    let DecoderMethod = "DecodeJumpTarget";
+    bit isCTI = 1;
+}
+
+/// R3_SA
+class Reg3_Sa<string opstr, RegisterOperand RO, Operand ImmOpnd,
+              SDPatternOperator OpNode= null_frag>
+    : InstForm<(outs RO:$rd), (ins RO:$rj, RO:$rk, ImmOpnd:$sa),
+          !strconcat(opstr, "\t$rd, $rj, $rk, $sa"),
+          [(set RO:$rd, (OpNode RO:$rj, RO:$rk, ImmOpnd:$sa))],
+          FrmR, opstr>;
+
+class Reg3_SaU<string opstr, RegisterOperand RD, RegisterOperand RS, Operand ImmOpnd,
+              SDPatternOperator OpNode= null_frag>
+    : InstForm<(outs RD:$rd), (ins RS:$rj, RS:$rk, ImmOpnd:$sa),
+          !strconcat(opstr, "\t$rd, $rj, $rk, $sa"),
+          [(set RD:$rd, (OpNode RS:$rj, RS:$rk, ImmOpnd:$sa))],
+          FrmR, opstr>;
+
+/// Assert
+class Assert<string opstr, RegisterOperand RO,
+             SDPatternOperator OpNode= null_frag>
+    : InstForm<(outs), (ins RO:$rj, RO:$rk),
+          !strconcat(opstr, "\t$rj, $rk"),
+          [(set (OpNode RO:$rj, RO:$rk))],
+          FrmR, opstr>;
+
+class Code15<string opstr,
+             SDPatternOperator OpNode= null_frag>
+    : InstForm<(outs), (ins uimm15:$Code),
+          !strconcat(opstr, "\t$Code"),
+          [(set (OpNode uimm15:$Code))],
+          FrmOther, opstr>;
+
+class TrapBase<Instruction RealInst>
+    : LoongArchPseudo<(outs), (ins), [(trap)]>,
+      PseudoInstExpansion<(RealInst 0)> {
+    let isBarrier = 1;
+    let isTerminator = 1;
+    let isCodeGenOnly = 1;
+    let isCTI = 1;
+}
+
+class CSR<string opstr, RegisterOperand RO, Operand ImmOpnd,
+          SDPatternOperator OpNode= null_frag>
+    : InstForm<(outs RO:$rd), (ins ImmOpnd:$csr),
+          !strconcat(opstr, "\t$rd, $csr"),
+          [(set RO:$rd, (OpNode ImmOpnd:$csr))],
+          FrmOther, opstr>;
+
+class CSRW<string opstr, RegisterOperand RO, Operand ImmOpnd,
+          SDPatternOperator OpNode= null_frag>
+    : InstForm<(outs RO:$dst), (ins RO:$rd, ImmOpnd:$csr),
+          !strconcat(opstr, "\t$rd, $csr"),
+          [(set RO:$dst, (OpNode RO:$rd, ImmOpnd:$csr))],
+          FrmOther, opstr>{
+    let Constraints = "$rd = $dst";
+}
+
+class CSRX<string opstr, RegisterOperand RO, Operand ImmOpnd,
+          SDPatternOperator OpNode= null_frag>
+    : InstForm<(outs RO:$dst), (ins RO:$rd, RO:$rj, ImmOpnd:$csr),
+          !strconcat(opstr, "\t$rd, $rj, $csr"),
+          [(set RO:$dst, (OpNode RO:$rd, RO:$rj, ImmOpnd:$csr))],
+          FrmOther, opstr>{
+    let Constraints = "$rd = $dst";
+}
+
+class CAC<string opstr, RegisterOperand RO, Operand ImmOpnd,
+          SDPatternOperator OpNode= null_frag>
+    : InstForm<(outs), (ins uimm5:$op, RO:$rj, ImmOpnd:$si12),
+          !strconcat(opstr, "\t$op, $rj, $si12"),
+          [(set (OpNode uimm5:$op, RO:$rj, ImmOpnd:$si12))],
+          FrmOther, opstr>;
+
+class LEVEL<string opstr, RegisterOperand RO,
+          SDPatternOperator OpNode= null_frag>
+    : InstForm<(outs RO:$rd), (ins RO:$rj, uimm8_64:$level),
+          !strconcat(opstr, "\t$rd, $rj, $level"),
+          [(set RO:$rd, (OpNode RO:$rj, uimm8_64:$level))],
+          FrmOther, opstr>;
+
+class SEQ<string opstr, RegisterOperand RO,
+          SDPatternOperator OpNode= null_frag>
+    : InstForm<(outs), (ins RO:$rj, uimm8_64:$seq),
+          !strconcat(opstr, "\t$rj, $seq"),
+          [(set (OpNode RO:$rj, uimm8_64:$seq))],
+          FrmOther, opstr>;
+
+class Wait<string opstr,
+          SDPatternOperator OpNode= null_frag>
+    : InstForm<(outs), (ins uimm15:$hint),
+          !strconcat(opstr, "\t$hint"),
+          [(set (OpNode uimm15:$hint))],
+          FrmOther, opstr>;
+
+class Invtlb<string opstr, RegisterOperand RO,
+          SDPatternOperator OpNode= null_frag>
+    : InstForm<(outs), (ins uimm5:$op, RO:$rj, RO:$rk),
+          !strconcat(opstr, "\t$op, $rj, $rk"),
+          [(set (OpNode uimm5:$op, RO:$rj, RO:$rk))],
+          FrmOther, opstr>;
+
+class OP32<string opstr,
+           SDPatternOperator OpNode= null_frag>
+    : InstForm<(outs), (ins),
+          !strconcat(opstr, ""),
+          [(set (OpNode))],
+          FrmOther, opstr>;
+
+class Bar<string opstr,
+          SDPatternOperator OpNode= null_frag>
+    : InstForm<(outs), (ins uimm15:$hint),
+          !strconcat(opstr, "\t$hint"),
+          [(set (OpNode uimm15:$hint))],
+          FrmOther, opstr>;
+
+//class CA<bits<2> op, string opstr>
+//    : R3_CA<op, (outs FGR64:$fd), (ins FGR64:$fj,FGR64:$fk,simm3:$ca),
+//          !strconcat(opstr, "\t$fd, $fj, $fk, $ca"), NoItinerary>;
+
+class SI16_R2<string opstr, RegisterOperand RO,
+              SDPatternOperator OpNode= null_frag>
+    : InstForm<(outs RO:$rd), (ins RO:$rj, simm16:$si16),
+          !strconcat(opstr, "\t$rd, $rj, $si16"),
+          [(set RO:$rd, (OpNode RO:$rj, simm16:$si16))],
+          FrmR, opstr>;
+
+class SI20<string opstr, RegisterOperand RO, Operand ImmOpnd,
+           SDPatternOperator OpNode= null_frag>
+    : InstForm<(outs RO:$rd), (ins ImmOpnd:$si20),
+          !strconcat(opstr, "\t$rd, $si20"),
+          [(set RO:$rd, (OpNode ImmOpnd:$si20))],
+          FrmR, opstr>;
+class RELOC_rii<string opstr, RegisterOperand RO, Operand ImmOpnd,
+           SDPatternOperator OpNode= null_frag>
+    : InstForm<(outs RO:$rd), (ins ImmOpnd:$si20, ImmOpnd:$i20),
+          !strconcat(opstr, "\t$rd, $si20"),
+          [(set RO:$rd, (OpNode ImmOpnd:$si20, ImmOpnd:$i20))],
+          FrmR, opstr>;
+
+// preld
+class Preld<string opstr,Operand MemOpnd ,RegisterOperand RO,
+            SDPatternOperator OpNode= null_frag>
+    : InstForm<(outs), (ins RO:$rj, MemOpnd:$addr, uimm5:$hint),
+          !strconcat(opstr, "\t$hint, $rj, $addr"),
+          [(set (OpNode RO:$rj, MemOpnd:$addr, uimm5:$hint))],
+          FrmR, opstr>;
+class Preld_Raw<string opstr, RegisterOperand RO>
+    : InstForm<(outs), (ins RO:$rj, simm12:$imm12, uimm5:$hint),
+               !strconcat(opstr, "\t$hint, $rj, $imm12"),
+               [],
+               FrmR, opstr>;
+class IsCall {
+  bit isCall = 1;
+  bit isCTI = 1;
+}
+
+class EffectiveAddress<string opstr, RegisterOperand RO>
+    : InstForm<(outs RO:$rd), (ins mem_ea:$addr),
+               !strconcat(opstr, "\t$rd, $addr"),
+               [(set RO:$rd, addr:$addr)], FrmI,
+               !strconcat(opstr, "_lea")> {
+  let isCodeGenOnly = 1;
+  let hasNoSchedulingInfo = 1;
+  let DecoderMethod = "DecodeMem";
+}
+
+def PtrRC : Operand<iPTR> {
+  let MIOperandInfo = (ops ptr_rc);
+  let DecoderMethod = "DecodePtrRegisterClass";
+  let ParserMatchClass = GPR32AsmOperand;
+}
+
+class Atomic2Ops<PatFrag Op, RegisterClass DRC> :
+  LoongArchPseudo<(outs DRC:$dst), (ins PtrRC:$ptr, DRC:$incr),
+           [(set DRC:$dst, (Op iPTR:$ptr, DRC:$incr))]>;
+
+class Atomic2OpsPostRA<RegisterClass RC> :
+  LoongArchPseudo<(outs RC:$dst), (ins PtrRC:$ptr, RC:$incr), []> {
+  let mayLoad = 1;
+  let mayStore = 1;
+}
+
+class Atomic2OpsSubwordPostRA<RegisterClass RC> :
+  LoongArchPseudo<(outs RC:$dst), (ins PtrRC:$ptr, RC:$incr, RC:$mask, RC:$mask2,
+                                RC:$shiftamnt), []>;
+class AtomicCmpSwap<PatFrag Op, RegisterClass DRC> :
+  LoongArchPseudo<(outs DRC:$dst), (ins PtrRC:$ptr, DRC:$cmp, DRC:$swap),
+           [(set DRC:$dst, (Op iPTR:$ptr, DRC:$cmp, DRC:$swap))]>;
+
+class AtomicCmpSwapPostRA<RegisterClass RC> :
+  LoongArchPseudo<(outs RC:$dst), (ins PtrRC:$ptr, RC:$cmp, RC:$swap), []> {
+  let mayLoad = 1;
+  let mayStore = 1;
+}
+
+class AtomicCmpSwapSubwordPostRA<RegisterClass RC> :
+  LoongArchPseudo<(outs RC:$dst), (ins PtrRC:$ptr, RC:$mask, RC:$ShiftCmpVal,
+                                RC:$mask2, RC:$ShiftNewVal, RC:$ShiftAmt), []> {
+  let mayLoad = 1;
+  let mayStore = 1;
+}
+
+class LoongArchInstAlias<string Asm, dag Result, bit Emit = 0b1> :
+  InstAlias<Asm, Result, Emit>, PredicateControl;
+
+//===---------------------------------------------------------------------===/
+// Instruction Definitions.
+//===---------------------------------------------------------------------===/
+///
+/// R2
+///
+
+def CLO_D : Count1<"clo.d", GPR64Opnd, ctlz>, R2I<0b01000>;
+def CLZ_D : Int_Reg2<"clz.d", GPR64Opnd, ctlz>, R2I<0b01001>;
+def CTO_D : Count1<"cto.d", GPR64Opnd, cttz>, R2I<0b01010>;
+def CTZ_D : Int_Reg2<"ctz.d", GPR64Opnd, cttz>, R2I<0b01011>;
+
+def REVB_4H : Int_Reg2<"revb.4h", GPR64Opnd>, R2I<0b01101>; //[]
+def REVB_2W : Int_Reg2<"revb.2w", GPR64Opnd>, R2I<0b01110>;
+def REVB_D  : Int_Reg2<"revb.d", GPR64Opnd>, R2I<0b01111>;
+def REVH_2W : Int_Reg2<"revh.2w", GPR64Opnd>, R2I<0b10000>;
+def REVH_D  : Int_Reg2<"revh.d", GPR64Opnd>, R2I<0b10001>; //[]
+
+def BITREV_8B : Int_Reg2<"bitrev.8b", GPR64Opnd>, R2I<0b10011>; //[]
+def BITREV_D  : Int_Reg2<"bitrev.d", GPR64Opnd, bitreverse>, R2I<0b10101>;
+
+def EXT_W_H : SignExtInReg<"ext.w.h", GPR64Opnd, i16>, R2I<0b10110>;
+def EXT_W_B : SignExtInReg<"ext.w.b", GPR64Opnd, i8>, R2I<0b10111>;
+
+def RDTIME_D  : Int_Reg2_Rdtime<"rdtime.d", GPR64Opnd>, R2I<0b11010>;
+def RDTIMEL_W : Int_Reg2_Rdtime<"rdtimel.w", GPR64Opnd>, R2I<0b11000>;
+def RDTIMEH_W : Int_Reg2_Rdtime<"rdtimeh.w", GPR64Opnd>, R2I<0b11001>;
+///
+/// R3
+///
+def ADD_D : Int_Reg3<"add.d", GPR64Opnd, add>, R3I<0b0100001>;
+def SUB_D : Int_Reg3<"sub.d", GPR64Opnd, sub>, R3I<0b0100011>;
+
+def SLT     : SetCC_R<"slt", GPR64Opnd, setlt>, R3I<0b0100100>;
+def SLTU    : SetCC_R<"sltu", GPR64Opnd, setult>, R3I<0b0100101>;
+def MASKEQZ : Int_Reg3<"maskeqz", GPR64Opnd>, R3I<0b0100110>; //[]
+def MASKNEZ : Int_Reg3<"masknez", GPR64Opnd>, R3I<0b0100111>; //[]
+
+def NOR   : Nor<"nor", GPR64Opnd>, R3I<0b0101000>;
+def AND   : Int_Reg3<"and", GPR64Opnd, and>, R3I<0b0101001>;
+def OR    : Int_Reg3<"or", GPR64Opnd, or>, R3I<0b0101010>;
+def XOR   : Int_Reg3<"xor", GPR64Opnd, xor>, R3I<0b0101011>;
+def ORN   : Int_Reg3<"orn", GPR64Opnd>, R3I<0b0101100>;
+def ANDN  : Int_Reg3<"andn", GPR64Opnd>, R3I<0b0101101>;
+
+def SLL_D : Shift_Var<"sll.d", GPR64Opnd, shl>, R3I<0b0110001>;
+def SRL_D : Shift_Var<"srl.d", GPR64Opnd, srl>, R3I<0b0110010>;
+def SRA_D : Shift_Var<"sra.d", GPR64Opnd, sra>, R3I<0b0110011>;
+def ROTR_D: Shift_Var<"rotr.d", GPR64Opnd, rotr>, R3I<0b0110111>;
+
+def MUL_D     : Int_Reg3<"mul.d", GPR64Opnd, mul>, R3I<0b0111011>;
+def MULH_D    : Int_Reg3<"mulh.d", GPR64Opnd, mulhs>, R3I<0b0111100>;
+def MULH_DU   : Int_Reg3<"mulh.du", GPR64Opnd, mulhu>, R3I<0b0111101>;
+def MULW_D_W  : Int_Reg3<"mulw.d.w", GPR64Opnd>, R3I<0b0111110>;
+def MULW_D_WU : Int_Reg3<"mulw.d.wu", GPR64Opnd>, R3I<0b0111111>;
+
+let usesCustomInserter = 1 in {
+def DIV_D  : Int_Reg3<"div.d", GPR64Opnd, sdiv>, R3I<0b1000100>;
+def MOD_D  : Int_Reg3<"mod.d", GPR64Opnd, srem>, R3I<0b1000101>;
+def DIV_DU : Int_Reg3<"div.du", GPR64Opnd, udiv>, R3I<0b1000110>;
+def MOD_DU : Int_Reg3<"mod.du", GPR64Opnd, urem>, R3I<0b1000111>;
+}
+
+def CRC_W_D_W  : Int_Reg3_Crc<"crc.w.d.w", GPR64Opnd, GPR32Opnd, int_loongarch_crc_w_d_w>, R3I<0b1001011>;
+def CRCC_W_D_W : Int_Reg3_Crc<"crcc.w.d.w", GPR64Opnd, GPR32Opnd, int_loongarch_crcc_w_d_w>, R3I<0b1001111>;
+///
+/// SLLI
+///
+def SLLI_D  : Shift_Imm64<"slli.d", GPR64Opnd, shl>, R2_IMM6<0b00>;
+def SRLI_D  : Shift_Imm64<"srli.d", GPR64Opnd, srl>, R2_IMM6<0b01>;
+def SRAI_D  : Shift_Imm64<"srai.d", GPR64Opnd, sra>, R2_IMM6<0b10>;
+def ROTRI_D : Shift_Imm64<"rotri.d", GPR64Opnd, rotr>, R2_IMM6<0b11>;
+///
+/// Misc
+///
+def ALSL_WU    : Reg3_SaU<"alsl.wu", GPR64Opnd, GPR32Opnd, uimm2_plus1>, R3_SA2<0b00011> {
+  let Pattern = [(set GPR64Opnd:$rd,
+               (i64 (zext (add GPR32Opnd:$rk, (shl GPR32Opnd:$rj, immZExt2Alsl:$sa)))))];
+}
+
+def ALSL_D     : Reg3_Sa<"alsl.d", GPR64Opnd, uimm2_plus1>, R3_SA2<0b10110> {
+  let Pattern = [(set GPR64Opnd:$rd,
+                  (add GPR64Opnd:$rk, (shl GPR64Opnd:$rj, immZExt2Alsl:$sa)))];
+}
+def BYTEPICK_D : Reg3_Sa<"bytepick.d", GPR64Opnd, uimm3>, R3_SA3; //[]
+
+def ASRTLE_D : Assert<"asrtle.d", GPR64Opnd, int_loongarch_asrtle_d>, ASSERT<0b10>;
+def ASRTGT_D : Assert<"asrtgt.d", GPR64Opnd, int_loongarch_asrtgt_d>, ASSERT<0b11>;
+
+def DBCL : Code15<"dbcl">, CODE15<0b1010101>;
+def HYPCALL : Code15<"hypcall">, CODE15<0b1010111>;
+
+///
+/// R2_IMM12
+///
+def SLTI    : SetCC_I<"slti", GPR64Opnd, simm12, setlt>, R2_IMM12<0b000>;
+def SLTUI   : SetCC_I<"sltui", GPR64Opnd, simm12, setult>, R2_IMM12<0b001>;
+def ADDI_D  : Int_Reg2_Imm12<"addi.d", GPR64Opnd, simm12, add>, R2_IMM12<0b011>;
+def LU52I_D : Int_Reg2_Imm12<"lu52i.d", GPR64Opnd, simm12>, R2_IMM12<0b100>;
+def ANDI : Int_Reg2_Imm12<"andi", GPR64Opnd, uimm12, and>, R2_IMM12<0b101>;
+def ORI  : Int_Reg2_Imm12<"ori", GPR64Opnd, uimm12, or>, R2_IMM12<0b110>;
+def XORI : Int_Reg2_Imm12<"xori", GPR64Opnd, uimm12, xor>, R2_IMM12<0b111>;
+
+///
+/// Privilege Instructions
+///
+def CSRRD : CSR<"csrrd", GPR64Opnd, uimm14, int_loongarch_dcsrrd>, R1_CSR<0b0000000000100>;
+def CSRWR : CSRW<"csrwr", GPR64Opnd, uimm14, int_loongarch_dcsrwr>, R1_CSR<0b0000100000100>;
+def CSRXCHG : CSRX<"csrxchg", GPR64Opnd, uimm14, int_loongarch_dcsrxchg>, R2_CSR<0b00000100>;
+def IOCSRRD_D : Int_Reg2_Iocsrrd<"iocsrrd.d", GPR64Opnd, GPR32Opnd, int_loongarch_iocsrrd_d>, R2P<0b011>;
+def IOCSRWR_D : Int_Reg2_Iocsrwr<"iocsrwr.d", GPR64Opnd, GPR32Opnd, int_loongarch_iocsrwr_d>, R2P<0b111>;
+def CACOP : CAC<"cacop", GPR64Opnd, simm12, int_loongarch_dcacop>, R1_CACHE;
+def LDDIR : LEVEL<"lddir", GPR64Opnd>, R2_LEVEL<0b00000110010000>;
+def LDPTE : SEQ<"ldpte", GPR64Opnd>, R1_SEQ<0b00000110010001>;
+
+def IDLE : Wait<"idle">, WAIT_FM;
+def INVTLB : Invtlb<"invtlb", GPR64Opnd>, R2_INVTLB;
+//
+def IOCSRRD_B : Int_Reg2<"iocsrrd.b", GPR64Opnd>, R2P<0b000>;
+def IOCSRRD_H : Int_Reg2<"iocsrrd.h", GPR64Opnd>, R2P<0b001>;
+def IOCSRRD_W : Int_Reg2<"iocsrrd.w", GPR64Opnd>, R2P<0b010>;
+//
+def TLBCLR   : OP32<"tlbclr", int_loongarch_tlbclr>, IMM32<0b001000>;
+def TLBFLUSH : OP32<"tlbflush", int_loongarch_tlbflush>, IMM32<0b001001>;
+def TLBSRCH     : OP32<"tlbsrch", int_loongarch_tlbsrch>, IMM32<0b001010>;
+def TLBRD     : OP32<"tlbrd", int_loongarch_tlbrd>, IMM32<0b001011>;
+def TLBWR    : OP32<"tlbwr", int_loongarch_tlbwr>, IMM32<0b001100>;
+def TLBFILL    : OP32<"tlbfill", int_loongarch_tlbfill>, IMM32<0b001101>;
+def ERTN     : OP32<"ertn">, IMM32<0b001110>;
+
+///
+/// R1_IMM20
+///
+def ADDU16I_D : SI16_R2<"addu16i.d", GPR64Opnd>, R2_SI16<0b000100>;
+def LU12I_W   : SI20<"lu12i.w", GPR64Opnd, simm20>, R1_SI20<0b0001010>;
+def LU32I_D   : SI20<"lu32i.d", GPR64Opnd, simm20>, R1_SI20<0b0001011>;
+def PCADDI    : SI20<"pcaddi", GPR64Opnd, simm20>, R1_SI20<0b0001100>;
+def PCALAU12I : SI20<"pcalau12i", GPR64Opnd, simm20>, R1_SI20<0b0001101>;
+def PCADDU12I : SI20<"pcaddu12i", GPR64Opnd, simm20>, R1_SI20<0b0001110>;
+def PCADDU18I : SI20<"pcaddu18i", GPR64Opnd, simm20>, R1_SI20<0b0001111>;
+
+
+def BEQZ  : Beqz<"beqz", brtarget, seteq, GPR64Opnd>, R1_IMM21BEQZ<0b010000>;
+def BNEZ  : Beqz<"bnez", brtarget, setne, GPR64Opnd>, R1_IMM21BEQZ<0b010001>;
+
+def JIRL  : FJirl<"jirl", simm16, GPR64Opnd>, R2_IMM16JIRL;
+let isCall = 1, isCTI=1, isCodeGenOnly = 1 in {
+def JIRL_CALL  : FJirl<"jirl", simm16, GPR64Opnd>, R2_IMM16JIRL;
+}
+
+def B     : JumpFB<jmptarget, "b", br, bb>, IMM26B<0b010100>;
+
+def BEQ   : Beq<"beq", brtarget, seteq, GPR64Opnd>, R2_IMM16BEQ<0b010110>;
+def BNE   : Beq<"bne", brtarget, setne, GPR64Opnd>, R2_IMM16BEQ<0b010111>;
+def BLT   : Beq<"blt", brtarget, setlt, GPR64Opnd>, R2_IMM16BEQ<0b011000>;
+def BGE   : Beq<"bge", brtarget, setge, GPR64Opnd>, R2_IMM16BEQ<0b011001>;
+def BLTU  : Beq<"bltu", brtarget, setult, GPR64Opnd>, R2_IMM16BEQ<0b011010>;
+def BGEU  : Beq<"bgeu", brtarget, setuge, GPR64Opnd>, R2_IMM16BEQ<0b011011>;
+
+///
+/// Mem access
+///
+class LLBase<string opstr, RegisterOperand RO, DAGOperand MO = mem> :
+  InstForm<(outs RO:$rd), (ins MO:$addr), !strconcat(opstr, "\t$rd, $addr"),
+           [], FrmI, opstr> {
+  let DecoderMethod = "DecodeMemSimm14";
+  let mayLoad = 1;
+}
+
+class SCBase<string opstr, RegisterOperand RO, DAGOperand MO = mem> :
+  InstForm<(outs RO:$dst), (ins RO:$rd, MO:$addr),
+           !strconcat(opstr, "\t$rd, $addr"), [], FrmI> {
+  let DecoderMethod = "DecodeMemSimm14";
+  let mayStore = 1;
+  let Constraints = "$rd = $dst";
+}
+
+def LL_D : LLBase<"ll.d", GPR64Opnd, mem_simm14_lsl2>, LL_SC<0b010>;
+def SC_D : SCBase<"sc.d", GPR64Opnd, mem_simm14_lsl2>, LL_SC<0b011>;
+
+def LDPTR_W : LdPtr<"ldptr.w", GPR64Opnd>, LL_SC<0b100>;
+def STPTR_W : StPtr<"stptr.w", GPR64Opnd>, LL_SC<0b101>;
+def LDPTR_D : LdPtr<"ldptr.d", GPR64Opnd>, LL_SC<0b110>;
+def STPTR_D : StPtr<"stptr.d", GPR64Opnd>, LL_SC<0b111>;
+
+def LD_B  : Ld<"ld.b", GPR64Opnd, mem, sextloadi8>, LOAD_STORE<0b0000>;
+def LD_H  : Ld<"ld.h", GPR64Opnd, mem, sextloadi16>, LOAD_STORE<0b0001>;
+def LD_W  : Ld<"ld.w", GPR64Opnd, mem, sextloadi32>, LOAD_STORE<0b0010>;
+def LD_D  : Ld<"ld.d", GPR64Opnd, mem_simmptr, load>, LOAD_STORE<0b0011>;
+def ST_B  : St<"st.b", GPR64Opnd, mem, truncstorei8>, LOAD_STORE<0b0100>;
+def ST_H  : St<"st.h", GPR64Opnd, mem, truncstorei16>, LOAD_STORE<0b0101>;
+def ST_W  : St<"st.w", GPR64Opnd, mem, truncstorei32>, LOAD_STORE<0b0110>;
+def ST_D  : St<"st.d", GPR64Opnd, mem_simmptr, store>, LOAD_STORE<0b0111>;
+def LD_BU : Ld<"ld.bu", GPR64Opnd, mem, zextloadi8>, LOAD_STORE<0b1000>;
+def LD_HU : Ld<"ld.hu", GPR64Opnd, mem, zextloadi16>, LOAD_STORE<0b1001>;
+def LD_WU : Ld<"ld.wu", GPR64Opnd, mem, zextloadi32>, LOAD_STORE<0b1010>;
+
+def AMSWAP_W  : ATOMIC<"amswap.w", GPR32Opnd, mem>, AM<0b000000>;
+def AMSWAP_D  : ATOMIC<"amswap.d", GPR64Opnd, mem>, AM<0b000001>;
+def AMADD_W   : ATOMIC<"amadd.w", GPR32Opnd, mem>, AM<0b000010>;
+def AMADD_D   : ATOMIC<"amadd.d", GPR64Opnd, mem>, AM<0b000011>;
+def AMAND_W   : ATOMIC<"amand.w", GPR32Opnd, mem>, AM<0b000100>;
+def AMAND_D   : ATOMIC<"amand.d", GPR64Opnd, mem>, AM<0b000101>;
+def AMOR_W    : ATOMIC<"amor.w", GPR32Opnd, mem>, AM<0b000110>;
+def AMOR_D    : ATOMIC<"amor.d", GPR64Opnd, mem>, AM<0b000111>;
+def AMXOR_W   : ATOMIC<"amxor.w", GPR32Opnd, mem>, AM<0b001000>;
+def AMXOR_D   : ATOMIC<"amxor.d", GPR64Opnd, mem>, AM<0b001001>;
+def AMMAX_W   : ATOMIC<"ammax.w", GPR32Opnd, mem>, AM<0b001010>;
+def AMMAX_D   : ATOMIC<"ammax.d", GPR64Opnd, mem>, AM<0b001011>;
+def AMMIN_W   : ATOMIC<"ammin.w", GPR32Opnd, mem>, AM<0b001100>;
+def AMMIN_D   : ATOMIC<"ammin.d", GPR64Opnd, mem>, AM<0b001101>;
+def AMMAX_WU  : ATOMIC<"ammax.wu", GPR32Opnd, mem>, AM<0b001110>;
+def AMMAX_DU  : ATOMIC<"ammax.du", GPR64Opnd, mem>, AM<0b001111>;
+def AMMIN_WU  : ATOMIC<"ammin.wu", GPR32Opnd, mem>, AM<0b010000>;
+def AMMIN_DU  : ATOMIC<"ammin.du", GPR64Opnd, mem>, AM<0b010001>;
+
+
+def AMSWAP_DB_W  : ATOMIC<"amswap_db.w", GPR32Opnd, mem>, AM<0b010010>;
+def AMSWAP_DB_D  : ATOMIC<"amswap_db.d", GPR64Opnd, mem>, AM<0b010011>;
+def AMADD_DB_W   : ATOMIC<"amadd_db.w", GPR32Opnd, mem>, AM<0b010100>;
+def AMADD_DB_D   : ATOMIC<"amadd_db.d", GPR64Opnd, mem>, AM<0b010101>;
+def AMAND_DB_W   : ATOMIC<"amand_db.w", GPR32Opnd, mem>, AM<0b010110>;
+def AMAND_DB_D   : ATOMIC<"amand_db.d", GPR64Opnd, mem>, AM<0b010111>;
+def AMOR_DB_W    : ATOMIC<"amor_db.w", GPR32Opnd, mem>, AM<0b011000>;
+def AMOR_DB_D    : ATOMIC<"amor_db.d", GPR64Opnd, mem>, AM<0b011001>;
+def AMXOR_DB_W   : ATOMIC<"amxor_db.w", GPR32Opnd, mem>, AM<0b011010>;
+def AMXOR_DB_D   : ATOMIC<"amxor_db.d", GPR64Opnd, mem>, AM<0b011011>;
+def AMMAX_DB_W   : ATOMIC<"ammax_db.w", GPR32Opnd, mem>, AM<0b011100>;
+def AMMAX_DB_D   : ATOMIC<"ammax_db.d", GPR64Opnd, mem>, AM<0b011101>;
+def AMMIN_DB_W   : ATOMIC<"ammin_db.w", GPR32Opnd, mem>, AM<0b011110>;
+def AMMIN_DB_D   : ATOMIC<"ammin_db.d", GPR64Opnd, mem>, AM<0b011111>;
+def AMMAX_DB_WU  : ATOMIC<"ammax_db.wu", GPR32Opnd, mem>, AM<0b100000>;
+def AMMAX_DB_DU  : ATOMIC<"ammax_db.du", GPR64Opnd, mem>, AM<0b100001>;
+def AMMIN_DB_WU  : ATOMIC<"ammin_db.wu", GPR32Opnd, mem>, AM<0b100010>;
+def AMMIN_DB_DU  : ATOMIC<"ammin_db.du", GPR64Opnd, mem>, AM<0b100011>;
+
+def LDGT_B : Int_Reg3<"ldgt.b", GPR64Opnd>, R3MI<0b11110000>;
+def LDGT_H : Int_Reg3<"ldgt.h", GPR64Opnd>, R3MI<0b11110001>;
+def LDGT_W : Int_Reg3<"ldgt.w", GPR64Opnd>, R3MI<0b11110010>;
+def LDGT_D : Int_Reg3<"ldgt.d", GPR64Opnd>, R3MI<0b11110011>;
+def LDLE_B : Int_Reg3<"ldle.b", GPR64Opnd>, R3MI<0b11110100>;
+def LDLE_H : Int_Reg3<"ldle.h", GPR64Opnd>, R3MI<0b11110101>;
+def LDLE_W : Int_Reg3<"ldle.w", GPR64Opnd>, R3MI<0b11110110>;
+def LDLE_D : Int_Reg3<"ldle.d", GPR64Opnd>, R3MI<0b11110111>;
+def STGT_B : Int_Reg3<"stgt.b", GPR64Opnd>, R3MI<0b11111000>;
+def STGT_H : Int_Reg3<"stgt.h", GPR64Opnd>, R3MI<0b11111001>;
+def STGT_W : Int_Reg3<"stgt.w", GPR64Opnd>, R3MI<0b11111010>;
+def STGT_D : Int_Reg3<"stgt.d", GPR64Opnd>, R3MI<0b11111011>;
+def STLE_B : Int_Reg3<"stle.b", GPR64Opnd>, R3MI<0b11111100>;
+def STLE_H : Int_Reg3<"stle.h", GPR64Opnd>, R3MI<0b11111101>;
+def STLE_W : Int_Reg3<"stle.w", GPR64Opnd>, R3MI<0b11111110>;
+def STLE_D : Int_Reg3<"stle.d", GPR64Opnd>, R3MI<0b11111111>;
+
+let isCodeGenOnly = 1 in {
+def PRELD  : Preld<"preld", mem, GPR64Opnd>, PRELD_FM;
+}
+
+def PRELD_Raw  : Preld_Raw<"preld", GPR64Opnd>, PRELD_FM;
+
+let isCall=1, isCTI=1, Defs = [RA] in {
+  class JumpLink<string opstr, DAGOperand opnd> :
+    InstForm<(outs), (ins opnd:$target), !strconcat(opstr, "\t$target"),
+             [(LoongArchJmpLink tglobaladdr:$target)], FrmJ, opstr> {
+               let DecoderMethod = "DecodeJumpTarget";
+             }
+}
+def LONG_BRANCH_PCADDU12I : LoongArchPseudo<(outs GPR64Opnd:$dst),
+    (ins brtarget:$tgt), []>, GPR_64;
+
+def LONG_BRANCH_ADDID2Op : LoongArchPseudo<(outs GPR64Opnd:$dst),
+    (ins GPR64Opnd:$src, brtarget:$tgt), []>, GPR_64;
+
+def LONG_BRANCH_ADDID : LoongArchPseudo<(outs GPR64Opnd:$dst),
+    (ins GPR64Opnd:$src, brtarget:$tgt, brtarget:$baltgt), []>, GPR_64;
+
+def LEA_ADDI_D: EffectiveAddress<"addi.d", GPR64Opnd>, LEA_ADDI_FM<0b011>, GPR_64;
+
+class PseudoReturnBase<RegisterOperand RO> : LoongArchPseudo<(outs), (ins RO:$rs),
+                                                        []> {
+  let isTerminator = 1;
+  let isBarrier = 1;
+  let isReturn = 1;
+  let isCodeGenOnly = 1;
+  let hasCtrlDep = 1;
+  let hasExtraSrcRegAllocReq = 1;
+  bit isCTI = 1;
+}
+
+def PseudoReturn64 : PseudoReturnBase<GPR64Opnd>;
+//def PseudoReturn : PseudoReturnBase<GPR32Opnd>;
+
+
+let isCall=1, isCTI=1, Defs=[RA], isCodeGenOnly=1 in {
+def PseudoCall : LoongArchPseudo<(outs), (ins calltarget:$target),
+                                        []>;
+}
+
+def : LoongArchPat<(LoongArchJmpLink tglobaladdr:$dst),
+              (PseudoCall tglobaladdr:$dst)>;
+
+def : LoongArchPat<(LoongArchJmpLink (i32 texternalsym:$dst)),
+              (PseudoCall texternalsym:$dst)>;
+def : LoongArchPat<(LoongArchJmpLink (i64 texternalsym:$dst)),
+              (PseudoCall texternalsym:$dst)>;
+
+def : LoongArchPat<(LoongArchJmpLink (i64 texternalsym:$dst)),
+              (PseudoCall texternalsym:$dst)>;
+
+def BL  : JumpLink<"bl", calltarget>, FJ<0b010101>;
+
+class IsAsCheapAsAMove {
+  bit isAsCheapAsAMove = 1;
+}
+class LoadUpper<string opstr, RegisterOperand RO, Operand Imm>:
+  InstForm<(outs RO:$rt), (ins Imm:$imm16), !strconcat(opstr, "\t$rt, $imm16"),
+         [], FrmI, opstr>, IsAsCheapAsAMove {
+  let hasSideEffects = 0;
+  let isReMaterializable = 1;
+  let mayLoad = 1;
+}
+
+let isCodeGenOnly = 1 in {
+def LAPCREL   : LoadUpper<"la.pcrel", GPR64Opnd, uimm16_64_relaxed>, LUI_FM, GPR_64;
+}
+
+def NOP : LoongArchPseudo<(outs), (ins), []>,
+                     PseudoInstExpansion<(ANDI ZERO_64, ZERO_64, 0)>;
+
+def : LoongArchInstAlias<"nop", (ANDI ZERO_64, ZERO_64, 0), 1>;
+def : LoongArchInstAlias<"jr $rd", (JIRL ZERO_64, GPR64Opnd:$rd, 0), 1>;
+def : LoongArchInstAlias<"move $dst, $src",
+                         (OR GPR64Opnd:$dst,  GPR64Opnd:$src, ZERO_64), 1>, GPR_64;
+
+// Materialize i64 constants.
+def : LoongArchPat<(i64 LU12IORIPred:$imm),
+                   (ORI (LU12I_W (HI20 imm:$imm)), (LO12 imm:$imm))>, GPR_64;
+def : LoongArchPat<(i64 LU12IPred:$imm), (LU12I_W (HI20 imm:$imm))>, GPR_64;
+def : LoongArchPat<(i64 ORi12Pred:$imm), (ORI ZERO_64, imm:$imm)>, GPR_64;
+def : LoongArchPat<(i64 immSExt12:$imm), (ADDI_D ZERO_64, imm:$imm)>, GPR_64;
+
+def UImm12RelaxedAsmOperandClass
+: UImmAsmOperandClass<12, [ConstantUImm20AsmOperandClass]> {
+  let Name = "UImm12_Relaxed";
+  let PredicateMethod = "isAnyImm<12>";
+  let DiagnosticType = "UImm12_Relaxed";
+}
+
+def SImm12RelaxedAsmOperandClass
+: SImmAsmOperandClass<12, [UImm12RelaxedAsmOperandClass]> {
+  let Name = "SImm12_Relaxed";
+  let PredicateMethod = "isAnyImm<12>";
+  let DiagnosticType = "SImm12_Relaxed";
+}
+
+def simm12_relaxed : Operand<i32> {
+  let DecoderMethod = "DecodeSImmWithOffsetAndScale<12>";
+  let ParserMatchClass = !cast<AsmOperandClass>("SImm12RelaxedAsmOperandClass");
+}
+
+def : LoongArchPat<(i64 (anyext GPR32:$src)),
+              (INSERT_SUBREG (i64 (IMPLICIT_DEF)), GPR32:$src, sub_32)>,GPR_64;
+
+let usesCustomInserter = 1 in {
+  def ATOMIC_LOAD_ADD_I64  : Atomic2Ops<atomic_load_add_64, GPR64>;
+  def ATOMIC_LOAD_SUB_I64  : Atomic2Ops<atomic_load_sub_64, GPR64>;
+  def ATOMIC_LOAD_AND_I64  : Atomic2Ops<atomic_load_and_64, GPR64>;
+  def ATOMIC_LOAD_OR_I64   : Atomic2Ops<atomic_load_or_64, GPR64>;
+  def ATOMIC_LOAD_XOR_I64  : Atomic2Ops<atomic_load_xor_64, GPR64>;
+  def ATOMIC_LOAD_NAND_I64 : Atomic2Ops<atomic_load_nand_64, GPR64>;
+  def ATOMIC_SWAP_I64      : Atomic2Ops<atomic_swap_64, GPR64>;
+  def ATOMIC_CMP_SWAP_I64  : AtomicCmpSwap<atomic_cmp_swap_64, GPR64>;
+
+  def ATOMIC_LOAD_MAX_I64  : Atomic2Ops<atomic_load_max_64, GPR64>;
+  def ATOMIC_LOAD_MIN_I64  : Atomic2Ops<atomic_load_min_64, GPR64>;
+  def ATOMIC_LOAD_UMAX_I64  : Atomic2Ops<atomic_load_umax_64, GPR64>;
+  def ATOMIC_LOAD_UMIN_I64  : Atomic2Ops<atomic_load_umin_64, GPR64>;
+}
+
+def ATOMIC_LOAD_ADD_I64_POSTRA  : Atomic2OpsPostRA<GPR64>;
+def ATOMIC_LOAD_SUB_I64_POSTRA  : Atomic2OpsPostRA<GPR64>;
+def ATOMIC_LOAD_AND_I64_POSTRA  : Atomic2OpsPostRA<GPR64>;
+def ATOMIC_LOAD_OR_I64_POSTRA   : Atomic2OpsPostRA<GPR64>;
+def ATOMIC_LOAD_XOR_I64_POSTRA  : Atomic2OpsPostRA<GPR64>;
+def ATOMIC_LOAD_NAND_I64_POSTRA : Atomic2OpsPostRA<GPR64>;
+
+def ATOMIC_SWAP_I64_POSTRA      : Atomic2OpsPostRA<GPR64>;
+
+def ATOMIC_CMP_SWAP_I64_POSTRA  : AtomicCmpSwapPostRA<GPR64>;
+
+def ATOMIC_LOAD_MAX_I64_POSTRA  : Atomic2OpsPostRA<GPR64>;
+
+def ATOMIC_LOAD_MIN_I64_POSTRA  : Atomic2OpsPostRA<GPR64>;
+
+def ATOMIC_LOAD_UMAX_I64_POSTRA  : Atomic2OpsPostRA<GPR64>;
+
+def ATOMIC_LOAD_UMIN_I64_POSTRA  : Atomic2OpsPostRA<GPR64>;
+
+def : LoongArchPat<(atomic_load_8 addr:$a), (LD_B addr:$a)>, GPR_64;
+def : LoongArchPat<(atomic_load_16 addr:$a), (LD_H addr:$a)>, GPR_64;
+def : LoongArchPat<(atomic_load_32 addrimm14lsl2:$a), (LDPTR_W addrimm14lsl2:$a)>, GPR_64;
+def : LoongArchPat<(atomic_load_32 addr:$a), (LD_W addr:$a)>, GPR_64;
+def : LoongArchPat<(atomic_load_64 addrimm14lsl2:$a), (LDPTR_D addrimm14lsl2:$a)>, GPR_64;
+def : LoongArchPat<(atomic_load_64 addr:$a), (LD_D addr:$a)>, GPR_64;
+
+def : LoongArchPat<(atomic_store_8 addr:$a, GPR64:$v),
+      (ST_B GPR64:$v, addr:$a)>, GPR_64;
+def : LoongArchPat<(atomic_store_16 addr:$a, GPR64:$v),
+      (ST_H GPR64:$v, addr:$a)>, GPR_64;
+def : LoongArchPat<(atomic_store_32 addrimm14lsl2:$a, GPR64:$v),
+      (STPTR_W GPR64:$v, addrimm14lsl2:$a)>, GPR_64;
+def : LoongArchPat<(atomic_store_32 addr:$a, GPR64:$v),
+      (ST_W GPR64:$v, addr:$a)>, GPR_64;
+def : LoongArchPat<(atomic_store_64 addrimm14lsl2:$a, GPR64:$v),
+      (STPTR_D GPR64:$v, addrimm14lsl2:$a)>, GPR_64;
+def : LoongArchPat<(atomic_store_64 addr:$a, GPR64:$v),
+      (ST_D GPR64:$v, addr:$a)>, GPR_64;
+
+def : LoongArchPat<(bswap GPR64:$rt), (REVH_D (REVB_4H GPR64:$rt))>;
+
+def immZExt5 : ImmLeaf<i32, [{return Imm == (Imm & 0x1f);}]>;
+
+def immZExtRange2To64 : PatLeaf<(imm), [{
+  return isUInt<7>(N->getZExtValue()) && (N->getZExtValue() >= 2) &&
+         (N->getZExtValue() <= 64);
+}]>;
+
+// bstrins and bstrpick
+class InsBase<string opstr, RegisterOperand RO, Operand ImmOpnd,
+                   SDPatternOperator OpNode= null_frag>
+    : InstForm<(outs RO:$rd), (ins RO:$rj, ImmOpnd:$msbd, ImmOpnd:$lsbd, RO:$src),
+          !strconcat(opstr, "\t$rd, $rj, $msbd, $lsbd"),
+          [(set RO:$rd, (OpNode RO:$rj, ImmOpnd:$msbd, ImmOpnd:$lsbd, RO:$src))],
+          FrmR, opstr> {
+  let Constraints = "$src = $rd";
+ }
+
+class InsBase_32<string opstr, RegisterOperand RO, Operand ImmOpnd,
+                   SDPatternOperator OpNode= null_frag>
+    : InstForm<(outs RO:$rd), (ins RO:$rj, ImmOpnd:$msbw, ImmOpnd:$lsbw, RO:$src),
+          !strconcat(opstr, "\t$rd, $rj, $msbw, $lsbw"),
+          [(set RO:$rd, (OpNode RO:$rj, ImmOpnd:$msbw, ImmOpnd:$lsbw, RO:$src))],
+          FrmR, opstr> {
+  let Constraints = "$src = $rd";
+}
+
+class PickBase<string opstr, RegisterOperand RO, Operand ImmOpnd,
+               SDPatternOperator Op = null_frag>
+    : InstForm<(outs RO:$rd), (ins RO:$rj, ImmOpnd:$msbd, ImmOpnd:$lsbd),
+               !strconcat(opstr, "\t$rd, $rj, $msbd, $lsbd"),
+               [(set RO:$rd, (Op RO:$rj, ImmOpnd:$msbd, ImmOpnd:$lsbd))],
+               FrmR, opstr>;
+
+class PickBase_32<string opstr, RegisterOperand RO,  Operand ImmOpnd,
+                  SDPatternOperator Op = null_frag>
+    : InstForm<(outs RO:$rd), (ins RO:$rj, ImmOpnd:$msbw, ImmOpnd:$lsbw),
+               !strconcat(opstr, "\t$rd, $rj, $msbw, $lsbw"),
+               [(set RO:$rd, (Op RO:$rj, ImmOpnd:$msbw, ImmOpnd:$lsbw))],
+               FrmR, opstr>;
+
+ def BSTRINS_D  : InsBase<"bstrins.d", GPR64Opnd, uimm6, LoongArchBstrins>,
+                  INSERT_BIT64<0>;
+ def BSTRPICK_D  : PickBase<"bstrpick.d", GPR64Opnd, uimm6, LoongArchBstrpick>,
+                   INSERT_BIT64<1>;
+
+let isCodeGenOnly = 1 in {
+  def ZEXT64_32 : InstForm<(outs GPR64Opnd:$rd),
+                         (ins GPR32Opnd:$rj, uimm6:$msbd,
+                              uimm6:$lsbd),
+                         "bstrpick.d $rd, $rj, $msbd, $lsbd", [], FrmR, "bstrpick.d">,
+                          INSERT_BIT64<1>;
+}
+
+//32-to-64-bit extension
+def : LoongArchPat<(i64 (zext GPR32:$src)), (ZEXT64_32 GPR32:$src, 31, 0)>;
+def : LoongArchPat<(i64 (extloadi1  addr:$src)), (LD_B addr:$src)>,
+      GPR_64;
+def : LoongArchPat<(i64 (extloadi8  addr:$src)), (LD_B addr:$src)>,
+      GPR_64;
+def : LoongArchPat<(i64 (extloadi16 addr:$src)), (LD_H addr:$src)>,
+      GPR_64;
+def : LoongArchPat<(i64 (extloadi32 addr:$src)), (LD_W addr:$src)>,
+      GPR_64;
+
+class LDX_FT_LA<string opstr, RegisterOperand DRC,
+                SDPatternOperator OpNode = null_frag> :
+  InstForm<(outs DRC:$rd), (ins PtrRC:$rj, PtrRC:$rk),
+           !strconcat(opstr, "\t$rd, $rj, $rk"),
+           [(set DRC:$rd, (OpNode (add iPTR:$rj, iPTR:$rk)))],
+           FrmR, opstr> {
+  let AddedComplexity = 20;
+  let canFoldAsLoad = 1;
+  string BaseOpcode = opstr;
+  let mayLoad = 1;
+}
+
+class SDX_FT_LA<string opstr, RegisterOperand DRC,
+                SDPatternOperator OpNode = null_frag> :
+  InstForm<(outs), (ins DRC:$rd, PtrRC:$rj, PtrRC:$rk),
+           !strconcat(opstr, "\t$rd, $rj, $rk"),
+           [(OpNode DRC:$rd, (add iPTR:$rj, iPTR:$rk))],
+           FrmI, opstr> {
+  string BaseOpcode = opstr;
+  let mayStore = 1;
+  let AddedComplexity = 20;
+}
+
+
+def LDX_B : LDX_FT_LA<"ldx.b", GPR64Opnd, sextloadi8>,
+            R3MI<0b00000000>;
+def LDX_H : LDX_FT_LA<"ldx.h", GPR64Opnd, sextloadi16>,
+            R3MI<0b00001000>;
+def LDX_W : LDX_FT_LA<"ldx.w", GPR64Opnd, sextloadi32>,
+            R3MI<0b00010000>;
+def LDX_D : LDX_FT_LA<"ldx.d", GPR64Opnd, load>,
+            R3MI<0b00011000>;
+def STX_B : SDX_FT_LA<"stx.b", GPR64Opnd, truncstorei8>,
+            R3MI<0b00100000>;
+def STX_H : SDX_FT_LA<"stx.h", GPR64Opnd, truncstorei16>,
+            R3MI<0b00101000>;
+def STX_W : SDX_FT_LA<"stx.w", GPR64Opnd, truncstorei32>,
+            R3MI<0b00110000>;
+def STX_D : SDX_FT_LA<"stx.d", GPR64Opnd, store>,
+            R3MI<0b00111000>;
+def LDX_BU : LDX_FT_LA<"ldx.bu", GPR64Opnd, extloadi8>,
+             R3MI<0b01000000>;
+def LDX_HU : LDX_FT_LA<"ldx.hu", GPR64Opnd, extloadi16>,
+             R3MI<0b01001000>;
+def LDX_WU : LDX_FT_LA<"ldx.wu", GPR64Opnd, zextloadi32>,
+             R3MI<0b01010000>;
+
+//def : LoongArchPat<(bswap GPR64:$rj), (REVH_D (REVB_4H GPR64:$rj))>;
+//def : LoongArchPat<(bswap GPR64:$rj), (ROTRI_D (REVB_2W GPR64:$rj), 32)>;
+def : LoongArchPat<(bswap GPR64:$rj), (REVB_D GPR64:$rj)>;
+
+let isCodeGenOnly = 1 in {
+  def SLLI_D_64_32 : Shift_Imm64<"", GPR64Opnd>, R2_IMM6<0b00>, GPR_64 {
+    let imm6 = 0;
+    let AsmString = "slli.d\t$rd, $rj, 32";
+    let InOperandList = (ins GPR32:$rj);
+    let OutOperandList = (outs GPR64:$rd);
+  }
+
+  let isMoveReg = 1, imm5 = 0,
+      AsmString = "slli.w\t$rd, $rj, 0",
+      OutOperandList = (outs GPR64:$rd) in {
+      let InOperandList = (ins GPR32:$rj) in
+        def SLLI_W_64_32 : Shift_Imm32<"", GPR32Opnd>, R2_IMM5<0b00>, GPR_64;
+      let InOperandList = (ins GPR64:$rj) in
+        def SLLI_W_64_64 : Shift_Imm32<"", GPR32Opnd>, R2_IMM5<0b00>, GPR_64;
+  }
+}
+
+// 32-to-64-bit extension
+//def : LoongArchPat<(i64 (zext GPR32:$src)), (SRLI_D (SLLI_D_64_32 GPR32:$src), 32)>, GPR_64;
+def : LoongArchPat<(i64 (sext GPR32:$src)), (SLLI_W_64_32 GPR32:$src)>, GPR_64;
+def : LoongArchPat<(i64 (sext_inreg GPR64:$src, i32)), (SLLI_W_64_64 GPR64:$src)>, GPR_64;
+
+let Uses = [A0, A1], isTerminator = 1, isReturn = 1, isBarrier = 1, isCTI = 1 in {
+  def LoongArcheh_return32 : LoongArchPseudo<(outs), (ins GPR32:$spoff, GPR32:$dst),
+                                [(LoongArchehret GPR32:$spoff, GPR32:$dst)]>;
+  def LoongArcheh_return64 : LoongArchPseudo<(outs), (ins GPR64:$spoff,GPR64:$dst),
+                                [(LoongArchehret GPR64:$spoff, GPR64:$dst)]>;
+}
+
+def : LoongArchPat<(select i32:$cond, i64:$t, i64:$f),
+                   (OR (MASKEQZ i64:$t, (SLLI_W_64_32 i32:$cond)),
+                       (MASKNEZ i64:$f, (SLLI_W_64_32 i32:$cond)))>;
+// setcc patterns
+multiclass SeteqPats<RegisterClass RC, Instruction SLTiuOp, Instruction XOROp,
+                     Instruction SLTuOp, Register ZEROReg> {
+  def : LoongArchPat<(seteq RC:$lhs, 0),
+                     (SLTiuOp RC:$lhs, 1)>;
+  def : LoongArchPat<(setne RC:$lhs, 0),
+                     (SLTuOp ZEROReg, RC:$lhs)>;
+  def : LoongArchPat<(seteq RC:$lhs, RC:$rhs),
+                     (SLTiuOp (XOROp RC:$lhs, RC:$rhs), 1)>;
+  def : LoongArchPat<(setne RC:$lhs, RC:$rhs),
+                     (SLTuOp ZEROReg, (XOROp RC:$lhs, RC:$rhs))>;
+}
+
+multiclass SetlePats<RegisterClass RC, Instruction XORiOp, Instruction SLTOp,
+                     Instruction SLTuOp> {
+  def : LoongArchPat<(setle RC:$lhs, RC:$rhs),
+                     (XORiOp (SLTOp RC:$rhs, RC:$lhs), 1)>;
+  def : LoongArchPat<(setule RC:$lhs, RC:$rhs),
+                     (XORiOp (SLTuOp RC:$rhs, RC:$lhs), 1)>;
+}
+
+multiclass SetgtPats<RegisterClass RC, Instruction SLTOp, Instruction SLTuOp> {
+  def : LoongArchPat<(setgt RC:$lhs, RC:$rhs),
+                     (SLTOp RC:$rhs, RC:$lhs)>;
+  def : LoongArchPat<(setugt RC:$lhs, RC:$rhs),
+                     (SLTuOp RC:$rhs, RC:$lhs)>;
+}
+
+multiclass SetgePats<RegisterClass RC, Instruction XORiOp, Instruction SLTOp,
+                     Instruction SLTuOp> {
+  def : LoongArchPat<(setge RC:$lhs, RC:$rhs),
+                     (XORiOp (SLTOp RC:$lhs, RC:$rhs), 1)>;
+  def : LoongArchPat<(setuge RC:$lhs, RC:$rhs),
+                     (XORiOp (SLTuOp RC:$lhs, RC:$rhs), 1)>;
+}
+
+multiclass SetgeImmPats<RegisterClass RC, Instruction XORiOp,
+                        Instruction SLTiOp, Instruction SLTiuOp> {
+  def : LoongArchPat<(setge RC:$lhs, immSExt12:$rhs),
+                     (XORiOp (SLTiOp RC:$lhs, immSExt12:$rhs), 1)>;
+  def : LoongArchPat<(setuge RC:$lhs, immSExt12:$rhs),
+                     (XORiOp (SLTiuOp RC:$lhs, immSExt12:$rhs), 1)>;
+}
+
+class LoadRegImmPat<Instruction LoadInst, ValueType ValTy, PatFrag Node> :
+  LoongArchPat<(ValTy (Node addrRegImm:$a)), (LoadInst addrRegImm:$a)>;
+
+class StoreRegImmPat<Instruction StoreInst, ValueType ValTy, PatFrag Node> :
+  LoongArchPat<(Node ValTy:$v, addrRegImm:$a), (StoreInst ValTy:$v, addrRegImm:$a)>;
+
+class LoadRegImm14Lsl2Pat<Instruction LoadInst, ValueType ValTy, PatFrag Node> :
+  LoongArchPat<(ValTy (Node addrimm14lsl2:$a)), (LoadInst addrimm14lsl2:$a)>;
+
+class StoreRegImm14Lsl2Pat<Instruction StoreInst, ValueType ValTy, PatFrag Node> :
+  LoongArchPat<(Node ValTy:$v, addrimm14lsl2:$a), (StoreInst ValTy:$v, addrimm14lsl2:$a)>;
+
+// Patterns for loads/stores with a reg+imm operand.
+// let AddedComplexity = 40 so that these instructions are selected instead of
+// LDX/STX which needs one more register and an ANDI instruction.
+let AddedComplexity = 40 in {
+  def : LoadRegImmPat<LD_B, i64, sextloadi8>;
+  def : LoadRegImmPat<LD_H, i64, sextloadi16>;
+  def : LoadRegImmPat<LD_W, i64, sextloadi32>;
+  def : LoadRegImmPat<LD_D, i64, load>;
+  def : LoadRegImmPat<LD_BU, i64, zextloadi8>;
+  def : LoadRegImmPat<LD_HU, i64, zextloadi16>;
+  def : LoadRegImmPat<LD_WU, i64, zextloadi32>;
+  def : StoreRegImmPat<ST_B, i64, truncstorei8>;
+  def : StoreRegImmPat<ST_H, i64, truncstorei16>;
+  def : StoreRegImmPat<ST_W, i64, truncstorei32>;
+  def : StoreRegImmPat<ST_D, i64, store>;
+
+  def : LoadRegImm14Lsl2Pat<LDPTR_W, i64, sextloadi32>;
+  def : LoadRegImm14Lsl2Pat<LDPTR_D, i64, load>;
+  def : StoreRegImm14Lsl2Pat<STPTR_W, i64, truncstorei32>;
+  def : StoreRegImm14Lsl2Pat<STPTR_D, i64, store>;
+}
+
+//===----------------------------------------------------------------------===//
+// Base Extension Support
+//===----------------------------------------------------------------------===//
+
+include "LoongArch32InstrInfo.td"
+include "LoongArchInstrInfoF.td"
+
+defm : SeteqPats<GPR64, SLTUI, XOR, SLTU, ZERO_64>, GPR_64;
+defm : SetlePats<GPR64, XORI32, SLT, SLTU>, GPR_64;
+defm : SetgtPats<GPR64, SLT, SLTU>, GPR_64;
+defm : SetgePats<GPR64, XORI32, SLT, SLTU>, GPR_64;
+defm : SetgeImmPats<GPR64, XORI32, SLTI, SLTUI>, GPR_64;
+
+///
+/// for relocation
+///
+let isCodeGenOnly = 1 in {
+def PCADDU12I_ri : SI20<"pcaddu12i", GPR64Opnd, simm20>, R1_SI20<0b0001110>;
+def PCADDU12I_rii : RELOC_rii<"pcaddu12i", GPR64Opnd, simm20>, R1_SI20<0b0001110>;
+def ORI_rri : Int_Reg2_Imm12<"ori", GPR64Opnd, uimm12, or>, R2_IMM12<0b110>;
+def ORI_rrii : RELOC_rrii<"ori", GPR64Opnd, uimm12>, R2_IMM12<0b110>;
+def LU12I_W_ri   : SI20<"lu12i.w", GPR64Opnd, simm20>, R1_SI20<0b0001010>;
+def LU32I_D_ri : SI20<"lu32i.d", GPR64Opnd, simm20>, R1_SI20<0b0001011>;
+def LU32I_D_rii : RELOC_rii<"lu32i.d", GPR64Opnd, simm20>, R1_SI20<0b0001011>;
+def LU52I_D_rri : Int_Reg2_Imm12<"lu52i.d", GPR64Opnd, simm12>, R2_IMM12<0b100>;
+def LU52I_D_rrii : RELOC_rrii<"lu52i.d", GPR64Opnd, simm12>, R2_IMM12<0b100>;
+def ADDI_D_rri : Int_Reg2_Imm12<"addi.d", GPR64Opnd, simm12, add>, R2_IMM12<0b011>;
+def ADDI_D_rrii : RELOC_rrii<"addi.d", GPR64Opnd, simm12>, R2_IMM12<0b011>;
+def LD_D_rri : Ld<"ld.d", GPR64Opnd, mem_simmptr, load>, LOAD_STORE<0b0011>;
+def LD_D_rrii : RELOC_rrii<"ld.d", GPR64Opnd, simm12>, LOAD_STORE_RRI<0b0011>;
+def ADD_D_rrr : Int_Reg3<"add.d", GPR64Opnd, add>, R3I<0b0100001>;
+def LDX_D_rrr : LDX_FT_LA<"ldx.d", GPR64Opnd, load>,
+                R3MI<0b00011000>;
+}
+
+//===----------------------------------------------------------------------===//
+// Assembler Pseudo Instructions
+//===----------------------------------------------------------------------===//
+def LoadImm32 : LoongArchAsmPseudoInst<(outs GPR32Opnd:$rd),
+                                       (ins uimm32_coerced:$imm32),
+                                       "li.w\t$rd, $imm32">;
+def LoadImm64 : LoongArchAsmPseudoInst<(outs GPR64Opnd:$rd),
+                                       (ins imm64:$imm64),
+                                       "li.d\t$rd, $imm64">;
+// load address
+def LoadAddrLocal : LoongArchAsmPseudoInst<(outs GPR64Opnd:$rd),
+                                           (ins imm64:$imm64),
+                                           "la.local\t$rd, $imm64">;
+def LoadAddrGlobal : LoongArchAsmPseudoInst<(outs GPR64Opnd:$rd),
+                                            (ins imm64:$imm64),
+                                            "la.global\t$rd, $imm64">;
+def LoadAddrGlobal_Alias : LoongArchAsmPseudoInst<(outs GPR64Opnd:$rd),
+                                                  (ins imm64:$imm64),
+                                                  "la\t$rd, $imm64">;
+def LoadAddrTLS_LE : LoongArchAsmPseudoInst<(outs GPR64Opnd:$rd),
+                                            (ins imm64:$imm64),
+                                            "la.tls.le\t$rd, $imm64">;
+def LoadAddrTLS_IE : LoongArchAsmPseudoInst<(outs GPR64Opnd:$rd),
+                                            (ins imm64:$imm64),
+                                            "la.tls.ie\t$rd, $imm64">;
+def LoadAddrTLS_GD : LoongArchAsmPseudoInst<(outs GPR64Opnd:$rd),
+                                            (ins imm64:$imm64),
+                                            "la.tls.gd\t$rd, $imm64">;
+def LoadAddrTLS_LD : LoongArchAsmPseudoInst<(outs GPR64Opnd:$rd),
+                                            (ins imm64:$imm64),
+                                            "la.tls.ld\t$rd, $imm64">;
+
+// load address with a temp reg
+def LoadAddrLocalRR : LoongArchAsmPseudoInst<(outs GPR64Opnd:$rd),
+                                      (ins GPR64Opnd:$rt, imm64:$imm64),
+                                      "la.local\t$rd, $rt, $imm64">;
+def LoadAddrGlobalRR : LoongArchAsmPseudoInst<(outs GPR64Opnd:$rd),
+                                      (ins GPR64Opnd:$rt, imm64:$imm64),
+                                      "la.global\t$rd, $rt, $imm64">;
+def LoadAddrTLS_IE_RR : LoongArchAsmPseudoInst<(outs GPR64Opnd:$rd),
+                                      (ins GPR64Opnd:$rt, imm64:$imm64),
+                                      "la.tls.ie\t$rd, $rt, $imm64">;
+def LoadAddrTLS_GD_RR : LoongArchAsmPseudoInst<(outs GPR64Opnd:$rd),
+                                      (ins GPR64Opnd:$rt, imm64:$imm64),
+                                      "la.tls.gd\t$rd, $rt, $imm64">;
+def LoadAddrTLS_LD_RR : LoongArchAsmPseudoInst<(outs GPR64Opnd:$rd),
+                                      (ins GPR64Opnd:$rt, imm64:$imm64),
+                                      "la.tls.ld\t$rd, $rt, $imm64">;
+
+// trap when div zero
+def PseudoTEQ : LoongArchPseudo<(outs), (ins GPR64Opnd:$rt), []>;
+
+
+def : LoongArchPat<(i64 (sext (i32 (add GPR32:$src, immSExt12:$imm12)))),
+              (INSERT_SUBREG (i64 (IMPLICIT_DEF)),
+              (ADDI_W GPR32:$src, immSExt12:$imm12), sub_32)>;
+
+def : LoongArchPat<(i64 (sext (i32 (add GPR32:$src, GPR32:$src2)))),
+              (INSERT_SUBREG (i64 (IMPLICIT_DEF)),
+              (ADD_W GPR32:$src, GPR32:$src2), sub_32)>;
+
+def : LoongArchPat<(i64 (sext (i32 (sub GPR32:$src, GPR32:$src2)))),
+              (INSERT_SUBREG (i64 (IMPLICIT_DEF)),
+              (SUB_W GPR32:$src, GPR32:$src2), sub_32)>;
+
+def : LoongArchPat<(i64 (sext (i32 (mul GPR32:$src, GPR32:$src2)))),
+              (INSERT_SUBREG (i64 (IMPLICIT_DEF)),
+              (MUL_W GPR32:$src, GPR32:$src2), sub_32)>;
+
+def : LoongArchPat<(i64 (sext (i32 (shl GPR32:$src, immZExt5:$imm5)))),
+              (INSERT_SUBREG (i64 (IMPLICIT_DEF)),
+              (SLLI_W GPR32:$src, immZExt5:$imm5), sub_32)>;
+
+def : LoongArchPat<(i64 (sext (i32 (shl GPR32:$src, GPR32:$src2)))),
+              (INSERT_SUBREG (i64 (IMPLICIT_DEF)),
+              (SLL_W GPR32:$src, GPR32:$src2), sub_32)>;
+
+def : LoongArchPat<(i64 (sext (i32 (srl GPR32:$src, immZExt5:$imm5)))),
+              (INSERT_SUBREG (i64 (IMPLICIT_DEF)),
+              (SRLI_W GPR32:$src, immZExt5:$imm5), sub_32)>;
+
+def : LoongArchPat<(i64 (sext (i32 (srl GPR32:$src, GPR32:$src2)))),
+              (INSERT_SUBREG (i64 (IMPLICIT_DEF)),
+              (SRL_W GPR32:$src, GPR32:$src2), sub_32)>;
+
+def : LoongArchPat<(i64 (sext (i32 (sra GPR32:$src, immZExt5:$imm5)))),
+              (INSERT_SUBREG (i64 (IMPLICIT_DEF)),
+              (SRAI_W GPR32:$src, immZExt5:$imm5), sub_32)>;
+
+def : LoongArchPat<(i64 (sext (i32 (sra GPR32:$src, GPR32:$src2)))),
+              (INSERT_SUBREG (i64 (IMPLICIT_DEF)),
+              (SRA_W GPR32:$src, GPR32:$src2), sub_32)>;
diff --git a/llvm/lib/Target/LoongArch/LoongArchInstrInfoF.td b/llvm/lib/Target/LoongArch/LoongArchInstrInfoF.td
new file mode 100644
index 000000000000..9eef35817a99
--- /dev/null
+++ b/llvm/lib/Target/LoongArch/LoongArchInstrInfoF.td
@@ -0,0 +1,626 @@
+//===- LoongArchInstrInfoF.td - Target Description for LoongArch Target -*- tablegen -*-=//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+//
+// This file contains the LoongArch implementation of the TargetInstrInfo class.
+//
+//===----------------------------------------------------------------------===//
+// FP immediate patterns.
+def fpimm0 : PatLeaf<(fpimm), [{
+  return N->isExactlyValue(+0.0);
+}]>;
+
+def fpimm0neg : PatLeaf<(fpimm), [{
+  return N->isExactlyValue(-0.0);
+}]>;
+
+def fpimm1 : PatLeaf<(fpimm), [{
+  return N->isExactlyValue(+1.0);
+}]>;
+
+def IsNotSoftFloat   : Predicate<"!Subtarget->useSoftFloat()">,
+                       AssemblerPredicate<(all_of FeatureSoftFloat)>;
+
+class HARDFLOAT { list<Predicate> HardFloatPredicate = [IsNotSoftFloat]; }
+
+def SDT_LoongArchTruncIntFP : SDTypeProfile<1, 1, [SDTCisFP<0>, SDTCisFP<1>]>;
+
+def LoongArchTruncIntFP : SDNode<"LoongArchISD::TruncIntFP", SDT_LoongArchTruncIntFP>;
+
+def SDT_LoongArchFPBrcond : SDTypeProfile<0, 3, [SDTCisInt<0>,
+                                            SDTCisVT<1, i32>,
+                                            SDTCisVT<2, OtherVT>]>;
+
+def LoongArchFPBrcond : SDNode<"LoongArchISD::FPBrcond", SDT_LoongArchFPBrcond,
+                          [SDNPHasChain, SDNPOptInGlue]>;
+
+def SDT_LoongArchCMovFP : SDTypeProfile<1, 3, [SDTCisSameAs<0, 1>, SDTCisVT<2, i32>,
+                                          SDTCisSameAs<1, 3>]>;
+
+def LoongArchCMovFP_T : SDNode<"LoongArchISD::CMovFP_T", SDT_LoongArchCMovFP, [SDNPInGlue]>;
+
+def LoongArchCMovFP_F : SDNode<"LoongArchISD::CMovFP_F", SDT_LoongArchCMovFP, [SDNPInGlue]>;
+
+def SDT_LoongArchFPCmp : SDTypeProfile<0, 3, [SDTCisSameAs<0, 1>, SDTCisFP<1>,
+                                         SDTCisVT<2, i32>]>;
+
+def LoongArchFPCmp : SDNode<"LoongArchISD::FPCmp", SDT_LoongArchFPCmp, [SDNPOutGlue]>;
+
+//===---------------------------------------------------------------------===/
+//Instruction Class Templates
+//===---------------------------------------------------------------------===/
+
+class Float_MOVF<string opstr, RegisterOperand RO, RegisterOperand RC,
+                 SDPatternOperator OpNode= null_frag>
+    : InstForm<(outs RO:$rd), (ins RC:$fj),
+          !strconcat(opstr, "\t$rd, $fj"),
+          [(set RO:$rd, (OpNode RC:$fj))],
+          FrmFR, opstr>, HARDFLOAT {
+     let isMoveReg = 1;
+}
+
+class Float_MOVT<string opstr, RegisterOperand RO, RegisterOperand RC,
+                 SDPatternOperator OpNode= null_frag>
+    : InstForm<(outs RO:$fd), (ins RC:$rj),
+          !strconcat(opstr, "\t$fd, $rj"),
+          [(set RO:$fd, (OpNode RC:$rj))],
+          FrmFR, opstr>, HARDFLOAT {
+     let isMoveReg = 1;
+}
+
+class Float_CVT<string opstr, RegisterOperand RO, RegisterOperand RS,
+                 SDPatternOperator OpNode= null_frag>
+    : InstForm<(outs RO:$fd), (ins RS:$fj),
+          !strconcat(opstr, "\t$fd, $fj"),
+          [(set RO:$fd, (OpNode RS:$fj))],
+          FrmFR, opstr>,
+      HARDFLOAT {
+    let hasSideEffects = 0;
+}
+
+/// float mov
+class Gpr_2_Fcsr<string opstr, RegisterOperand RO,
+                 SDPatternOperator OpNode= null_frag>
+    : InstForm<(outs FCSROpnd:$fcsr), (ins RO:$rj),
+               !strconcat(opstr, "\t$fcsr, $rj"),
+               [(set FCSROpnd:$fcsr, (OpNode RO:$rj))],
+               FrmR, opstr>;
+class Fcsr_2_Gpr<string opstr, RegisterOperand RO,
+                 SDPatternOperator OpNode= null_frag>
+    : InstForm<(outs RO:$rd), (ins FCSROpnd:$fcsr),
+               !strconcat(opstr, "\t$rd, $fcsr"),
+               [(set RO:$rd, (OpNode FCSROpnd:$fcsr))],
+               FrmR, opstr>;
+class Fgr_2_Fcfr<string opstr, RegisterOperand RO,
+                 SDPatternOperator OpNode= null_frag>
+    : InstForm<(outs FCFROpnd:$cd), (ins RO:$fj),
+                !strconcat(opstr, "\t$cd, $fj"),
+                [(set FCFROpnd:$cd, (OpNode RO:$fj))],
+                FrmR, opstr>;
+class Fcfr_2_Fgr<string opstr, RegisterOperand RO,
+                 SDPatternOperator OpNode= null_frag>
+    : InstForm<(outs RO:$fd), (ins FCFROpnd:$cj),
+               !strconcat(opstr, "\t$fd, $cj"),
+               [(set RO:$fd, (OpNode FCFROpnd:$cj))],
+               FrmR, opstr>;
+class Gpr_2_Fcfr<string opstr, RegisterOperand RO,
+                 SDPatternOperator OpNode= null_frag>
+    : InstForm<(outs FCFROpnd:$cd), (ins RO:$rj),
+                !strconcat(opstr, "\t$cd, $rj"),
+                [(set FCFROpnd:$cd, (OpNode RO:$rj))],
+                FrmR, opstr>;
+class Fcfr_2_Gpr<string opstr, RegisterOperand RO,
+                 SDPatternOperator OpNode= null_frag>
+    : InstForm<(outs RO:$rd), (ins FCFROpnd:$cj),
+               !strconcat(opstr, "\t$rd, $cj"),
+               [(set RO:$rd, (OpNode FCFROpnd:$cj))],
+               FrmR, opstr>;
+
+class FLDX<string opstr, RegisterOperand DRC,
+           SDPatternOperator OpNode = null_frag> :
+  InstForm<(outs DRC:$fd), (ins PtrRC:$rj, PtrRC:$rk),
+           !strconcat(opstr, "\t$fd, $rj, $rk"),
+           [(set DRC:$fd, (OpNode (add iPTR:$rj, iPTR:$rk)))],
+           FrmR, opstr> {
+  let AddedComplexity = 20;
+}
+
+class FSTX<string opstr, RegisterOperand DRC,
+           SDPatternOperator OpNode = null_frag> :
+  InstForm<(outs), (ins DRC:$fd, PtrRC:$rj, PtrRC:$rk),
+           !strconcat(opstr, "\t$fd, $rj, $rk"),
+           [(OpNode DRC:$fd, (add iPTR:$rj, iPTR:$rk))],
+           FrmR, opstr> {
+  let AddedComplexity = 20;
+}
+
+/// f{maxa/mina}.{s/d}
+class Float_Reg3_Fmaxa<string opstr, RegisterOperand RO>
+    : InstForm<(outs RO:$fd), (ins RO:$fj, RO:$fk),
+          !strconcat(opstr, "\t$fd, $fj, $fk"),
+          [], FrmR, opstr>;
+/// frecip
+class Float_Reg2_Frecip<string opstr, RegisterOperand RO,
+                 SDPatternOperator OpNode= null_frag>
+    : InstForm<(outs RO:$fd), (ins RO:$fj),
+          !strconcat(opstr, "\t$fd, $fj"),
+          [(set RO:$fd, (OpNode fpimm1, RO:$fj))],
+          FrmR, opstr>;
+/// frsqrt
+class Float_Reg2_Frsqrt<string opstr, RegisterOperand RO,
+                 SDPatternOperator OpNode= null_frag>
+    : InstForm<(outs RO:$fd), (ins RO:$fj),
+          !strconcat(opstr, "\t$fd, $fj"),
+          [(set RO:$fd, (OpNode fpimm1, (fsqrt RO:$fj)))],
+          FrmR, opstr>;
+
+class BceqzBr<string opstr, DAGOperand opnd,
+              SDPatternOperator Op = null_frag> :
+  InstForm<(outs), (ins FCFROpnd:$cj, opnd:$offset),
+         !strconcat(opstr, "\t$cj, $offset"),
+         [(LoongArchFPBrcond Op, FCFROpnd:$cj, bb:$offset)],
+         FrmFI, opstr>, HARDFLOAT {
+  let isBranch = 1;
+  let isTerminator = 1;
+  let hasFCCRegOperand = 1;
+}
+
+class FCMP_COND<string CondStr, string TypeStr, RegisterOperand RO,
+                SDPatternOperator OpNode = null_frag>
+    : InstForm<(outs FCFROpnd:$cd), (ins RO:$fj, RO:$fk),
+               !strconcat("fcmp.", CondStr, ".", TypeStr, "\t$cd, $fj, $fk"),
+               [(set FCFROpnd:$cd, (OpNode RO:$fj, RO:$fk))],
+               FrmOther,
+               !strconcat("fcmp.", CondStr, ".", TypeStr)> {
+    bit isCTI = 1; // for what? from Mips32r6InstrInfo.td line 219
+}
+
+class FIELD_CMP_COND<bits<5> Val> {
+  bits<5> Value = Val;
+}
+def FIELD_CMP_COND_CAF  : FIELD_CMP_COND<0x0>;
+def FIELD_CMP_COND_CUN  : FIELD_CMP_COND<0x8>;
+def FIELD_CMP_COND_CEQ  : FIELD_CMP_COND<0x4>;
+def FIELD_CMP_COND_CUEQ : FIELD_CMP_COND<0xC>;
+def FIELD_CMP_COND_CLT  : FIELD_CMP_COND<0x2>;
+def FIELD_CMP_COND_CULT : FIELD_CMP_COND<0xA>;
+def FIELD_CMP_COND_CLE  : FIELD_CMP_COND<0x6>;
+def FIELD_CMP_COND_CULE : FIELD_CMP_COND<0xE>;
+def FIELD_CMP_COND_CNE  : FIELD_CMP_COND<0x10>;
+def FIELD_CMP_COND_COR  : FIELD_CMP_COND<0x14>;
+def FIELD_CMP_COND_CUNE : FIELD_CMP_COND<0x18>;
+def FIELD_CMP_COND_SAF  : FIELD_CMP_COND<0x1>;
+def FIELD_CMP_COND_SUN  : FIELD_CMP_COND<0x9>;
+def FIELD_CMP_COND_SEQ  : FIELD_CMP_COND<0x5>;
+def FIELD_CMP_COND_SUEQ : FIELD_CMP_COND<0xD>;
+def FIELD_CMP_COND_SLT  : FIELD_CMP_COND<0x3>;
+def FIELD_CMP_COND_SULT : FIELD_CMP_COND<0xB>;
+def FIELD_CMP_COND_SLE  : FIELD_CMP_COND<0x7>;
+def FIELD_CMP_COND_SULE : FIELD_CMP_COND<0xF>;
+def FIELD_CMP_COND_SNE  : FIELD_CMP_COND<0x11>;
+def FIELD_CMP_COND_SOR  : FIELD_CMP_COND<0x15>;
+def FIELD_CMP_COND_SUNE : FIELD_CMP_COND<0x19>;
+
+multiclass FCMP_COND_M <bits<2> op, string TypeStr,
+                        RegisterOperand RO> {
+  def FCMP_CAF_#NAME  : FCMP_COND<"caf",  TypeStr, RO>,
+                        R2_COND<op, FIELD_CMP_COND_CAF.Value>;
+  def FCMP_CUN_#NAME  : FCMP_COND<"cun",  TypeStr, RO, setuo>,
+                        R2_COND<op, FIELD_CMP_COND_CUN.Value>;
+  def FCMP_CEQ_#NAME  : FCMP_COND<"ceq",  TypeStr, RO, setoeq>,
+                        R2_COND<op, FIELD_CMP_COND_CEQ.Value>;
+  def FCMP_CUEQ_#NAME : FCMP_COND<"cueq", TypeStr, RO, setueq>,
+                        R2_COND<op, FIELD_CMP_COND_CUEQ.Value>;
+  def FCMP_CLT_#NAME  : FCMP_COND<"clt",  TypeStr, RO, setolt>,
+                        R2_COND<op, FIELD_CMP_COND_CLT.Value>;
+  def FCMP_CULT_#NAME : FCMP_COND<"cult", TypeStr, RO, setult>,
+                        R2_COND<op, FIELD_CMP_COND_CULT.Value>;
+  def FCMP_CLE_#NAME  : FCMP_COND<"cle",  TypeStr, RO, setole>,
+                        R2_COND<op, FIELD_CMP_COND_CLE.Value>;
+  def FCMP_CULE_#NAME : FCMP_COND<"cule", TypeStr, RO, setule>,
+                        R2_COND<op, FIELD_CMP_COND_CULE.Value>;
+  def FCMP_CNE_#NAME  : FCMP_COND<"cne",  TypeStr, RO, setone>,
+                        R2_COND<op, FIELD_CMP_COND_CNE.Value>;
+  def FCMP_COR_#NAME  : FCMP_COND<"cor",  TypeStr, RO, seto>,
+                        R2_COND<op, FIELD_CMP_COND_COR.Value>;
+  def FCMP_CUNE_#NAME : FCMP_COND<"cune", TypeStr, RO, setune>,
+                        R2_COND<op, FIELD_CMP_COND_CUNE.Value>;
+
+  def FCMP_SAF_#NAME  : FCMP_COND<"saf",  TypeStr, RO>,
+                        R2_COND<op, FIELD_CMP_COND_SAF.Value>;
+  def FCMP_SUN_#NAME  : FCMP_COND<"sun",  TypeStr, RO>,
+                        R2_COND<op, FIELD_CMP_COND_SUN.Value>;
+  def FCMP_SEQ_#NAME  : FCMP_COND<"seq",  TypeStr, RO>,
+                        R2_COND<op, FIELD_CMP_COND_SEQ.Value>;
+  def FCMP_SUEQ_#NAME : FCMP_COND<"sueq", TypeStr, RO>,
+                        R2_COND<op, FIELD_CMP_COND_SUEQ.Value>;
+  def FCMP_SLT_#NAME  : FCMP_COND<"slt",  TypeStr, RO>,
+                        R2_COND<op, FIELD_CMP_COND_SLT.Value>;
+  def FCMP_SULT_#NAME : FCMP_COND<"sult", TypeStr, RO>,
+                        R2_COND<op, FIELD_CMP_COND_SULT.Value>;
+  def FCMP_SLE_#NAME  : FCMP_COND<"sle",  TypeStr, RO>,
+                        R2_COND<op, FIELD_CMP_COND_SLE.Value>;
+  def FCMP_SULE_#NAME : FCMP_COND<"sule", TypeStr, RO>,
+                        R2_COND<op, FIELD_CMP_COND_SULE.Value>;
+  def FCMP_SNE_#NAME  : FCMP_COND<"sne",  TypeStr, RO>,
+                        R2_COND<op, FIELD_CMP_COND_SNE.Value>;
+  def FCMP_SOR_#NAME  : FCMP_COND<"sor",  TypeStr, RO>,
+                        R2_COND<op, FIELD_CMP_COND_SOR.Value>;
+  def FCMP_SUNE_#NAME : FCMP_COND<"sune", TypeStr, RO>,
+                        R2_COND<op, FIELD_CMP_COND_SUNE.Value>;
+}
+
+//// comparisons supported via another comparison
+//multiclass FCmp_Pats<ValueType VT, Instruction NOROp, Register ZEROReg> {
+//  def : LoongArchPat<(seteq VT:$lhs, VT:$rhs),
+//                     (!cast<Instruction>("FCMP_CEQ_"#NAME) VT:$lhs, VT:$rhs)>;
+//  def : LoongArchPat<(setgt VT:$lhs, VT:$rhs),
+//                     (!cast<Instruction>("FCMP_CLE_"#NAME) VT:$rhs, VT:$lhs)>;
+//  def : LoongArchPat<(setge VT:$lhs, VT:$rhs),
+//                     (!cast<Instruction>("FCMP_CLT_"#NAME) VT:$rhs, VT:$lhs)>;
+//  def : LoongArchPat<(setlt VT:$lhs, VT:$rhs),
+//                     (!cast<Instruction>("FCMP_CLT_"#NAME) VT:$lhs, VT:$rhs)>;
+//  def : LoongArchPat<(setle VT:$lhs, VT:$rhs),
+//                     (!cast<Instruction>("FCMP_CLE_"#NAME) VT:$lhs, VT:$rhs)>;
+//  def : LoongArchPat<(setne VT:$lhs, VT:$rhs),
+//                     (NOROp
+//                      (!cast<Instruction>("FCMP_CEQ_"#NAME) VT:$lhs, VT:$rhs),
+//                      ZEROReg)>;
+//}
+
+
+///
+/// R2
+///
+def FABS_S   : Float_Reg2<"fabs.s", FGR32Opnd, fabs>, R2F<0b0100000001>;
+def FABS_D   : Float_Reg2<"fabs.d", FGR64Opnd, fabs>, R2F<0b0100000010>;
+def FNEG_S   : Float_Reg2<"fneg.s", FGR32Opnd, fneg>, R2F<0b0100000101>;
+def FNEG_D   : Float_Reg2<"fneg.d", FGR64Opnd, fneg>, R2F<0b0100000110>;
+def FLOGB_S  : Float_Reg2<"flogb.s", FGR32Opnd>, R2F<0b0100001001>;
+def FLOGB_D  : Float_Reg2<"flogb.d", FGR64Opnd>, R2F<0b0100001010>;
+def FCLASS_S : Float_Reg2<"fclass.s", FGR32Opnd>, R2F<0b0100001101>;
+def FCLASS_D : Float_Reg2<"fclass.d", FGR64Opnd>, R2F<0b0100001110>;
+def FSQRT_S  : Float_Reg2<"fsqrt.s", FGR32Opnd, fsqrt>, R2F<0b0100010001>;
+def FSQRT_D  : Float_Reg2<"fsqrt.d", FGR64Opnd, fsqrt>, R2F<0b0100010010>;
+def FRECIP_S : Float_Reg2_Frecip<"frecip.s", FGR32Opnd, fdiv>, R2F<0b0100010101>;
+def FRECIP_D : Float_Reg2_Frecip<"frecip.d", FGR64Opnd, fdiv>, R2F<0b0100010110>;
+def FRSQRT_S : Float_Reg2_Frsqrt<"frsqrt.s", FGR32Opnd, fdiv>, R2F<0b0100011001>;
+def FRSQRT_D : Float_Reg2_Frsqrt<"frsqrt.d", FGR64Opnd, fdiv>, R2F<0b0100011010>;
+def FMOV_S   : Float_Reg2<"fmov.s", FGR32Opnd>, R2F<0b0100100101>;
+def FMOV_D   : Float_Reg2<"fmov.d", FGR64Opnd>, R2F<0b0100100110>;
+
+def MOVGR2FR_W  : Float_MOVT<"movgr2fr.w", FGR32Opnd, GPR32Opnd, bitconvert>, MOVFI<0b0100101001>;
+def MOVGR2FR_D  : Float_MOVT<"movgr2fr.d", FGR64Opnd, GPR64Opnd, bitconvert>, MOVFI<0b0100101010>;
+def MOVGR2FRH_W : Float_MOVT<"movgr2frh.w", FGR64Opnd, GPR32Opnd>, MOVFI<0b0100101011>; //not realize
+def MOVFR2GR_S  : Float_MOVF<"movfr2gr.s", GPR32Opnd, FGR32Opnd, bitconvert>, MOVIF<0b0100101101>;
+def MOVFR2GR_D  : Float_MOVF<"movfr2gr.d", GPR64Opnd, FGR64Opnd, bitconvert>, MOVIF<0b0100101110>;
+def MOVFRH2GR_S : Float_MOVF<"movfrh2gr.s", GPR32Opnd, FGR32Opnd>, MOVIF<0b0100101111>; //not realize
+
+let isCodeGenOnly = 1 in {
+  def MOVGR2FR_DW : Float_MOVT<"movgr2fr.w", FGR64Opnd, GPR32Opnd>, MOVFI<0b0100101001>;
+  def MOVFR2GR_WD : Float_MOVF<"movfr2gr.s", GPR32Opnd, FGR64Opnd>, MOVIF<0b0100101101>;
+}
+
+def FCVT_S_D : Float_CVT<"fcvt.s.d", FGR32Opnd, FGR64Opnd>, R2F<0b1001000110>;
+def FCVT_D_S : Float_CVT<"fcvt.d.s", FGR64Opnd, FGR32Opnd>, R2F<0b1001001001>;
+
+def FTINTRM_W_S  : Float_Reg2<"ftintrm.w.s", FGR32Opnd>, R2F<0b1010000001>;
+def FTINTRM_W_D  : Float_Reg2<"ftintrm.w.d", FGR64Opnd>, R2F<0b1010000010>;
+def FTINTRM_L_S  : Float_Reg2<"ftintrm.l.s", FGR32Opnd>, R2F<0b1010001001>;
+def FTINTRM_L_D  : Float_Reg2<"ftintrm.l.d", FGR64Opnd>, R2F<0b1010001010>;
+def FTINTRP_W_S  : Float_Reg2<"ftintrp.w.s", FGR32Opnd>, R2F<0b1010010001>;
+def FTINTRP_W_D  : Float_Reg2<"ftintrp.w.d", FGR64Opnd>, R2F<0b1010010010>;
+def FTINTRP_L_S  : Float_Reg2<"ftintrp.l.s", FGR32Opnd>, R2F<0b1010011001>;
+def FTINTRP_L_D  : Float_Reg2<"ftintrp.l.d", FGR64Opnd>, R2F<0b1010011010>;
+def FTINTRZ_W_S  : Float_Reg2<"ftintrz.w.s", FGR32Opnd>, R2F<0b1010100001>;
+def FTINTRZ_L_D  : Float_Reg2<"ftintrz.l.d", FGR64Opnd>, R2F<0b1010101010>;
+def FTINTRNE_W_S : Float_Reg2<"ftintrne.w.s", FGR32Opnd>, R2F<0b1010110001>;
+def FTINTRNE_W_D : Float_Reg2<"ftintrne.w.d", FGR64Opnd>, R2F<0b1010110010>;
+def FTINTRNE_L_S : Float_Reg2<"ftintrne.l.s", FGR32Opnd>, R2F<0b1010111001>;
+def FTINTRNE_L_D : Float_Reg2<"ftintrne.l.d", FGR64Opnd>, R2F<0b1010111010>;
+
+def FTINT_W_S    : Float_CVT<"ftint.w.s", FGR32Opnd, FGR32Opnd>, R2F<0b1011000001>;
+def FTINT_W_D    : Float_CVT<"ftint.w.d", FGR32Opnd, FGR64Opnd>, R2F<0b1011000010>;
+def FTINT_L_S    : Float_CVT<"ftint.l.s", FGR64Opnd, FGR32Opnd>, R2F<0b1011001001>;
+def FTINT_L_D    : Float_CVT<"ftint.l.d", FGR64Opnd, FGR64Opnd>, R2F<0b1011001010>;
+def FFINT_S_W    : Float_CVT<"ffint.s.w", FGR32Opnd, FGR32Opnd>, R2F<0b1101000100>;
+def FFINT_S_L    : Float_CVT<"ffint.s.l", FGR32Opnd, FGR64Opnd>, R2F<0b1101000110>;
+def FFINT_D_W    : Float_CVT<"ffint.d.w", FGR64Opnd, FGR32Opnd>, R2F<0b1101001000>;
+def FFINT_D_L    : Float_CVT<"ffint.d.l", FGR64Opnd, FGR64Opnd>, R2F<0b1101001010>;
+
+def FRINT_S      : Float_Reg2<"frint.s", FGR32Opnd, frint>, R2F<0b1110010001>;
+def FRINT_D      : Float_Reg2<"frint.d", FGR64Opnd, frint>, R2F<0b1110010010>;
+
+///
+/// R3
+///
+def FADD_S      : Float_Reg3<"fadd.s", FGR32Opnd, fadd>, R3F<0b000001>;
+def FADD_D      : Float_Reg3<"fadd.d", FGR64Opnd, fadd>, R3F<0b000010>;
+def FSUB_S      : Float_Reg3<"fsub.s", FGR32Opnd, fsub>, R3F<0b000101>;
+def FSUB_D      : Float_Reg3<"fsub.d", FGR64Opnd, fsub>, R3F<0b000110>;
+def FMUL_S      : Float_Reg3<"fmul.s", FGR32Opnd, fmul>, R3F<0b001001>;
+def FMUL_D      : Float_Reg3<"fmul.d", FGR64Opnd, fmul>, R3F<0b001010>;
+def FDIV_S      : Float_Reg3<"fdiv.s", FGR32Opnd, fdiv>, R3F<0b001101>;
+def FDIV_D      : Float_Reg3<"fdiv.d", FGR64Opnd, fdiv>, R3F<0b001110>;
+def FMAX_S      : Float_Reg3<"fmax.s", FGR32Opnd, fmaxnum_ieee>, R3F<0b010001>;
+def FMAX_D      : Float_Reg3<"fmax.d", FGR64Opnd, fmaxnum_ieee>, R3F<0b010010>;
+def FMIN_S      : Float_Reg3<"fmin.s", FGR32Opnd, fminnum_ieee>, R3F<0b010101>;
+def FMIN_D      : Float_Reg3<"fmin.d", FGR64Opnd, fminnum_ieee>, R3F<0b010110>;
+def FMAXA_S     : Float_Reg3_Fmaxa<"fmaxa.s", FGR32Opnd>, R3F<0b011001>;
+def FMAXA_D     : Float_Reg3_Fmaxa<"fmaxa.d", FGR64Opnd>, R3F<0b011010>;
+def FMINA_S     : Float_Reg3_Fmaxa<"fmina.s", FGR32Opnd>, R3F<0b011101>;
+def FMINA_D     : Float_Reg3_Fmaxa<"fmina.d", FGR64Opnd>, R3F<0b011110>;
+def FSCALEB_S   : Float_Reg3<"fscaleb.s", FGR32Opnd>, R3F<0b100001>;
+def FSCALEB_D   : Float_Reg3<"fscaleb.d", FGR64Opnd>, R3F<0b100010>;
+def FCOPYSIGN_S : Float_Reg3<"fcopysign.s", FGR32Opnd, fcopysign>, R3F<0b100101>;
+def FCOPYSIGN_D : Float_Reg3<"fcopysign.d", FGR64Opnd, fcopysign>, R3F<0b100110>;
+///
+/// R4_IMM21
+///
+def FMADD_S  : Mul_Reg4<"fmadd.s", FGR32Opnd>, R4MUL<0b0001>;
+def FMADD_D  : Mul_Reg4<"fmadd.d", FGR64Opnd>, R4MUL<0b0010>;
+def FMSUB_S  : Mul_Reg4<"fmsub.s", FGR32Opnd>, R4MUL<0b0101>;
+def FMSUB_D  : Mul_Reg4<"fmsub.d", FGR64Opnd>, R4MUL<0b0110>;
+def FNMADD_S : NMul_Reg4<"fnmadd.s", FGR32Opnd>, R4MUL<0b1001>;
+def FNMADD_D : NMul_Reg4<"fnmadd.d", FGR64Opnd>, R4MUL<0b1010>;
+def FNMSUB_S : NMul_Reg4<"fnmsub.s", FGR32Opnd>, R4MUL<0b1101>;
+def FNMSUB_D : NMul_Reg4<"fnmsub.d", FGR64Opnd>, R4MUL<0b1110>;
+
+
+// fmadd: fj * fk + fa
+def : LoongArchPat<(fma FGR64Opnd:$fj, FGR64Opnd:$fk, FGR64Opnd:$fa),
+                   (FMADD_D $fj, $fk, $fa)>;
+
+def : LoongArchPat<(fma FGR32Opnd:$fj, FGR32Opnd:$fk, FGR32Opnd:$fa),
+                   (FMADD_S $fj, $fk, $fa)>;
+
+
+// fmsub: fj * fk - fa
+def : LoongArchPat<(fma FGR64Opnd:$fj, FGR64Opnd:$fk, (fneg FGR64Opnd:$fa)),
+                   (FMSUB_D FGR64Opnd:$fj, FGR64Opnd:$fk, FGR64Opnd:$fa)>;
+
+def : LoongArchPat<(fma FGR32Opnd:$fj, FGR32Opnd:$fk, (fneg FGR32Opnd:$fa)),
+                   (FMSUB_S FGR32Opnd:$fj, FGR32Opnd:$fk, FGR32Opnd:$fa)>;
+
+
+// fnmadd: -(fj * fk + fa)
+def : LoongArchPat<(fma (fneg FGR64Opnd:$fj), FGR64Opnd:$fk, (fneg FGR64Opnd:$fa)),
+                   (FNMADD_D FGR64Opnd:$fj, FGR64Opnd:$fk, FGR64Opnd:$fa)>;
+
+def : LoongArchPat<(fma (fneg FGR32Opnd:$fj), FGR32Opnd:$fk, (fneg FGR32Opnd:$fa)),
+                   (FNMADD_S FGR32Opnd:$fj, FGR32Opnd:$fk, FGR32Opnd:$fa)>;
+
+// fnmsub: -(fj * fk - fa)
+def : LoongArchPat<(fma (fneg FGR64Opnd:$fj), FGR64Opnd:$fk, FGR64Opnd:$fa),
+                   (FNMSUB_D FGR64Opnd:$fj, FGR64Opnd:$fk, FGR64Opnd:$fa)>;
+
+def : LoongArchPat<(fma (fneg FGR32Opnd:$fj), FGR32Opnd:$fk, FGR32Opnd:$fa),
+                   (FNMSUB_S FGR32Opnd:$fj, FGR32Opnd:$fk, FGR32Opnd:$fa)>;
+
+let Pattern = []<dag> in {
+defm S : FCMP_COND_M<0b01, "s", FGR32Opnd>;
+defm D : FCMP_COND_M<0b10, "d", FGR64Opnd>;
+}
+//
+//defm S : FCmp_Pats<f32, NOR32, ZERO>;
+//defm D : FCmp_Pats<f64, NOR32, ZERO>;
+
+///
+/// Float point branching
+///
+def LoongArch_BRANCH_F  : PatLeaf<(i32 0)>;
+def LoongArch_BRANCH_T  : PatLeaf<(i32 1)>;
+
+def BCEQZ : BceqzBr<"bceqz", brtarget, LoongArch_BRANCH_F>, R1_BCEQZ<0>;
+def BCNEZ : BceqzBr<"bcnez", brtarget, LoongArch_BRANCH_T>, R1_BCEQZ<1>;
+
+///
+/// FMOV
+///
+def MOVGR2FCSR : Gpr_2_Fcsr<"movgr2fcsr", GPR64Opnd>, MOVGPR2FCSR;
+def MOVFCSR2GR : Fcsr_2_Gpr<"movfcsr2gr", GPR64Opnd>, MOVFCSR2GPR;
+def MOVFR2CF   : Fgr_2_Fcfr<"movfr2cf", FGR64Opnd>,   MOVFGR2FCFR;
+def MOVCF2FR   : Fcfr_2_Fgr<"movcf2fr", FGR64Opnd>,   MOVFCFR2FGR;
+def MOVGR2CF   : Gpr_2_Fcfr<"movgr2cf", GPR64Opnd>,   MOVGPR2FCFR;
+def MOVCF2GR   : Fcfr_2_Gpr<"movcf2gr", GPR64Opnd>,   MOVFCFR2GPR;
+
+let isCodeGenOnly = 1 in {
+  def MOVFR2CF32   : Fgr_2_Fcfr<"movfr2cf", FGR32Opnd>,   MOVFGR2FCFR;
+  def MOVCF2FR32   : Fcfr_2_Fgr<"movcf2fr", FGR32Opnd>,   MOVFCFR2FGR;
+  def MOVGR2CF32   : Gpr_2_Fcfr<"movgr2cf", GPR32Opnd>,   MOVGPR2FCFR;
+  def MOVCF2GR32   : Fcfr_2_Gpr<"movcf2gr", GPR32Opnd>,   MOVFCFR2GPR;
+}
+
+class Sel_Reg4<string opstr, RegisterOperand RO,
+               SDPatternOperator OpNode= null_frag>
+    : InstForm<(outs RO:$fd), (ins FCFROpnd:$ca, RO:$fj, RO:$fk),
+          !strconcat(opstr, "\t$fd, $fj, $fk, $ca"),
+//          [(set RO:$fd, (select FCFROpnd:$ca, RO:$fk, RO:$fj))],
+          [],
+          FrmR, opstr>{
+       let Defs = [FCC0, FCC1, FCC2, FCC3, FCC4, FCC5, FCC6];
+       let hasFCCRegOperand = 1;
+ }
+
+def FSEL_T_S : Sel_Reg4<"fsel", FGR32Opnd>, R4SEL;
+let isCodeGenOnly = 1 in {
+  def FSEL_T_D : Sel_Reg4<"fsel", FGR64Opnd>, R4SEL;
+}
+
+///
+/// Mem access
+///
+def FLD_S : FLd<"fld.s", FGR32Opnd, mem, load>, LOAD_STORE<0b1100>;
+def FST_S : FSt<"fst.s", FGR32Opnd, mem, store>, LOAD_STORE<0b1101>;
+def FLD_D : FLd<"fld.d", FGR64Opnd, mem, load>, LOAD_STORE<0b1110>;
+def FST_D : FSt<"fst.d", FGR64Opnd, mem, store>, LOAD_STORE<0b1111>;
+
+def FLDX_S  : FLDX<"fldx.s", FGR32Opnd, load>, R3MF<0b01100000>;
+def FLDX_D  : FLDX<"fldx.d", FGR64Opnd, load>, R3MF<0b01101000>;
+def FSTX_S  : FSTX<"fstx.s", FGR32Opnd, store>, R3MF<0b01110000>;
+def FSTX_D  : FSTX<"fstx.d", FGR64Opnd, store>, R3MF<0b01111000>;
+
+def FLDGT_S : Float_Int_Reg3<"fldgt.s", FGR32Opnd, GPR64Opnd>, R3MF<0b11101000>;
+def FLDGT_D : Float_Int_Reg3<"fldgt.d", FGR64Opnd, GPR64Opnd>, R3MF<0b11101001>;
+def FLDLE_S : Float_Int_Reg3<"fldle.s", FGR32Opnd, GPR64Opnd>, R3MF<0b11101010>;
+def FLDLE_D : Float_Int_Reg3<"fldle.d", FGR64Opnd, GPR64Opnd>, R3MF<0b11101011>;
+def FSTGT_S : Float_Int_Reg3<"fstgt.s", FGR32Opnd, GPR64Opnd>, R3MF<0b11101100>;
+def FSTGT_D : Float_Int_Reg3<"fstgt.d", FGR64Opnd, GPR64Opnd>, R3MF<0b11101101>;
+def FSTLE_S : Float_Int_Reg3<"fstle.s", FGR32Opnd, GPR64Opnd>, R3MF<0b11101110>;
+def FSTLE_D : Float_Int_Reg3<"fstle.d", FGR64Opnd, GPR64Opnd>, R3MF<0b11101111>;
+
+let isPseudo = 1, isCodeGenOnly = 1 in {
+  def PseudoFFINT_S_W : Float_CVT<"", FGR32Opnd, GPR32Opnd>;
+  def PseudoFFINT_D_W : Float_CVT<"", FGR64Opnd, GPR32Opnd>;
+  def PseudoFFINT_S_L : Float_CVT<"", FGR64Opnd, GPR64Opnd>;
+  def PseudoFFINT_D_L : Float_CVT<"", FGR64Opnd, GPR64Opnd>;
+}
+
+def : LoongArchPat<(f32 (fpround FGR64Opnd:$src)),
+                   (FCVT_S_D FGR64Opnd:$src)>;
+def : LoongArchPat<(f64 (fpextend FGR32Opnd:$src)),
+                   (FCVT_D_S FGR32Opnd:$src)>;
+
+def : LoongArchPat<(f32 (sint_to_fp GPR32Opnd:$src)),
+                   (PseudoFFINT_S_W GPR32Opnd:$src)>;
+def : LoongArchPat<(f64 (sint_to_fp GPR32Opnd:$src)),
+                   (PseudoFFINT_D_W GPR32Opnd:$src)>;
+def : LoongArchPat<(f32 (sint_to_fp GPR64Opnd:$src)),
+                   (EXTRACT_SUBREG (PseudoFFINT_S_L GPR64Opnd:$src), sub_lo)>;
+def : LoongArchPat<(f64 (sint_to_fp GPR64Opnd:$src)),
+                   (PseudoFFINT_D_L GPR64Opnd:$src)>;
+
+def : LoongArchPat<(f32 fpimm0), (MOVGR2FR_W ZERO)>;
+def : LoongArchPat<(f32 fpimm0neg), (FNEG_S (MOVGR2FR_W ZERO))>;
+def : LoongArchPat<(f32 fpimm1), (FFINT_S_W (MOVGR2FR_W (ADDI_W ZERO, 1)))>;
+def : LoongArchPat<(f64 fpimm1), (FFINT_D_L (MOVGR2FR_D (ADDI_D ZERO_64, 1)))>;
+
+// Patterns for loads/stores with a reg+imm operand.
+let AddedComplexity = 40 in {
+  def : LoadRegImmPat<FLD_S, f32, load>;
+  def : StoreRegImmPat<FST_S, f32, store>;
+  def : LoadRegImmPat<FLD_D, f64, load>;
+  def : StoreRegImmPat<FST_D, f64, store>;
+}
+
+def : LoongArchPat<(LoongArchTruncIntFP FGR32Opnd:$src),
+                   (FTINTRZ_W_S FGR32Opnd:$src)>;
+
+def : LoongArchPat<(LoongArchTruncIntFP FGR64Opnd:$src),
+                   (FTINTRZ_L_D FGR64Opnd:$src)>;
+
+def : LoongArchPat<(LoongArchTruncIntFP FGR32Opnd:$src),
+                   (FCVT_D_S (FTINTRZ_W_S FGR32Opnd:$src))>;
+
+def : LoongArchPat<(f32 (fcopysign FGR32Opnd:$lhs, FGR64Opnd:$rhs)),
+                   (FCOPYSIGN_S FGR32Opnd:$lhs, (FCVT_S_D FGR64Opnd:$rhs))>;
+def : LoongArchPat<(f64 (fcopysign FGR64Opnd:$lhs, FGR32Opnd:$rhs)),
+                   (FCOPYSIGN_D FGR64Opnd:$lhs, (FCVT_D_S FGR32Opnd:$rhs))>;
+
+let PrintMethod = "printFCCOperand",EncoderMethod = "getFCMPEncoding" in
+  def condcode : Operand<i32>;
+
+class CEQS_FT<string typestr, RegisterClass RC,
+              SDPatternOperator OpNode = null_frag>  :
+  InstForm<(outs), (ins RC:$fj, RC:$fk, condcode:$cond),
+         !strconcat("fcmp.$cond.", typestr, "\t$$fcc0, $fj, $fk"),
+         [(OpNode RC:$fj, RC:$fk, imm:$cond)], FrmFR,
+         !strconcat("fcmp.$cond.", typestr)>, HARDFLOAT {
+  let Defs = [FCC0, FCC1, FCC2, FCC3, FCC4, FCC5, FCC6, FCC7];
+  let isCodeGenOnly = 1;
+  let hasFCCRegOperand = 1;
+}
+
+def FCMP_S32 : CEQS_FT<"s", FGR32, LoongArchFPCmp>, CEQS_FM<0b01> {
+      bits<3> cd = 0;
+}
+def FCMP_D64 : CEQS_FT<"d", FGR64, LoongArchFPCmp>, CEQS_FM<0b10>{
+      bits<3> cd = 0;
+}
+
+
+//multiclass FCmp_Pats2<ValueType VT, Instruction NOROp, Register ZEROReg> {
+//  def : LoongArchPat<(seteq VT:$lhs, VT:$rhs),
+//                     (!cast<Instruction>("SFCMP_CEQ_"#NAME) VT:$lhs, VT:$rhs)>;
+//  def : LoongArchPat<(setgt VT:$lhs, VT:$rhs),
+//                     (!cast<Instruction>("SFCMP_CLE_"#NAME) VT:$rhs, VT:$lhs)>;
+//  def : LoongArchPat<(setge VT:$lhs, VT:$rhs),
+//                     (!cast<Instruction>("SFCMP_CLT_"#NAME) VT:$rhs, VT:$lhs)>;
+//  def : LoongArchPat<(setlt VT:$lhs, VT:$rhs),
+//                     (!cast<Instruction>("SFCMP_CLT_"#NAME) VT:$lhs, VT:$rhs)>;
+//  def : LoongArchPat<(setle VT:$lhs, VT:$rhs),
+//                     (!cast<Instruction>("SFCMP_CLE_"#NAME) VT:$lhs, VT:$rhs)>;
+//  def : LoongArchPat<(setne VT:$lhs, VT:$rhs),
+//                     (NOROp
+//                      (!cast<Instruction>("SFCMP_CEQ_"#NAME) VT:$lhs, VT:$rhs),
+//                      ZEROReg)>;
+//
+//  def : LoongArchPat<(seteq VT:$lhs, VT:$rhs),
+//                     (!cast<Instruction>("DFCMP_CEQ_"#NAME) VT:$lhs, VT:$rhs)>;
+//  def : LoongArchPat<(setgt VT:$lhs, VT:$rhs),
+//                     (!cast<Instruction>("DFCMP_CLE_"#NAME) VT:$rhs, VT:$lhs)>;
+//  def : LoongArchPat<(setge VT:$lhs, VT:$rhs),
+//                     (!cast<Instruction>("DFCMP_CLT_"#NAME) VT:$rhs, VT:$lhs)>;
+//  def : LoongArchPat<(setlt VT:$lhs, VT:$rhs),
+//                     (!cast<Instruction>("DFCMP_CLT_"#NAME) VT:$lhs, VT:$rhs)>;
+//  def : LoongArchPat<(setle VT:$lhs, VT:$rhs),
+//                     (!cast<Instruction>("DFCMP_CLE_"#NAME) VT:$lhs, VT:$rhs)>;
+//  def : LoongArchPat<(setne VT:$lhs, VT:$rhs),
+//                     (NOROp
+//                      (!cast<Instruction>("DFCMP_CEQ_"#NAME) VT:$lhs, VT:$rhs),
+//                      ZEROReg)>;
+// }
+//
+//defm S : FCmp_Pats2<f32, NOR32, ZERO>;
+//defm D : FCmp_Pats2<f64, NOR32, ZERO>;
+
+let usesCustomInserter = 1 in {
+  class Select_Pseudo<RegisterOperand RC> :
+    LoongArchPseudo<(outs RC:$dst), (ins GPR32Opnd:$cond, RC:$T, RC:$F),
+            [(set RC:$dst, (select GPR32Opnd:$cond, RC:$T, RC:$F))]>;
+
+  class SelectFP_Pseudo_T<RegisterOperand RC> :
+    LoongArchPseudo<(outs RC:$dst), (ins FCFROpnd:$cond, RC:$T, RC:$F),
+             [(set RC:$dst, (LoongArchCMovFP_T RC:$T, FCFROpnd:$cond, RC:$F))]>;
+
+  class SelectFP_Pseudo_F<RegisterOperand RC> :
+    LoongArchPseudo<(outs RC:$dst), (ins FCFROpnd:$cond, RC:$T, RC:$F),
+             [(set RC:$dst, (LoongArchCMovFP_F RC:$T, FCFROpnd:$cond, RC:$F))]>;
+}
+
+def PseudoSELECT_I : Select_Pseudo<GPR32Opnd>;
+def PseudoSELECT_I64 : Select_Pseudo<GPR64Opnd>;
+def PseudoSELECT_S : Select_Pseudo<FGR32Opnd>;
+def PseudoSELECT_D64 : Select_Pseudo<FGR64Opnd>;
+
+def PseudoSELECTFP_T_I : SelectFP_Pseudo_T<GPR32Opnd>;
+def PseudoSELECTFP_T_I64 : SelectFP_Pseudo_T<GPR64Opnd>;
+def PseudoSELECTFP_T_S : SelectFP_Pseudo_T<FGR32Opnd>;
+def PseudoSELECTFP_T_D64 : SelectFP_Pseudo_T<FGR64Opnd>;
+
+def PseudoSELECTFP_F_I : SelectFP_Pseudo_F<GPR32Opnd>;
+def PseudoSELECTFP_F_I64 : SelectFP_Pseudo_F<GPR64Opnd>;
+def PseudoSELECTFP_F_S : SelectFP_Pseudo_F<FGR32Opnd>;
+def PseudoSELECTFP_F_D64 : SelectFP_Pseudo_F<FGR64Opnd>;
+
+class ABSS_FT<string opstr, RegisterOperand DstRC, RegisterOperand SrcRC,
+              SDPatternOperator OpNode= null_frag> :
+  InstForm<(outs DstRC:$fd), (ins SrcRC:$fj), !strconcat(opstr, "\t$fd, $fj"),
+         [(set DstRC:$fd, (OpNode SrcRC:$fj))], FrmFR, opstr>;
+
+def TRUNC_W_D : ABSS_FT<"ftintrz.w.d", FGR32Opnd, FGR64Opnd>, R2F<0b1010100010>;
+
+def FTINTRZ_L_S  : ABSS_FT<"ftintrz.l.s", FGR64Opnd, FGR32Opnd>, R2F<0b1010101001>;
+
+def : LoongArchPat<(LoongArchTruncIntFP FGR64Opnd:$src),
+              (TRUNC_W_D FGR64Opnd:$src)>;
+
+def : LoongArchPat<(LoongArchTruncIntFP FGR32Opnd:$src),
+              (FTINTRZ_L_S FGR32Opnd:$src)>;
+
+def : Pat<(fcanonicalize FGR32Opnd:$src), (FMAX_S $src, $src)>;
+def : Pat<(fcanonicalize FGR64Opnd:$src), (FMAX_D $src, $src)>;
diff --git a/llvm/lib/Target/LoongArch/LoongArchMCInstLower.cpp b/llvm/lib/Target/LoongArch/LoongArchMCInstLower.cpp
new file mode 100644
index 000000000000..bf70b09d42c7
--- /dev/null
+++ b/llvm/lib/Target/LoongArch/LoongArchMCInstLower.cpp
@@ -0,0 +1,342 @@
+//===- LoongArchMCInstLower.cpp - Convert LoongArch MachineInstr to MCInst ----------===//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+//
+// This file contains code to lower LoongArch MachineInstrs to their corresponding
+// MCInst records.
+//
+//===----------------------------------------------------------------------===//
+
+#include "LoongArchMCInstLower.h"
+#include "MCTargetDesc/LoongArchBaseInfo.h"
+#include "MCTargetDesc/LoongArchMCExpr.h"
+#include "LoongArchAsmPrinter.h"
+#include "llvm/CodeGen/MachineBasicBlock.h"
+#include "llvm/CodeGen/MachineInstr.h"
+#include "llvm/CodeGen/MachineOperand.h"
+#include "llvm/MC/MCExpr.h"
+#include "llvm/MC/MCInst.h"
+#include "llvm/Support/ErrorHandling.h"
+#include <cassert>
+
+using namespace llvm;
+
+LoongArchMCInstLower::LoongArchMCInstLower(LoongArchAsmPrinter &asmprinter)
+  : AsmPrinter(asmprinter) {}
+
+void LoongArchMCInstLower::Initialize(MCContext *C) {
+  Ctx = C;
+}
+
+MCOperand LoongArchMCInstLower::LowerSymbolOperand(const MachineOperand &MO,
+                                              MachineOperandType MOTy,
+                                              unsigned Offset) const {
+  MCSymbolRefExpr::VariantKind Kind = MCSymbolRefExpr::VK_None;
+  LoongArchMCExpr::LoongArchExprKind TargetKind = LoongArchMCExpr::MEK_None;
+  const MCSymbol *Symbol;
+
+  switch(MO.getTargetFlags()) {
+  default:
+    llvm_unreachable("Invalid target flag!");
+  case LoongArchII::MO_NO_FLAG:
+    break;
+  case LoongArchII::MO_GOT_HI:
+    TargetKind = LoongArchMCExpr::MEK_GOT_HI;
+    break;
+  case LoongArchII::MO_GOT_LO:
+    TargetKind = LoongArchMCExpr::MEK_GOT_LO;
+    break;
+  case LoongArchII::MO_GOT_RRHI:
+    TargetKind = LoongArchMCExpr::MEK_GOT_RRHI;
+    break;
+  case LoongArchII::MO_GOT_RRHIGHER:
+    TargetKind = LoongArchMCExpr::MEK_GOT_RRHIGHER;
+    break;
+  case LoongArchII::MO_GOT_RRHIGHEST:
+    TargetKind = LoongArchMCExpr::MEK_GOT_RRHIGHEST;
+    break;
+  case LoongArchII::MO_GOT_RRLO:
+    TargetKind = LoongArchMCExpr::MEK_GOT_RRLO;
+    break;
+  case LoongArchII::MO_PCREL_HI:
+    TargetKind = LoongArchMCExpr::MEK_PCREL_HI;
+    break;
+  case LoongArchII::MO_PCREL_LO:
+    TargetKind = LoongArchMCExpr::MEK_PCREL_LO;
+    break;
+  case LoongArchII::MO_PCREL_RRHI:
+    TargetKind = LoongArchMCExpr::MEK_PCREL_RRHI;
+    break;
+  case LoongArchII::MO_PCREL_RRHIGHER:
+    TargetKind = LoongArchMCExpr::MEK_PCREL_RRHIGHER;
+    break;
+  case LoongArchII::MO_PCREL_RRHIGHEST:
+    TargetKind = LoongArchMCExpr::MEK_PCREL_RRHIGHEST;
+    break;
+  case LoongArchII::MO_PCREL_RRLO:
+    TargetKind = LoongArchMCExpr::MEK_PCREL_RRLO;
+    break;
+  case LoongArchII::MO_TLSIE_HI:
+    TargetKind = LoongArchMCExpr::MEK_TLSIE_HI;
+    break;
+  case LoongArchII::MO_TLSIE_LO:
+    TargetKind = LoongArchMCExpr::MEK_TLSIE_LO;
+    break;
+  case LoongArchII::MO_TLSIE_RRHI:
+    TargetKind = LoongArchMCExpr::MEK_TLSIE_RRHI;
+    break;
+  case LoongArchII::MO_TLSIE_RRHIGHER:
+    TargetKind = LoongArchMCExpr::MEK_TLSIE_RRHIGHER;
+    break;
+  case LoongArchII::MO_TLSIE_RRHIGHEST:
+    TargetKind = LoongArchMCExpr::MEK_TLSIE_RRHIGHEST;
+    break;
+  case LoongArchII::MO_TLSIE_RRLO:
+    TargetKind = LoongArchMCExpr::MEK_TLSIE_RRLO;
+    break;
+  case LoongArchII::MO_TLSLE_HI:
+    TargetKind = LoongArchMCExpr::MEK_TLSLE_HI;
+    break;
+  case LoongArchII::MO_TLSLE_HIGHER:
+    TargetKind = LoongArchMCExpr::MEK_TLSLE_HIGHER;
+    break;
+  case LoongArchII::MO_TLSLE_HIGHEST:
+    TargetKind = LoongArchMCExpr::MEK_TLSLE_HIGHEST;
+    break;
+  case LoongArchII::MO_TLSLE_LO:
+    TargetKind = LoongArchMCExpr::MEK_TLSLE_LO;
+    break;
+  case LoongArchII::MO_TLSGD_HI:
+    TargetKind = LoongArchMCExpr::MEK_TLSGD_HI;
+    break;
+  case LoongArchII::MO_TLSGD_LO:
+    TargetKind = LoongArchMCExpr::MEK_TLSGD_LO;
+    break;
+  case LoongArchII::MO_TLSGD_RRHI:
+    TargetKind = LoongArchMCExpr::MEK_TLSGD_RRHI;
+    break;
+  case LoongArchII::MO_TLSGD_RRHIGHER:
+    TargetKind = LoongArchMCExpr::MEK_TLSGD_RRHIGHER;
+    break;
+  case LoongArchII::MO_TLSGD_RRHIGHEST:
+    TargetKind = LoongArchMCExpr::MEK_TLSGD_RRHIGHEST;
+    break;
+  case LoongArchII::MO_TLSGD_RRLO:
+    TargetKind = LoongArchMCExpr::MEK_TLSGD_RRLO;
+    break;
+  case LoongArchII::MO_ABS_HI:
+    TargetKind = LoongArchMCExpr::MEK_ABS_HI;
+    break;
+  case LoongArchII::MO_ABS_HIGHER:
+    TargetKind = LoongArchMCExpr::MEK_ABS_HIGHER;
+    break;
+  case LoongArchII::MO_ABS_HIGHEST:
+    TargetKind = LoongArchMCExpr::MEK_ABS_HIGHEST;
+    break;
+  case LoongArchII::MO_ABS_LO:
+    TargetKind = LoongArchMCExpr::MEK_ABS_LO;
+    break;
+  case LoongArchII::MO_CALL_HI:
+    TargetKind = LoongArchMCExpr::MEK_CALL_HI;
+    break;
+  case LoongArchII::MO_CALL_LO:
+    TargetKind = LoongArchMCExpr::MEK_CALL_LO;
+    break;
+  }
+
+  switch (MOTy) {
+  case MachineOperand::MO_MachineBasicBlock:
+    Symbol = MO.getMBB()->getSymbol();
+    break;
+
+  case MachineOperand::MO_GlobalAddress:
+    Symbol = AsmPrinter.getSymbol(MO.getGlobal());
+    Offset += MO.getOffset();
+    break;
+
+  case MachineOperand::MO_BlockAddress:
+    Symbol = AsmPrinter.GetBlockAddressSymbol(MO.getBlockAddress());
+    Offset += MO.getOffset();
+    break;
+
+  case MachineOperand::MO_ExternalSymbol:
+    Symbol = AsmPrinter.GetExternalSymbolSymbol(MO.getSymbolName());
+    Offset += MO.getOffset();
+    break;
+
+  case MachineOperand::MO_MCSymbol:
+    Symbol = MO.getMCSymbol();
+    Offset += MO.getOffset();
+    break;
+
+  case MachineOperand::MO_JumpTableIndex:
+    Symbol = AsmPrinter.GetJTISymbol(MO.getIndex());
+    break;
+
+  case MachineOperand::MO_ConstantPoolIndex:
+    Symbol = AsmPrinter.GetCPISymbol(MO.getIndex());
+    Offset += MO.getOffset();
+    break;
+
+  default:
+    llvm_unreachable("<unknown operand type>");
+  }
+
+  const MCExpr *Expr = MCSymbolRefExpr::create(Symbol, Kind, *Ctx);
+
+  if (Offset) {
+    // Assume offset is never negative.
+    assert(Offset > 0);
+
+    Expr = MCBinaryExpr::createAdd(Expr, MCConstantExpr::create(Offset, *Ctx),
+                                   *Ctx);
+  }
+
+  if (TargetKind != LoongArchMCExpr::MEK_None)
+    Expr = LoongArchMCExpr::create(TargetKind, Expr, *Ctx);
+
+  return MCOperand::createExpr(Expr);
+}
+
+MCOperand LoongArchMCInstLower::LowerOperand(const MachineOperand &MO,
+                                        unsigned offset) const {
+  MachineOperandType MOTy = MO.getType();
+
+  switch (MOTy) {
+  default: llvm_unreachable("unknown operand type");
+  case MachineOperand::MO_Register:
+    // Ignore all implicit register operands.
+    if (MO.isImplicit()) break;
+    return MCOperand::createReg(MO.getReg());
+  case MachineOperand::MO_Immediate:
+    return MCOperand::createImm(MO.getImm() + offset);
+  case MachineOperand::MO_MachineBasicBlock:
+  case MachineOperand::MO_GlobalAddress:
+  case MachineOperand::MO_ExternalSymbol:
+  case MachineOperand::MO_MCSymbol:
+  case MachineOperand::MO_JumpTableIndex:
+  case MachineOperand::MO_ConstantPoolIndex:
+  case MachineOperand::MO_BlockAddress:
+    return LowerSymbolOperand(MO, MOTy, offset);
+  case MachineOperand::MO_RegisterMask:
+    break;
+ }
+
+  return MCOperand();
+}
+
+MCOperand LoongArchMCInstLower::createSub(MachineBasicBlock *BB1,
+                                     MachineBasicBlock *BB2,
+                                     LoongArchMCExpr::LoongArchExprKind Kind) const {
+  const MCSymbolRefExpr *Sym1 = MCSymbolRefExpr::create(BB1->getSymbol(), *Ctx);
+  const MCSymbolRefExpr *Sym2 = MCSymbolRefExpr::create(BB2->getSymbol(), *Ctx);
+  const MCBinaryExpr *Sub = MCBinaryExpr::createSub(Sym1, Sym2, *Ctx);
+
+  return MCOperand::createExpr(LoongArchMCExpr::create(Kind, Sub, *Ctx));
+}
+
+void LoongArchMCInstLower::lowerLongBranchADDI(const MachineInstr *MI,
+                                           MCInst &OutMI, int Opcode) const {
+  OutMI.setOpcode(Opcode);
+
+  LoongArchMCExpr::LoongArchExprKind Kind;
+  unsigned TargetFlags = MI->getOperand(2).getTargetFlags();
+  switch (TargetFlags) {
+  case LoongArchII::MO_ABS_HIGHEST:
+    Kind = LoongArchMCExpr::MEK_ABS_HIGHEST;
+    break;
+  case LoongArchII::MO_ABS_HIGHER:
+    Kind = LoongArchMCExpr::MEK_ABS_HIGHER;
+    break;
+  case LoongArchII::MO_ABS_HI:
+    Kind = LoongArchMCExpr::MEK_ABS_HI;
+    break;
+  case LoongArchII::MO_ABS_LO:
+    Kind = LoongArchMCExpr::MEK_ABS_LO;
+    break;
+  default:
+    report_fatal_error("Unexpected flags for lowerLongBranchADDI");
+  }
+
+  // Lower two register operands.
+  for (unsigned I = 0, E = 2; I != E; ++I) {
+    const MachineOperand &MO = MI->getOperand(I);
+    OutMI.addOperand(LowerOperand(MO));
+  }
+
+  if (MI->getNumOperands() == 3) {
+    // Lower register operand.
+    const MCExpr *Expr =
+        MCSymbolRefExpr::create(MI->getOperand(2).getMBB()->getSymbol(), *Ctx);
+    const LoongArchMCExpr *LoongArchExpr = LoongArchMCExpr::create(Kind, Expr, *Ctx);
+    OutMI.addOperand(MCOperand::createExpr(LoongArchExpr));
+  } else if (MI->getNumOperands() == 4) {
+    // Create %lo($tgt-$baltgt) or %hi($tgt-$baltgt).
+    OutMI.addOperand(createSub(MI->getOperand(2).getMBB(),
+                               MI->getOperand(3).getMBB(), Kind));
+  }
+}
+
+void LoongArchMCInstLower::lowerLongBranchPCADDU12I(const MachineInstr *MI,
+                                           MCInst &OutMI, int Opcode) const {
+  OutMI.setOpcode(Opcode);
+
+  LoongArchMCExpr::LoongArchExprKind Kind;
+  unsigned TargetFlags = MI->getOperand(1).getTargetFlags();
+  switch (TargetFlags) {
+  case LoongArchII::MO_PCREL_HI:
+    Kind = LoongArchMCExpr::MEK_PCREL_HI;
+    break;
+  case LoongArchII::MO_PCREL_LO:
+    Kind = LoongArchMCExpr::MEK_PCREL_LO;
+    break;
+  default:
+    report_fatal_error("Unexpected flags for lowerLongBranchADDI");
+  }
+
+  // Lower one register operands.
+  const MachineOperand &MO = MI->getOperand(0);
+  OutMI.addOperand(LowerOperand(MO));
+
+  const MCExpr *Expr =
+    MCSymbolRefExpr::create(MI->getOperand(1).getMBB()->getSymbol(), *Ctx);
+  const LoongArchMCExpr *LoongArchExpr = LoongArchMCExpr::create(Kind, Expr, *Ctx);
+  OutMI.addOperand(MCOperand::createExpr(LoongArchExpr));
+}
+bool LoongArchMCInstLower::lowerLongBranch(const MachineInstr *MI,
+                                      MCInst &OutMI) const {
+  switch (MI->getOpcode()) {
+  default:
+    return false;
+  case LoongArch::LONG_BRANCH_ADDIW:
+  case LoongArch::LONG_BRANCH_ADDIW2Op:
+    lowerLongBranchADDI(MI, OutMI, LoongArch::ADDI_W);
+    return true;
+  case LoongArch::LONG_BRANCH_ADDID:
+  case LoongArch::LONG_BRANCH_ADDID2Op:
+    lowerLongBranchADDI(MI, OutMI, LoongArch::ADDI_D);
+    return true;
+  case LoongArch::LONG_BRANCH_PCADDU12I:
+    lowerLongBranchPCADDU12I(MI, OutMI, LoongArch::PCADDU12I);
+    return true;
+  }
+}
+
+void LoongArchMCInstLower::Lower(const MachineInstr *MI, MCInst &OutMI) const {
+  if (lowerLongBranch(MI, OutMI))
+    return;
+
+  OutMI.setOpcode(MI->getOpcode());
+
+  for (unsigned i = 0, e = MI->getNumOperands(); i != e; ++i) {
+    const MachineOperand &MO = MI->getOperand(i);
+    MCOperand MCOp = LowerOperand(MO);
+
+    if (MCOp.isValid())
+      OutMI.addOperand(MCOp);
+  }
+}
diff --git a/llvm/lib/Target/LoongArch/LoongArchMCInstLower.h b/llvm/lib/Target/LoongArch/LoongArchMCInstLower.h
new file mode 100644
index 000000000000..6463a7b64fb0
--- /dev/null
+++ b/llvm/lib/Target/LoongArch/LoongArchMCInstLower.h
@@ -0,0 +1,55 @@
+//===- LoongArchMCInstLower.h - Lower MachineInstr to MCInst --------*- C++ -*--===//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef LLVM_LIB_TARGET_LOONGARCH_LOONGARCHMCINSTLOWER_H
+#define LLVM_LIB_TARGET_LOONGARCH_LOONGARCHMCINSTLOWER_H
+
+#include "MCTargetDesc/LoongArchMCExpr.h"
+#include "llvm/CodeGen/MachineOperand.h"
+#include "llvm/Support/Compiler.h"
+
+namespace llvm {
+
+class MachineBasicBlock;
+class MachineInstr;
+class MCContext;
+class MCInst;
+class MCOperand;
+class LoongArchAsmPrinter;
+
+/// LoongArchMCInstLower - This class is used to lower an MachineInstr into an
+///                   MCInst.
+class LLVM_LIBRARY_VISIBILITY LoongArchMCInstLower {
+  using MachineOperandType = MachineOperand::MachineOperandType;
+
+  MCContext *Ctx;
+  LoongArchAsmPrinter &AsmPrinter;
+
+public:
+  LoongArchMCInstLower(LoongArchAsmPrinter &asmprinter);
+
+  void Initialize(MCContext *C);
+  void Lower(const MachineInstr *MI, MCInst &OutMI) const;
+  MCOperand LowerOperand(const MachineOperand& MO, unsigned offset = 0) const;
+
+private:
+  MCOperand LowerSymbolOperand(const MachineOperand &MO,
+                               MachineOperandType MOTy, unsigned Offset) const;
+  MCOperand createSub(MachineBasicBlock *BB1, MachineBasicBlock *BB2,
+                      LoongArchMCExpr::LoongArchExprKind Kind) const;
+  void lowerLongBranchLUi(const MachineInstr *MI, MCInst &OutMI) const;
+  void lowerLongBranchADDI(const MachineInstr *MI, MCInst &OutMI,
+                           int Opcode) const;
+  void lowerLongBranchPCADDU12I(const MachineInstr *MI, MCInst &OutMI,
+                            int Opcode) const;
+  bool lowerLongBranch(const MachineInstr *MI, MCInst &OutMI) const;
+};
+
+} // end namespace llvm
+
+#endif // LLVM_LIB_TARGET_LOONGARCH_LOONGARCHMCINSTLOWER_H
diff --git a/llvm/lib/Target/LoongArch/LoongArchMachineFunction.cpp b/llvm/lib/Target/LoongArch/LoongArchMachineFunction.cpp
new file mode 100644
index 000000000000..501766609e85
--- /dev/null
+++ b/llvm/lib/Target/LoongArch/LoongArchMachineFunction.cpp
@@ -0,0 +1,58 @@
+//===-- LoongArchMachineFunctionInfo.cpp - Private data used for LoongArch ----------===//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+
+#include "LoongArchMachineFunction.h"
+#include "MCTargetDesc/LoongArchABIInfo.h"
+#include "LoongArchSubtarget.h"
+#include "LoongArchTargetMachine.h"
+#include "llvm/CodeGen/MachineFrameInfo.h"
+#include "llvm/CodeGen/MachineRegisterInfo.h"
+#include "llvm/CodeGen/PseudoSourceValue.h"
+#include "llvm/CodeGen/TargetRegisterInfo.h"
+#include "llvm/Support/CommandLine.h"
+
+using namespace llvm;
+
+LoongArchFunctionInfo::~LoongArchFunctionInfo() = default;
+
+void LoongArchFunctionInfo::createEhDataRegsFI() {
+  const TargetRegisterInfo &TRI = *MF.getSubtarget().getRegisterInfo();
+  for (int I = 0; I < 4; ++I) {
+    const TargetRegisterClass &RC =
+        static_cast<const LoongArchTargetMachine &>(MF.getTarget()).getABI().IsLP64D()
+            ? LoongArch::GPR64RegClass
+            : LoongArch::GPR32RegClass;
+
+    EhDataRegFI[I] = MF.getFrameInfo().CreateStackObject(TRI.getSpillSize(RC),
+        TRI.getSpillAlign(RC), false);
+  }
+}
+
+bool LoongArchFunctionInfo::isEhDataRegFI(int FI) const {
+  return CallsEhReturn && (FI == EhDataRegFI[0] || FI == EhDataRegFI[1]
+                        || FI == EhDataRegFI[2] || FI == EhDataRegFI[3]);
+}
+
+MachinePointerInfo LoongArchFunctionInfo::callPtrInfo(const char *ES) {
+  return MachinePointerInfo(MF.getPSVManager().getExternalSymbolCallEntry(ES));
+}
+
+MachinePointerInfo LoongArchFunctionInfo::callPtrInfo(const GlobalValue *GV) {
+  return MachinePointerInfo(MF.getPSVManager().getGlobalValueCallEntry(GV));
+}
+
+int LoongArchFunctionInfo::getMoveF64ViaSpillFI(const TargetRegisterClass *RC) {
+  const TargetRegisterInfo &TRI = *MF.getSubtarget().getRegisterInfo();
+  if (MoveF64ViaSpillFI == -1) {
+    MoveF64ViaSpillFI = MF.getFrameInfo().CreateStackObject(
+        TRI.getSpillSize(*RC), TRI.getSpillAlign(*RC), false);
+  }
+  return MoveF64ViaSpillFI;
+}
+
+void LoongArchFunctionInfo::anchor() {}
diff --git a/llvm/lib/Target/LoongArch/LoongArchMachineFunction.h b/llvm/lib/Target/LoongArch/LoongArchMachineFunction.h
new file mode 100644
index 000000000000..46847c45947e
--- /dev/null
+++ b/llvm/lib/Target/LoongArch/LoongArchMachineFunction.h
@@ -0,0 +1,99 @@
+//===- LoongArchMachineFunctionInfo.h - Private data used for LoongArch ---*- C++ -*-===//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+//
+// This file declares the LoongArch specific subclass of MachineFunctionInfo.
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef LLVM_LIB_TARGET_LOONGARCH_LOONGARCHMACHINEFUNCTION_H
+#define LLVM_LIB_TARGET_LOONGARCH_LOONGARCHMACHINEFUNCTION_H
+
+#include "llvm/CodeGen/MachineFunction.h"
+#include "llvm/CodeGen/MachineMemOperand.h"
+#include <map>
+
+namespace llvm {
+
+/// LoongArchFunctionInfo - This class is derived from MachineFunction private
+/// LoongArch target-specific information for each MachineFunction.
+class LoongArchFunctionInfo : public MachineFunctionInfo {
+public:
+  LoongArchFunctionInfo(MachineFunction &MF) : MF(MF) {}
+
+  ~LoongArchFunctionInfo() override;
+
+  unsigned getSRetReturnReg() const { return SRetReturnReg; }
+  void setSRetReturnReg(unsigned Reg) { SRetReturnReg = Reg; }
+
+  int getVarArgsFrameIndex() const { return VarArgsFrameIndex; }
+  void setVarArgsFrameIndex(int Index) { VarArgsFrameIndex = Index; }
+
+  bool hasByvalArg() const { return HasByvalArg; }
+  void setFormalArgInfo(unsigned Size, bool HasByval) {
+    IncomingArgSize = Size;
+    HasByvalArg = HasByval;
+  }
+
+  unsigned getIncomingArgSize() const { return IncomingArgSize; }
+
+  bool callsEhReturn() const { return CallsEhReturn; }
+  void setCallsEhReturn() { CallsEhReturn = true; }
+
+  void createEhDataRegsFI();
+  int getEhDataRegFI(unsigned Reg) const { return EhDataRegFI[Reg]; }
+  bool isEhDataRegFI(int FI) const;
+
+  /// Create a MachinePointerInfo that has an ExternalSymbolPseudoSourceValue
+  /// object representing a GOT entry for an external function.
+  MachinePointerInfo callPtrInfo(const char *ES);
+
+  /// Create a MachinePointerInfo that has a GlobalValuePseudoSourceValue object
+  /// representing a GOT entry for a global function.
+  MachinePointerInfo callPtrInfo(const GlobalValue *GV);
+
+  void setSaveS2() { SaveS2 = true; }
+  bool hasSaveS2() const { return SaveS2; }
+
+  int getMoveF64ViaSpillFI(const TargetRegisterClass *RC);
+
+private:
+  virtual void anchor();
+
+  MachineFunction& MF;
+
+  /// SRetReturnReg - Some subtargets require that sret lowering includes
+  /// returning the value of the returned struct in a register. This field
+  /// holds the virtual register into which the sret argument is passed.
+  unsigned SRetReturnReg = 0;
+
+  /// VarArgsFrameIndex - FrameIndex for start of varargs area.
+  int VarArgsFrameIndex = 0;
+
+  /// True if function has a byval argument.
+  bool HasByvalArg;
+
+  /// Size of incoming argument area.
+  unsigned IncomingArgSize;
+
+  /// CallsEhReturn - Whether the function calls llvm.eh.return.
+  bool CallsEhReturn = false;
+
+  /// Frame objects for spilling eh data registers.
+  int EhDataRegFI[4];
+
+  // saveS2
+  bool SaveS2 = false;
+
+  /// FrameIndex for expanding BuildPairF64 nodes to spill and reload when the
+  /// LP32 FPXX ABI is enabled. -1 is used to denote invalid index.
+  int MoveF64ViaSpillFI = -1;
+};
+
+} // end namespace llvm
+
+#endif // LLVM_LIB_TARGET_LOONGARCH_LOONGARCHMACHINEFUNCTION_H
diff --git a/llvm/lib/Target/LoongArch/LoongArchModuleISelDAGToDAG.cpp b/llvm/lib/Target/LoongArch/LoongArchModuleISelDAGToDAG.cpp
new file mode 100644
index 000000000000..ef039aaad370
--- /dev/null
+++ b/llvm/lib/Target/LoongArch/LoongArchModuleISelDAGToDAG.cpp
@@ -0,0 +1,55 @@
+//===----------------------------------------------------------------------===//
+// Instruction Selector Subtarget Control
+//===----------------------------------------------------------------------===//
+
+//===----------------------------------------------------------------------===//
+// This file defines a pass used to change the subtarget for the
+// LoongArch Instruction selector.
+//
+//===----------------------------------------------------------------------===//
+
+#include "LoongArch.h"
+#include "LoongArchTargetMachine.h"
+#include "llvm/CodeGen/TargetPassConfig.h"
+#include "llvm/CodeGen/StackProtector.h"
+#include "llvm/Support/Debug.h"
+#include "llvm/Support/raw_ostream.h"
+
+using namespace llvm;
+
+#define DEBUG_TYPE "loongarch-isel"
+
+namespace {
+  class LoongArchModuleDAGToDAGISel : public MachineFunctionPass {
+  public:
+    static char ID;
+
+    LoongArchModuleDAGToDAGISel() : MachineFunctionPass(ID) {}
+
+    // Pass Name
+    StringRef getPassName() const override {
+      return "LoongArch DAG->DAG Pattern Instruction Selection";
+    }
+
+    void getAnalysisUsage(AnalysisUsage &AU) const override {
+      AU.addRequired<TargetPassConfig>();
+      AU.addPreserved<StackProtector>();
+      MachineFunctionPass::getAnalysisUsage(AU);
+    }
+
+    bool runOnMachineFunction(MachineFunction &MF) override;
+  };
+
+  char LoongArchModuleDAGToDAGISel::ID = 0;
+}
+
+bool LoongArchModuleDAGToDAGISel::runOnMachineFunction(MachineFunction &MF) {
+  LLVM_DEBUG(errs() << "In LoongArchModuleDAGToDAGISel::runMachineFunction\n");
+  auto &TPC = getAnalysis<TargetPassConfig>();
+  auto &TM = TPC.getTM<LoongArchTargetMachine>();
+  return false;
+}
+
+llvm::FunctionPass *llvm::createLoongArchModuleISelDagPass() {
+  return new LoongArchModuleDAGToDAGISel();
+}
diff --git a/llvm/lib/Target/LoongArch/LoongArchRegisterInfo.cpp b/llvm/lib/Target/LoongArch/LoongArchRegisterInfo.cpp
new file mode 100644
index 000000000000..e08236335f66
--- /dev/null
+++ b/llvm/lib/Target/LoongArch/LoongArchRegisterInfo.cpp
@@ -0,0 +1,407 @@
+//===- LoongArchRegisterInfo.cpp - LoongArch Register Information -------------------===//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+//
+// This file contains the LoongArch implementation of the TargetRegisterInfo class.
+//
+//===----------------------------------------------------------------------===//
+
+#include "LoongArchRegisterInfo.h"
+#include "MCTargetDesc/LoongArchABIInfo.h"
+#include "LoongArch.h"
+#include "LoongArchMachineFunction.h"
+#include "LoongArchSubtarget.h"
+#include "LoongArchTargetMachine.h"
+#include "llvm/ADT/BitVector.h"
+#include "llvm/ADT/STLExtras.h"
+#include "llvm/CodeGen/MachineFrameInfo.h"
+#include "llvm/CodeGen/MachineFunction.h"
+#include "llvm/CodeGen/MachineInstr.h"
+#include "llvm/CodeGen/MachineRegisterInfo.h"
+#include "llvm/CodeGen/TargetFrameLowering.h"
+#include "llvm/CodeGen/TargetRegisterInfo.h"
+#include "llvm/CodeGen/TargetSubtargetInfo.h"
+#include "llvm/IR/Function.h"
+#include "llvm/MC/MCRegisterInfo.h"
+#include "llvm/Support/Debug.h"
+#include "llvm/Support/ErrorHandling.h"
+#include "llvm/Support/raw_ostream.h"
+#include <cstdint>
+
+using namespace llvm;
+
+#define DEBUG_TYPE "loongarch-reg-info"
+
+#define GET_REGINFO_TARGET_DESC
+#include "LoongArchGenRegisterInfo.inc"
+
+LoongArchRegisterInfo::LoongArchRegisterInfo() : LoongArchGenRegisterInfo(LoongArch::RA) {}
+
+unsigned LoongArchRegisterInfo::getPICCallReg() { return LoongArch::T8; }
+
+const TargetRegisterClass *
+LoongArchRegisterInfo::getPointerRegClass(const MachineFunction &MF,
+                                     unsigned Kind) const {
+  LoongArchABIInfo ABI = MF.getSubtarget<LoongArchSubtarget>().getABI();
+  LoongArchPtrClass PtrClassKind = static_cast<LoongArchPtrClass>(Kind);
+
+  switch (PtrClassKind) {
+  case LoongArchPtrClass::Default:
+    return ABI.ArePtrs64bit() ? &LoongArch::GPR64RegClass : &LoongArch::GPR32RegClass;
+  case LoongArchPtrClass::StackPointer:
+    return ABI.ArePtrs64bit() ? &LoongArch::SP64RegClass : &LoongArch::SP32RegClass;
+  }
+
+  llvm_unreachable("Unknown pointer kind");
+}
+
+unsigned
+LoongArchRegisterInfo::getRegPressureLimit(const TargetRegisterClass *RC,
+                                      MachineFunction &MF) const {
+  switch (RC->getID()) {
+  default:
+    return 0;
+  case LoongArch::GPR32RegClassID:
+  case LoongArch::GPR64RegClassID:
+  {
+    const TargetFrameLowering *TFI = MF.getSubtarget().getFrameLowering();
+    return 28 - TFI->hasFP(MF);
+  }
+  case LoongArch::FGR32RegClassID:
+    return 32;
+  case LoongArch::FGR64RegClassID:
+    return 32;
+  }
+}
+
+//===----------------------------------------------------------------------===//
+// Callee Saved Registers methods
+//===----------------------------------------------------------------------===//
+
+/// LoongArch Callee Saved Registers
+const MCPhysReg *
+LoongArchRegisterInfo::getCalleeSavedRegs(const MachineFunction *MF) const {
+  const LoongArchSubtarget &Subtarget = MF->getSubtarget<LoongArchSubtarget>();
+
+  if (Subtarget.isSingleFloat())
+    return CSR_SingleFloatOnly_SaveList;
+
+  if (Subtarget.isABI_LP64D())
+    return CSR_LP64_SaveList;
+
+  if (Subtarget.isABI_LPX32())
+    return CSR_LPX32_SaveList;
+
+  return CSR_LP32_SaveList;
+}
+
+const uint32_t *
+LoongArchRegisterInfo::getCallPreservedMask(const MachineFunction &MF,
+                                       CallingConv::ID) const {
+  const LoongArchSubtarget &Subtarget = MF.getSubtarget<LoongArchSubtarget>();
+
+  if (Subtarget.isSingleFloat())
+    return CSR_SingleFloatOnly_RegMask;
+
+  if (Subtarget.isABI_LP64D())
+    return CSR_LP64_RegMask;
+
+  return CSR_LP32_RegMask;
+}
+
+BitVector LoongArchRegisterInfo::
+getReservedRegs(const MachineFunction &MF) const {
+  static const MCPhysReg ReservedGPR32[] = {
+    LoongArch::ZERO, LoongArch::SP, LoongArch::TP, LoongArch::T9
+  };
+
+  static const MCPhysReg ReservedGPR64[] = {
+    LoongArch::ZERO_64, LoongArch::SP_64, LoongArch::TP_64, LoongArch::T9_64
+  };
+
+  BitVector Reserved(getNumRegs());
+  const LoongArchSubtarget &Subtarget = MF.getSubtarget<LoongArchSubtarget>();
+
+  for (unsigned I = 0; I < array_lengthof(ReservedGPR32); ++I)
+    Reserved.set(ReservedGPR32[I]);
+
+  for (unsigned I = 0; I < array_lengthof(ReservedGPR64); ++I)
+    Reserved.set(ReservedGPR64[I]);
+
+  // Reserve FP if this function should have a dedicated frame pointer register.
+  if (Subtarget.getFrameLowering()->hasFP(MF)) {
+    Reserved.set(LoongArch::FP);
+    Reserved.set(LoongArch::FP_64);
+
+    // Reserve the base register if we need to both realign the stack and
+    // allocate variable-sized objects at runtime. This should test the
+    // same conditions as LoongArchFrameLowering::hasBP().
+    if (needsStackRealignment(MF) &&
+        MF.getFrameInfo().hasVarSizedObjects()) {
+      Reserved.set(LoongArch::S7);
+      Reserved.set(LoongArch::S7_64);
+    }
+  }
+
+  return Reserved;
+}
+
+bool
+LoongArchRegisterInfo::requiresRegisterScavenging(const MachineFunction &MF) const {
+  return true;
+}
+
+bool LoongArchRegisterInfo::
+requiresFrameIndexScavenging(const MachineFunction &MF) const {
+  return true;
+}
+
+bool
+LoongArchRegisterInfo::trackLivenessAfterRegAlloc(const MachineFunction &MF) const {
+  return true;
+}
+
+/// Get the size of the offset supported by the given load/store/inline asm.
+/// The result includes the effects of any scale factors applied to the
+/// instruction immediate.
+static inline unsigned getLoadStoreOffsetSizeInBits(const unsigned Opcode,
+                                                    MachineOperand MO) {
+  switch (Opcode) {
+  case LoongArch::LDPTR_W:
+  case LoongArch::LDPTR_W32:
+  case LoongArch::LDPTR_D:
+  case LoongArch::STPTR_W:
+  case LoongArch::STPTR_W32:
+  case LoongArch::STPTR_D:
+  case LoongArch::LL_W:
+  case LoongArch::LL_D:
+  case LoongArch::SC_W:
+  case LoongArch::SC_D:
+    return 14 + 2 /* scale factor */;
+  case LoongArch::INLINEASM: {
+    unsigned ConstraintID = InlineAsm::getMemoryConstraintID(MO.getImm());
+    switch (ConstraintID) {
+    case InlineAsm::Constraint_ZC: {
+      return 14 + 2 /* scale factor */;
+    }
+    default:
+      return 12;
+    }
+  }
+  default:
+    return 12;
+  }
+}
+
+/// Get the scale factor applied to the immediate in the given load/store.
+static inline unsigned getLoadStoreOffsetAlign(const unsigned Opcode) {
+  switch (Opcode) {
+  case LoongArch::LDPTR_W:
+  case LoongArch::LDPTR_W32:
+  case LoongArch::LDPTR_D:
+  case LoongArch::STPTR_W:
+  case LoongArch::STPTR_W32:
+  case LoongArch::STPTR_D:
+  case LoongArch::LL_W:
+  case LoongArch::LL_D:
+  case LoongArch::SC_W:
+  case LoongArch::SC_D:
+    return 4;
+  default:
+    return 1;
+  }
+}
+
+// FrameIndex represent objects inside a abstract stack.
+// We must replace FrameIndex with an stack/frame pointer
+// direct reference.
+void LoongArchRegisterInfo::
+eliminateFrameIndex(MachineBasicBlock::iterator II, int SPAdj,
+                    unsigned FIOperandNum, RegScavenger *RS) const {
+  MachineInstr &MI = *II;
+  MachineFunction &MF = *MI.getParent()->getParent();
+
+  LLVM_DEBUG(errs() << "\nFunction : " << MF.getName() << "\n";
+             errs() << "<--------->\n"
+                    << MI);
+
+  int FrameIndex = MI.getOperand(FIOperandNum).getIndex();
+  uint64_t stackSize = MF.getFrameInfo().getStackSize();
+  int64_t spOffset = MF.getFrameInfo().getObjectOffset(FrameIndex);
+
+  LLVM_DEBUG(errs() << "FrameIndex : " << FrameIndex << "\n"
+                    << "spOffset   : " << spOffset << "\n"
+                    << "stackSize  : " << stackSize << "\n"
+                    << "SPAdj      : " << SPAdj << "\n"
+                    << "alignment  : "
+                    << DebugStr(MF.getFrameInfo().getObjectAlign(FrameIndex))
+                    << "\n");
+
+  MachineFrameInfo &MFI = MF.getFrameInfo();
+  LoongArchFunctionInfo *LoongArchFI = MF.getInfo<LoongArchFunctionInfo>();
+
+  LoongArchABIInfo ABI =
+    static_cast<const LoongArchTargetMachine &>(MF.getTarget()).getABI();
+  const LoongArchRegisterInfo *RegInfo =
+    static_cast<const LoongArchRegisterInfo *>(MF.getSubtarget().getRegisterInfo());
+
+  const std::vector<CalleeSavedInfo> &CSI = MFI.getCalleeSavedInfo();
+  int MinCSFI = 0;
+  int MaxCSFI = -1;
+
+  if (CSI.size()) {
+    MinCSFI = CSI[0].getFrameIdx();
+    MaxCSFI = CSI[CSI.size() - 1].getFrameIdx();
+  }
+
+  bool EhDataRegFI = LoongArchFI->isEhDataRegFI(FrameIndex);
+  // The following stack frame objects are always referenced relative to $sp:
+  //  1. Outgoing arguments.
+  //  2. Pointer to dynamically allocated stack space.
+  //  3. Locations for callee-saved registers.
+  //  4. Locations for eh data registers.
+  // Everything else is referenced relative to whatever register
+  // getFrameRegister() returns.
+  unsigned FrameReg;
+
+  if ((FrameIndex >= MinCSFI && FrameIndex <= MaxCSFI) || EhDataRegFI)
+    FrameReg = ABI.GetStackPtr();
+  else if (RegInfo->needsStackRealignment(MF)) {
+    if (MFI.hasVarSizedObjects() && !MFI.isFixedObjectIndex(FrameIndex))
+      FrameReg = ABI.GetBasePtr();
+    else if (MFI.isFixedObjectIndex(FrameIndex))
+      FrameReg = getFrameRegister(MF);
+    else
+      FrameReg = ABI.GetStackPtr();
+  } else
+    FrameReg = getFrameRegister(MF);
+
+  // Calculate final offset.
+  // - There is no need to change the offset if the frame object is one of the
+  //   following: an outgoing argument, pointer to a dynamically allocated
+  //   stack space or a $gp restore location,
+  // - If the frame object is any of the following, its offset must be adjusted
+  //   by adding the size of the stack:
+  //   incoming argument, callee-saved register location or local variable.
+  bool IsKill = false;
+  int64_t Offset;
+
+  Offset = spOffset + (int64_t)stackSize;
+  Offset += MI.getOperand(FIOperandNum + 1).getImm();
+  // If the frame-pointer register($fp) is omited, the offset must be adjusted
+  // by adding the SP adjustment.
+  if (FrameReg == ABI.GetStackPtr())
+    Offset += SPAdj;
+
+  LLVM_DEBUG(errs() << "Offset     : " << Offset << "\n"
+                    << "<--------->\n");
+
+  if (!MI.isDebugValue()) {
+    // Make sure Offset fits within the field available.
+    // For ldptr/stptr/ll/sc instructions, this is a 14-bit signed immediate (scaled by
+    // 2), otherwise it is a 12-bit signed immediate.
+    unsigned OffsetBitSize =
+        getLoadStoreOffsetSizeInBits(MI.getOpcode(), MI.getOperand(FIOperandNum - 1));
+    const Align OffsetAlign(getLoadStoreOffsetAlign(MI.getOpcode()));
+
+    if (OffsetBitSize == 16 && isInt<12>(Offset) &&
+        !isAligned(OffsetAlign, Offset)) {
+      // If we have an offset that needs to fit into a signed n-bit immediate
+      // (where n == 16) and doesn't aligned and does fit into 12-bits
+      // then use an ADDI
+      MachineBasicBlock &MBB = *MI.getParent();
+      DebugLoc DL = II->getDebugLoc();
+      const TargetRegisterClass *PtrRC =
+          ABI.ArePtrs64bit() ? &LoongArch::GPR64RegClass : &LoongArch::GPR32RegClass;
+      MachineRegisterInfo &RegInfo = MBB.getParent()->getRegInfo();
+      unsigned Reg = RegInfo.createVirtualRegister(PtrRC);
+      const LoongArchInstrInfo &TII =
+          *static_cast<const LoongArchInstrInfo *>(
+              MBB.getParent()->getSubtarget().getInstrInfo());
+      BuildMI(MBB, II, DL, TII.get(ABI.GetPtrAddiOp()), Reg)
+          .addReg(FrameReg)
+          .addImm(Offset);
+
+      FrameReg = Reg;
+      Offset = 0;
+      IsKill = true;
+    } else if (!isInt<12>(Offset)) {
+      // Otherwise split the offset into several pieces and add it in multiple
+      // instructions.
+      MachineBasicBlock &MBB = *MI.getParent();
+      DebugLoc DL = II->getDebugLoc();
+      bool IsNewImm = false;
+      unsigned NewImm = 0;
+      const LoongArchInstrInfo &TII =
+          *static_cast<const LoongArchInstrInfo *>(
+              MBB.getParent()->getSubtarget().getInstrInfo());
+      if (OffsetBitSize == 12 || isAligned(OffsetAlign, Offset))
+        IsNewImm = true;
+      unsigned Reg = TII.loadImmediate(Offset, MBB, II, DL,
+                                       IsNewImm ? &NewImm : nullptr);
+      BuildMI(MBB, II, DL, TII.get(ABI.GetPtrAddOp()), Reg).addReg(FrameReg)
+        .addReg(Reg, RegState::Kill);
+
+      FrameReg = Reg;
+      Offset = SignExtend64<12>(NewImm);
+      IsKill = true;
+    }
+  }
+
+  MI.getOperand(FIOperandNum).ChangeToRegister(FrameReg, false, false, IsKill);
+  MI.getOperand(FIOperandNum + 1).ChangeToImmediate(Offset);
+}
+
+Register LoongArchRegisterInfo::
+getFrameRegister(const MachineFunction &MF) const {
+  const LoongArchSubtarget &Subtarget = MF.getSubtarget<LoongArchSubtarget>();
+  const TargetFrameLowering *TFI = Subtarget.getFrameLowering();
+  bool IsLP64D =
+      static_cast<const LoongArchTargetMachine &>(MF.getTarget()).getABI().IsLP64D();
+
+  return TFI->hasFP(MF) ? (IsLP64D ? LoongArch::FP_64 : LoongArch::FP) :
+                            (IsLP64D ? LoongArch::SP_64 : LoongArch::SP);
+}
+
+const TargetRegisterClass *
+LoongArchRegisterInfo::intRegClass(unsigned Size) const {
+  if (Size == 4)
+    return &LoongArch::GPR32RegClass;
+
+  assert(Size == 8);
+  return &LoongArch::GPR64RegClass;
+}
+
+bool LoongArchRegisterInfo::canRealignStack(const MachineFunction &MF) const {
+  // Avoid realigning functions that explicitly do not want to be realigned.
+  // Normally, we should report an error when a function should be dynamically
+  // realigned but also has the attribute no-realign-stack. Unfortunately,
+  // with this attribute, MachineFrameInfo clamps each new object's alignment
+  // to that of the stack's alignment as specified by the ABI. As a result,
+  // the information of whether we have objects with larger alignment
+  // requirement than the stack's alignment is already lost at this point.
+  if (!TargetRegisterInfo::canRealignStack(MF))
+    return false;
+
+  const LoongArchSubtarget &Subtarget = MF.getSubtarget<LoongArchSubtarget>();
+  unsigned FP = Subtarget.is64Bit() ? LoongArch::FP_64 : LoongArch::FP;
+  unsigned BP = Subtarget.is64Bit() ? LoongArch::S7_64 : LoongArch::S7;
+
+  // We can't perform dynamic stack realignment if we can't reserve the
+  // frame pointer register.
+  if (!MF.getRegInfo().canReserveReg(FP))
+    return false;
+
+  // We can realign the stack if we know the maximum call frame size and we
+  // don't have variable sized objects.
+  if (Subtarget.getFrameLowering()->hasReservedCallFrame(MF))
+    return true;
+
+  // We have to reserve the base pointer register in the presence of variable
+  // sized objects.
+  return MF.getRegInfo().canReserveReg(BP);
+}
diff --git a/llvm/lib/Target/LoongArch/LoongArchRegisterInfo.h b/llvm/lib/Target/LoongArch/LoongArchRegisterInfo.h
new file mode 100644
index 000000000000..dd3be916a73c
--- /dev/null
+++ b/llvm/lib/Target/LoongArch/LoongArchRegisterInfo.h
@@ -0,0 +1,80 @@
+//===- LoongArchRegisterInfo.h - LoongArch Register Information Impl ------*- C++ -*-===//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+//
+// This file contains the LoongArch implementation of the TargetRegisterInfo class.
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef LLVM_LIB_TARGET_LOONGARCH_LOONGARCHREGISTERINFO_H
+#define LLVM_LIB_TARGET_LOONGARCH_LOONGARCHREGISTERINFO_H
+
+#include "LoongArch.h"
+#include "llvm/CodeGen/MachineBasicBlock.h"
+#include <cstdint>
+
+#define GET_REGINFO_HEADER
+#include "LoongArchGenRegisterInfo.inc"
+
+namespace llvm {
+
+class TargetRegisterClass;
+
+class LoongArchRegisterInfo : public LoongArchGenRegisterInfo {
+public:
+  enum class LoongArchPtrClass {
+    /// The default register class for integer values.
+    Default = 0,
+    /// The stack pointer only.
+    StackPointer = 1,
+  };
+
+  LoongArchRegisterInfo();
+
+  /// Get PIC indirect call register
+  static unsigned getPICCallReg();
+
+  /// Code Generation virtual methods...
+  const TargetRegisterClass *getPointerRegClass(const MachineFunction &MF,
+                                                unsigned Kind) const override;
+
+  unsigned getRegPressureLimit(const TargetRegisterClass *RC,
+                               MachineFunction &MF) const override;
+  const MCPhysReg *getCalleeSavedRegs(const MachineFunction *MF) const override;
+  const uint32_t *getCallPreservedMask(const MachineFunction &MF,
+                                       CallingConv::ID) const override;
+  BitVector getReservedRegs(const MachineFunction &MF) const override;
+
+  bool requiresRegisterScavenging(const MachineFunction &MF) const override;
+
+  bool requiresFrameIndexScavenging(const MachineFunction &MF) const override;
+
+  bool trackLivenessAfterRegAlloc(const MachineFunction &MF) const override;
+
+  /// Stack Frame Processing Methods
+  void eliminateFrameIndex(MachineBasicBlock::iterator II,
+                           int SPAdj, unsigned FIOperandNum,
+                           RegScavenger *RS = nullptr) const override;
+
+  // Stack realignment queries.
+  bool canRealignStack(const MachineFunction &MF) const override;
+
+  /// Debug information queries.
+  Register getFrameRegister(const MachineFunction &MF) const override;
+
+  /// Return GPR register class.
+  const TargetRegisterClass *intRegClass(unsigned Size) const;
+
+private:
+  void eliminateFI(MachineBasicBlock::iterator II, unsigned OpNo,
+                   int FrameIndex, uint64_t StackSize,
+                   int SPAdj, int64_t SPOffset) const;
+};
+
+} // end namespace llvm
+
+#endif // LLVM_LIB_TARGET_LOONGARCH_LOONGARCHREGISTERINFO_H
diff --git a/llvm/lib/Target/LoongArch/LoongArchRegisterInfo.td b/llvm/lib/Target/LoongArch/LoongArchRegisterInfo.td
new file mode 100644
index 000000000000..b5341c5d6431
--- /dev/null
+++ b/llvm/lib/Target/LoongArch/LoongArchRegisterInfo.td
@@ -0,0 +1,275 @@
+//===-- LoongArchRegisterInfo.td - LoongArch Register defs -----------*- tablegen -*-===//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+
+//===----------------------------------------------------------------------===//
+//  Declarations that describe the LoongArch register file
+//===----------------------------------------------------------------------===//
+let Namespace = "LoongArch" in {
+def sub_32     : SubRegIndex<32>;
+def sub_64     : SubRegIndex<64>;
+def sub_128    : SubRegIndex<128>;
+def sub_fcsr1  : SubRegIndex<5>;
+def sub_fcsr2  : SubRegIndex<13, 16>;
+def sub_fcsr3  : SubRegIndex<2, 8>;
+def sub_lo     : SubRegIndex<32>;
+def sub_hi     : SubRegIndex<32, 32>;
+def PC : Register<"pc">;
+}
+
+class Unallocatable {
+  bit isAllocatable = 0;
+}
+
+/// We have banks of registers each.
+class LoongArchReg<bits<16> Enc, string n> : Register<n> {
+  let HWEncoding = Enc;
+  let Namespace = "LoongArch";
+}
+
+class LoongArchRegWithSubRegs<bits<16> Enc, string n, list<Register> subregs>
+  : RegisterWithSubRegs<n, subregs> {
+  let HWEncoding = Enc;
+  let Namespace = "LoongArch";
+}
+
+/// LoongArch 32-bit CPU Registers.
+class LoongArch32GPR<bits<16> Enc, string n> : LoongArchReg<Enc, n>;
+
+/// LoongArch 64-bit CPU Registers.
+class LoongArch64GPR<bits<16> Enc, string n, list<Register> subregs>
+  : LoongArchRegWithSubRegs<Enc, n, subregs> {
+  let SubRegIndices = [sub_32];
+}
+
+/// LoongArch 64-bit Floating-point Registers
+class FGR32<bits<16> Enc, string n> : LoongArchReg<Enc, n>;
+class FGR64<bits<16> Enc, string n, list<Register> subregs>
+  : LoongArchRegWithSubRegs<Enc, n, subregs> {
+  let SubRegIndices = [sub_lo];
+}
+
+//===----------------------------------------------------------------------===//
+//  Registers
+//===----------------------------------------------------------------------===//
+
+/// General Purpose 32-bit Registers
+def ZERO : LoongArch32GPR<0,  "zero">, DwarfRegNum<[0]>;
+def RA   : LoongArch32GPR<1,  "ra">,   DwarfRegNum<[1]>;
+def TP   : LoongArch32GPR<2,  "tp">,   DwarfRegNum<[2]>;
+def SP   : LoongArch32GPR<3,  "sp">,   DwarfRegNum<[3]>;
+def A0   : LoongArch32GPR<4,  "r4">,   DwarfRegNum<[4]>;
+def A1   : LoongArch32GPR<5,  "r5">,   DwarfRegNum<[5]>;
+def A2   : LoongArch32GPR<6,  "r6">,   DwarfRegNum<[6]>;
+def A3   : LoongArch32GPR<7,  "r7">,   DwarfRegNum<[7]>;
+def A4   : LoongArch32GPR<8,  "r8">,   DwarfRegNum<[8]>;
+def A5   : LoongArch32GPR<9,  "r9">,   DwarfRegNum<[9]>;
+def A6   : LoongArch32GPR<10, "r10">,  DwarfRegNum<[10]>;
+def A7   : LoongArch32GPR<11, "r11">,  DwarfRegNum<[11]>;
+def T0   : LoongArch32GPR<12, "r12">,  DwarfRegNum<[12]>;
+def T1   : LoongArch32GPR<13, "r13">,  DwarfRegNum<[13]>;
+def T2   : LoongArch32GPR<14, "r14">,  DwarfRegNum<[14]>;
+def T3   : LoongArch32GPR<15, "r15">,  DwarfRegNum<[15]>;
+def T4   : LoongArch32GPR<16, "r16">,  DwarfRegNum<[16]>;
+def T5   : LoongArch32GPR<17, "r17">,  DwarfRegNum<[17]>;
+def T6   : LoongArch32GPR<18, "r18">,  DwarfRegNum<[18]>;
+def T7   : LoongArch32GPR<19, "r19">,  DwarfRegNum<[19]>;
+def T8   : LoongArch32GPR<20, "r20">,  DwarfRegNum<[20]>;
+def T9   : LoongArch32GPR<21, "r21">,  DwarfRegNum<[21]>;
+def FP   : LoongArch32GPR<22, "r22">,  DwarfRegNum<[22]>;
+def S0   : LoongArch32GPR<23, "r23">,  DwarfRegNum<[23]>;
+def S1   : LoongArch32GPR<24, "r24">,  DwarfRegNum<[24]>;
+def S2   : LoongArch32GPR<25, "r25">,  DwarfRegNum<[25]>;
+def S3   : LoongArch32GPR<26, "r26">,  DwarfRegNum<[26]>;
+def S4   : LoongArch32GPR<27, "r27">,  DwarfRegNum<[27]>;
+def S5   : LoongArch32GPR<28, "r28">,  DwarfRegNum<[28]>;
+def S6   : LoongArch32GPR<29, "r29">,  DwarfRegNum<[29]>;
+def S7   : LoongArch32GPR<30, "r30">,  DwarfRegNum<[30]>;
+def S8   : LoongArch32GPR<31, "r31">,  DwarfRegNum<[31]>;
+
+let SubRegIndices = [sub_32] in {
+def V0   : LoongArchRegWithSubRegs<4, "r4",  [A0]>, DwarfRegNum<[4]>;
+def V1   : LoongArchRegWithSubRegs<5, "r5",  [A1]>, DwarfRegNum<[5]>;
+}
+
+/// General Purpose 64-bit Registers
+def ZERO_64 : LoongArch64GPR<0, "zero", [ZERO]>, DwarfRegNum<[0]>;
+def RA_64   : LoongArch64GPR<1, "ra",    [RA]>, DwarfRegNum<[1]>;
+def TP_64   : LoongArch64GPR<2, "tp",    [TP]>, DwarfRegNum<[2]>;
+def SP_64   : LoongArch64GPR<3, "sp",    [SP]>, DwarfRegNum<[3]>;
+def A0_64   : LoongArch64GPR<4, "r4",    [A0]>, DwarfRegNum<[4]>;
+def A1_64   : LoongArch64GPR<5, "r5",    [A1]>, DwarfRegNum<[5]>;
+def A2_64   : LoongArch64GPR<6, "r6",    [A2]>, DwarfRegNum<[6]>;
+def A3_64   : LoongArch64GPR<7, "r7",    [A3]>, DwarfRegNum<[7]>;
+def A4_64   : LoongArch64GPR<8, "r8",    [A4]>, DwarfRegNum<[8]>;
+def A5_64   : LoongArch64GPR<9, "r9",    [A5]>, DwarfRegNum<[9]>;
+def A6_64   : LoongArch64GPR<10, "r10",  [A6]>, DwarfRegNum<[10]>;
+def A7_64   : LoongArch64GPR<11, "r11",  [A7]>, DwarfRegNum<[11]>;
+def T0_64   : LoongArch64GPR<12, "r12",  [T0]>, DwarfRegNum<[12]>;
+def T1_64   : LoongArch64GPR<13, "r13",  [T1]>, DwarfRegNum<[13]>;
+def T2_64   : LoongArch64GPR<14, "r14",  [T2]>, DwarfRegNum<[14]>;
+def T3_64   : LoongArch64GPR<15, "r15",  [T3]>, DwarfRegNum<[15]>;
+def T4_64   : LoongArch64GPR<16, "r16",  [T4]>, DwarfRegNum<[16]>;
+def T5_64   : LoongArch64GPR<17, "r17",  [T5]>, DwarfRegNum<[17]>;
+def T6_64   : LoongArch64GPR<18, "r18",  [T6]>, DwarfRegNum<[18]>;
+def T7_64   : LoongArch64GPR<19, "r19",  [T7]>, DwarfRegNum<[19]>;
+def T8_64   : LoongArch64GPR<20, "r20",  [T8]>, DwarfRegNum<[20]>;
+def T9_64   : LoongArch64GPR<21, "r21",  [T9]>, DwarfRegNum<[21]>;
+def FP_64   : LoongArch64GPR<22, "r22",  [FP]>, DwarfRegNum<[22]>;
+def S0_64   : LoongArch64GPR<23, "r23",  [S0]>, DwarfRegNum<[23]>;
+def S1_64   : LoongArch64GPR<24, "r24",  [S1]>, DwarfRegNum<[24]>;
+def S2_64   : LoongArch64GPR<25, "r25",  [S2]>, DwarfRegNum<[25]>;
+def S3_64   : LoongArch64GPR<26, "r26",  [S3]>, DwarfRegNum<[26]>;
+def S4_64   : LoongArch64GPR<27, "r27",  [S4]>, DwarfRegNum<[27]>;
+def S5_64   : LoongArch64GPR<28, "r28",  [S5]>, DwarfRegNum<[28]>;
+def S6_64   : LoongArch64GPR<29, "r29",  [S6]>, DwarfRegNum<[29]>;
+def S7_64   : LoongArch64GPR<30, "r30",  [S7]>, DwarfRegNum<[30]>;
+def S8_64   : LoongArch64GPR<31, "r31",  [S8]>, DwarfRegNum<[31]>;
+
+let SubRegIndices = [sub_64] in {
+def V0_64   : LoongArch64GPR<4, "r4",  [A0_64]>, DwarfRegNum<[4]>;
+def V1_64   : LoongArch64GPR<5, "r5",  [A1_64]>, DwarfRegNum<[5]>;
+}
+
+/// FP registers
+foreach I = 0-31 in
+def F#I : FGR32<I, "f"#I>, DwarfRegNum<[!add(I, 32)]>;
+
+foreach I = 0-31 in
+def F#I#_64 : FGR64<I, "f"#I, [!cast<FGR32>("F"#I)]>, DwarfRegNum<[!add(I, 32)]>;
+
+/// FP Condition Flag 0~7
+foreach I = 0-7 in
+def FCC#I : LoongArchReg<I, "fcc"#I>;
+
+/// FP Control and Status Registers, FCSR 1~3
+foreach I = 1-3 in
+def FCSR#I : LoongArchReg<I, "fcsr"#I>;
+
+class FCSRReg<bits<16> Enc, string n, list<Register> subregs> :
+    RegisterWithSubRegs<n, subregs> {
+//  field bits<2> chan_encoding = 0;
+  let Namespace = "LoongArch";
+  let SubRegIndices = [sub_fcsr1, sub_fcsr2, sub_fcsr3];
+//  let HWEncoding{8-0} = encoding{8-0};
+//  let HWEncoding{10-9} = chan_encoding;
+}
+
+def FCSR0 : FCSRReg<0, "fcsr0", [FCSR1, FCSR2, FCSR3]>;
+
+/// PC register
+//let NameSpace = "LoongArch" in
+//def PC : Register<"pc">;
+
+//===----------------------------------------------------------------------===//
+// Register Classes
+//===----------------------------------------------------------------------===//
+
+def GPR32 : RegisterClass<"LoongArch", [i32], 32, (add
+  // Reserved
+  ZERO,
+  // Return Values and Arguments
+  A0, A1, A2, A3, A4, A5, A6, A7,
+  // Not preserved across procedure calls
+  T0, T1, T2, T3, T4, T5, T6, T7, T8,
+  // Callee save
+  S0, S1, S2, S3, S4, S5, S6, S7, S8,
+  // Reserved
+  RA, TP, SP,
+  // Reserved
+  T9, FP)>;
+
+def GPR64 : RegisterClass<"LoongArch", [i64], 64, (add
+  // Reserved
+  ZERO_64,
+  // Return Values and Arguments
+  A0_64, A1_64, A2_64, A3_64, A4_64, A5_64, A6_64, A7_64,
+  // Not preserved across procedure calls
+  T0_64, T1_64, T2_64, T3_64, T4_64, T5_64, T6_64, T7_64, T8_64,
+  // Callee save
+  S0_64, S1_64, S2_64, S3_64, S4_64, S5_64, S6_64, S7_64, S8_64,
+  // Reserved
+  RA_64, TP_64, SP_64,
+  // Reserved
+  T9_64, FP_64)>;
+
+/// FP Registers.
+def FGR64 : RegisterClass<"LoongArch", [f64], 64, (sequence "F%u_64", 0, 31)>;
+def FGR32 : RegisterClass<"LoongArch", [f32], 64, (sequence "F%u", 0, 31)>;
+
+/// FP condition Flag registers.
+def FCFR : RegisterClass<"LoongArch", [i32], 32, (sequence "FCC%u", 0, 7)>,
+           Unallocatable;
+
+def SP32 : RegisterClass<"LoongArch", [i32], 32, (add SP)>, Unallocatable;
+def SP64 : RegisterClass<"LoongArch", [i64], 64, (add SP_64)>, Unallocatable;
+def TP32 : RegisterClass<"LoongArch", [i32], 32, (add TP)>, Unallocatable;
+def TP64 : RegisterClass<"LoongArch", [i64], 64, (add TP_64)>, Unallocatable;
+
+/// FP control and Status registers.
+def FCSR : RegisterClass<"LoongArch", [i32], 4, (sequence "FCSR%u", 0, 3)>,
+           Unallocatable;
+
+//===----------------------------------------------------------------------===//
+// Register Operands.
+//===----------------------------------------------------------------------===//
+
+class LoongArchAsmRegOperand : AsmOperandClass {
+  let ParserMethod = "parseAnyRegister";
+}
+
+def GPR32AsmOperand : LoongArchAsmRegOperand {
+  let Name = "GPR32AsmReg";
+  let PredicateMethod = "isGPRAsmReg";
+}
+
+def GPR64AsmOperand : LoongArchAsmRegOperand {
+  let Name = "GPR64AsmReg";
+  let PredicateMethod = "isGPRAsmReg";
+}
+
+def FGR32AsmOperand : LoongArchAsmRegOperand {
+  let Name = "FGR32AsmReg";
+  let PredicateMethod = "isFGRAsmReg";
+}
+
+def FGR64AsmOperand : LoongArchAsmRegOperand {
+  let Name = "FGR64AsmReg";
+  let PredicateMethod = "isFGRAsmReg";
+}
+
+def FCSRAsmOperand : LoongArchAsmRegOperand {
+  let Name = "FCSRAsmReg";
+}
+
+def FCFRAsmOperand : LoongArchAsmRegOperand {
+  let Name = "FCFRAsmReg";
+}
+
+def GPR32Opnd : RegisterOperand<GPR32> {
+  let ParserMatchClass = GPR32AsmOperand;
+}
+
+def GPR64Opnd : RegisterOperand<GPR64> {
+  let ParserMatchClass = GPR64AsmOperand;
+}
+
+def FGR32Opnd : RegisterOperand<FGR32> {
+  let ParserMatchClass = FGR32AsmOperand;
+}
+
+def FGR64Opnd : RegisterOperand<FGR64> {
+  let ParserMatchClass = FGR64AsmOperand;
+}
+
+def FCSROpnd : RegisterOperand<FCSR> {
+  let ParserMatchClass = FCSRAsmOperand;
+}
+
+def FCFROpnd : RegisterOperand<FCFR> {
+  let ParserMatchClass = FCFRAsmOperand;
+}
diff --git a/llvm/lib/Target/LoongArch/LoongArchSubtarget.cpp b/llvm/lib/Target/LoongArch/LoongArchSubtarget.cpp
new file mode 100644
index 000000000000..74036c7c3e93
--- /dev/null
+++ b/llvm/lib/Target/LoongArch/LoongArchSubtarget.cpp
@@ -0,0 +1,102 @@
+//===-- LoongArchSubtarget.cpp - LoongArch Subtarget Information --------------------===//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+//
+// This file implements the LoongArch specific subclass of TargetSubtargetInfo.
+//
+//===----------------------------------------------------------------------===//
+
+#include "LoongArchSubtarget.h"
+#include "LoongArch.h"
+#include "LoongArchMachineFunction.h"
+#include "LoongArchRegisterInfo.h"
+#include "LoongArchTargetMachine.h"
+#include "llvm/IR/Attributes.h"
+#include "llvm/IR/Function.h"
+#include "llvm/Support/CommandLine.h"
+#include "llvm/Support/Debug.h"
+#include "llvm/Support/TargetRegistry.h"
+#include "llvm/Support/raw_ostream.h"
+
+using namespace llvm;
+
+#define DEBUG_TYPE "loongarch-subtarget"
+
+#define GET_SUBTARGETINFO_TARGET_DESC
+#define GET_SUBTARGETINFO_CTOR
+#include "LoongArchGenSubtargetInfo.inc"
+
+void LoongArchSubtarget::anchor() {}
+
+LoongArchSubtarget::LoongArchSubtarget(const Triple &TT, StringRef CPU, StringRef FS,
+                             const LoongArchTargetMachine &TM,
+                             MaybeAlign StackAlignOverride)
+    : LoongArchGenSubtargetInfo(TT, CPU, FS),
+      HasLA64(false),
+      IsSoftFloat(false), IsSingleFloat(false),
+      IsFP64bit(false),
+      StackAlignOverride(StackAlignOverride),
+      TM(TM), TargetTriple(TT), TSInfo(),
+      InstrInfo(initializeSubtargetDependencies(CPU, FS, TM)),
+      FrameLowering(*this),
+      TLInfo(TM, *this) {
+
+  // Check if Architecture and ABI are compatible.
+  assert(((!is64Bit() && isABI_LP32()) ||
+          (is64Bit() && (isABI_LPX32() || isABI_LP64D()))) &&
+         "Invalid  Arch & ABI pair.");
+
+  assert(isFP64bit());
+}
+
+bool LoongArchSubtarget::isPositionIndependent() const {
+  return TM.isPositionIndependent();
+}
+
+/// This overrides the PostRAScheduler bit in the SchedModel for any CPU.
+bool LoongArchSubtarget::enablePostRAScheduler() const { return true; }
+
+void LoongArchSubtarget::getCriticalPathRCs(RegClassVector &CriticalPathRCs) const {
+  CriticalPathRCs.clear();
+  CriticalPathRCs.push_back(is64Bit() ? &LoongArch::GPR64RegClass
+                                        : &LoongArch::GPR32RegClass);
+}
+
+CodeGenOpt::Level LoongArchSubtarget::getOptLevelToEnablePostRAScheduler() const {
+  return CodeGenOpt::Aggressive;
+}
+
+LoongArchSubtarget &
+LoongArchSubtarget::initializeSubtargetDependencies(StringRef CPU, StringRef FS,
+                                               const TargetMachine &TM) {
+  StringRef CPUName = LoongArch_MC::selectLoongArchCPU(TM.getTargetTriple(), CPU);
+
+  // Parse features string.
+  ParseSubtargetFeatures(CPUName, FS);
+  // Initialize scheduling itinerary for the specified CPU.
+  InstrItins = getInstrItineraryForCPU(CPUName);
+
+  if (StackAlignOverride)
+    stackAlignment = *StackAlignOverride;
+  else if (isABI_LPX32() || isABI_LP64D())
+    stackAlignment = Align(16);
+  else {
+    assert(isABI_LP32() && "Unknown ABI for stack alignment!");
+    stackAlignment = Align(8);
+  }
+
+  return *this;
+}
+
+Reloc::Model LoongArchSubtarget::getRelocationModel() const {
+  return TM.getRelocationModel();
+}
+
+bool LoongArchSubtarget::isABI_LP64D() const { return getABI().IsLP64D(); }
+bool LoongArchSubtarget::isABI_LPX32() const { return getABI().IsLPX32(); }
+bool LoongArchSubtarget::isABI_LP32() const { return getABI().IsLP32(); }
+const LoongArchABIInfo &LoongArchSubtarget::getABI() const { return TM.getABI(); }
diff --git a/llvm/lib/Target/LoongArch/LoongArchSubtarget.h b/llvm/lib/Target/LoongArch/LoongArchSubtarget.h
new file mode 100644
index 000000000000..467ebb9c14f2
--- /dev/null
+++ b/llvm/lib/Target/LoongArch/LoongArchSubtarget.h
@@ -0,0 +1,129 @@
+//===-- LoongArchSubtarget.h - Define Subtarget for the LoongArch ---------*- C++ -*-===//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+//
+// This file declares the LoongArch specific subclass of TargetSubtargetInfo.
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef LLVM_LIB_TARGET_LOONGARCH_LOONGARCHSUBTARGET_H
+#define LLVM_LIB_TARGET_LOONGARCH_LOONGARCHSUBTARGET_H
+
+#include "MCTargetDesc/LoongArchABIInfo.h"
+#include "LoongArchFrameLowering.h"
+#include "LoongArchISelLowering.h"
+#include "LoongArchInstrInfo.h"
+#include "llvm/CodeGen/SelectionDAGTargetInfo.h"
+#include "llvm/CodeGen/TargetSubtargetInfo.h"
+#include "llvm/IR/DataLayout.h"
+#include "llvm/MC/MCInstrItineraries.h"
+#include "llvm/Support/ErrorHandling.h"
+#include <string>
+
+#define GET_SUBTARGETINFO_HEADER
+#include "LoongArchGenSubtargetInfo.inc"
+
+namespace llvm {
+class StringRef;
+
+class LoongArchTargetMachine;
+
+class LoongArchSubtarget : public LoongArchGenSubtargetInfo {
+  virtual void anchor();
+
+  // HasLA64 - The target processor has LA64 ISA support.
+  bool HasLA64;
+
+  // IsSoftFloat - The target does not support any floating point instructions.
+  bool IsSoftFloat;
+
+  // IsSingleFloat - The target only supports single precision float
+  // point operations. This enable the target to use all 32 32-bit
+  // floating point registers instead of only using even ones.
+  bool IsSingleFloat;
+
+  // IsFP64bit - The target processor has 64-bit floating point registers.
+  bool IsFP64bit;
+
+  /// The minimum alignment known to hold of the stack frame on
+  /// entry to the function and which must be maintained by every function.
+  Align stackAlignment;
+
+  /// The overridden stack alignment.
+  MaybeAlign StackAlignOverride;
+
+  InstrItineraryData InstrItins;
+
+  const LoongArchTargetMachine &TM;
+
+  Triple TargetTriple;
+
+  const SelectionDAGTargetInfo TSInfo;
+  const LoongArchInstrInfo InstrInfo;
+  const LoongArchFrameLowering FrameLowering;
+  const LoongArchTargetLowering TLInfo;
+
+public:
+  bool isPositionIndependent() const;
+  /// This overrides the PostRAScheduler bit in the SchedModel for each CPU.
+  bool enablePostRAScheduler() const override;
+  void getCriticalPathRCs(RegClassVector &CriticalPathRCs) const override;
+  CodeGenOpt::Level getOptLevelToEnablePostRAScheduler() const override;
+
+  bool isABI_LP64D() const;
+  bool isABI_LPX32() const;
+  bool isABI_LP32() const;
+  const LoongArchABIInfo &getABI() const;
+
+  /// This constructor initializes the data members to match that
+  /// of the specified triple.
+  LoongArchSubtarget(const Triple &TT, StringRef CPU, StringRef FS,
+      const LoongArchTargetMachine &TM, MaybeAlign StackAlignOverride);
+
+  /// ParseSubtargetFeatures - Parses features string setting specified
+  /// subtarget options.  Definition of function is auto generated by tblgen.
+  void ParseSubtargetFeatures(StringRef CPU, StringRef FS);
+
+  bool is64Bit() const { return HasLA64; }
+  bool isFP64bit() const { return IsFP64bit; }
+  unsigned getGPRSizeInBytes() const { return is64Bit() ? 8 : 4; }
+  bool isSingleFloat() const { return IsSingleFloat; }
+  bool useSoftFloat() const { return IsSoftFloat; }
+
+  // After compiler-rt is supported in LA, this returns true.
+  bool isXRaySupported() const override { return false; }
+
+  Align getStackAlignment() const { return stackAlignment; }
+
+  // Grab relocation model
+  Reloc::Model getRelocationModel() const;
+
+  LoongArchSubtarget &initializeSubtargetDependencies(StringRef CPU, StringRef FS,
+                                                 const TargetMachine &TM);
+
+  const SelectionDAGTargetInfo *getSelectionDAGInfo() const override {
+    return &TSInfo;
+  }
+  const LoongArchInstrInfo *getInstrInfo() const override {
+    return &InstrInfo;
+  }
+  const TargetFrameLowering *getFrameLowering() const override {
+    return &FrameLowering;
+  }
+  const LoongArchRegisterInfo *getRegisterInfo() const override {
+    return &InstrInfo.getRegisterInfo();
+  }
+  const LoongArchTargetLowering *getTargetLowering() const override {
+    return &TLInfo;
+  }
+  const InstrItineraryData *getInstrItineraryData() const override {
+    return &InstrItins;
+  }
+};
+} // End llvm namespace
+
+#endif
diff --git a/llvm/lib/Target/LoongArch/LoongArchTargetMachine.cpp b/llvm/lib/Target/LoongArch/LoongArchTargetMachine.cpp
new file mode 100644
index 000000000000..051bf84b8970
--- /dev/null
+++ b/llvm/lib/Target/LoongArch/LoongArchTargetMachine.cpp
@@ -0,0 +1,194 @@
+//===-- LoongArchTargetMachine.cpp - Define TargetMachine for LoongArch -------------===//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+//
+// Implements the info about LoongArch target spec.
+//
+//===----------------------------------------------------------------------===//
+
+#include "LoongArchTargetMachine.h"
+#include "MCTargetDesc/LoongArchABIInfo.h"
+#include "MCTargetDesc/LoongArchMCTargetDesc.h"
+#include "LoongArch.h"
+#include "LoongArchISelDAGToDAG.h"
+#include "LoongArchSubtarget.h"
+#include "LoongArchTargetObjectFile.h"
+#include "llvm/ADT/Optional.h"
+#include "llvm/ADT/STLExtras.h"
+#include "llvm/ADT/StringRef.h"
+#include "llvm/Analysis/TargetTransformInfo.h"
+#include "llvm/CodeGen/BasicTTIImpl.h"
+#include "llvm/CodeGen/MachineFunction.h"
+#include "llvm/CodeGen/Passes.h"
+#include "llvm/CodeGen/TargetPassConfig.h"
+#include "llvm/IR/Attributes.h"
+#include "llvm/IR/Function.h"
+#include "llvm/Support/CodeGen.h"
+#include "llvm/Support/Debug.h"
+#include "llvm/Support/TargetRegistry.h"
+#include "llvm/Support/raw_ostream.h"
+#include "llvm/Target/TargetOptions.h"
+#include <string>
+#include <cassert>
+
+using namespace llvm;
+
+#define DEBUG_TYPE "loongarch"
+
+extern "C" LLVM_EXTERNAL_VISIBILITY void LLVMInitializeLoongArchTarget() {
+  // Register the target.
+  RegisterTargetMachine<LoongArchTargetMachine> X(getTheLoongArch32Target());
+  RegisterTargetMachine<LoongArchTargetMachine> A(getTheLoongArch64Target());
+}
+
+static std::string computeDataLayout(const Triple &TT, StringRef CPU,
+                                     const TargetOptions &Options) {
+  std::string Ret;
+  LoongArchABIInfo ABI = LoongArchABIInfo::computeTargetABI(TT, CPU, Options.MCOptions);
+
+  Ret += "e";
+
+  if (ABI.IsLP32())
+    Ret += "-m:m";
+  else
+    Ret += "-m:e";
+
+  // Pointers are 32 bit on some ABIs.
+  if (!(ABI.IsLP64D()))
+    Ret += "-p:32:32";
+
+  // 8 and 16 bit integers only need to have natural alignment, but try to
+  // align them to 32 bits. 64 bit integers have natural alignment.
+  Ret += "-i8:8:32-i16:16:32-i64:64";
+
+  // 32 bit registers are always available and the stack is at least 64 bit
+  // aligned. On LP64 64 bit registers are also available and the stack is
+  // 128 bit aligned.
+  if (ABI.IsLP64D() || ABI.IsLPX32())
+    Ret += "-n32:64-S128";
+  else
+    Ret += "-n32-S64";
+
+  return Ret;
+}
+
+static Reloc::Model getEffectiveRelocModel(bool JIT,
+                                           Optional<Reloc::Model> RM) {
+  if (!RM.hasValue() || JIT)
+    return Reloc::Static;
+  return *RM;
+}
+
+// On function prologue, the stack is created by decrementing
+// its pointer. Once decremented, all references are done with positive
+// offset from the stack/frame pointer, using StackGrowsUp enables
+// an easier handling.
+// Using CodeModel::Large enables different CALL behavior.
+LoongArchTargetMachine::LoongArchTargetMachine(const Target &T, const Triple &TT,
+                                     StringRef CPU, StringRef FS,
+                                     const TargetOptions &Options,
+                                     Optional<Reloc::Model> RM,
+                                     Optional<CodeModel::Model> CM,
+                                     CodeGenOpt::Level OL, bool JIT)
+    : LLVMTargetMachine(T, computeDataLayout(TT, CPU, Options), TT,
+                        CPU, FS, Options, getEffectiveRelocModel(JIT, RM),
+                        getEffectiveCodeModel(CM, CodeModel::Small), OL),
+      TLOF(std::make_unique<LoongArchTargetObjectFile>()),
+      ABI(LoongArchABIInfo::computeTargetABI(TT, CPU, Options.MCOptions)) {
+  initAsmInfo();
+}
+
+LoongArchTargetMachine::~LoongArchTargetMachine() = default;
+
+const LoongArchSubtarget *
+LoongArchTargetMachine::getSubtargetImpl(const Function &F) const {
+  Attribute CPUAttr = F.getFnAttribute("target-cpu");
+  Attribute FSAttr = F.getFnAttribute("target-features");
+
+  std::string CPU = !CPUAttr.hasAttribute(Attribute::None)
+                        ? CPUAttr.getValueAsString().str()
+                        : TargetCPU;
+  std::string FS = !FSAttr.hasAttribute(Attribute::None)
+                       ? FSAttr.getValueAsString().str()
+                       : TargetFS;
+
+  // FIXME: This is related to the code below to reset the target options,
+  // we need to know whether or not the soft float flag is set on the
+  // function, so we can enable it as a subtarget feature.
+  bool softFloat =
+      F.hasFnAttribute("use-soft-float") &&
+      F.getFnAttribute("use-soft-float").getValueAsString() == "true";
+
+  if (softFloat)
+    FS += FS.empty() ? "+soft-float" : ",+soft-float";
+
+  auto &I = SubtargetMap[CPU + FS];
+  if (!I) {
+    // This needs to be done before we create a new subtarget since any
+    // creation will depend on the TM and the code generation flags on the
+    // function that reside in TargetOptions.
+    resetTargetOptions(F);
+    I = std::make_unique<LoongArchSubtarget>(TargetTriple, CPU, FS, *this,
+        MaybeAlign(Options.StackAlignmentOverride));
+  }
+  return I.get();
+}
+
+namespace {
+
+/// LoongArch Code Generator Pass Configuration Options.
+class LoongArchPassConfig : public TargetPassConfig {
+public:
+  LoongArchPassConfig(LoongArchTargetMachine &TM, PassManagerBase &PM)
+      : TargetPassConfig(TM, PM) {
+  }
+
+  LoongArchTargetMachine &getLoongArchTargetMachine() const {
+    return getTM<LoongArchTargetMachine>();
+  }
+
+  void addIRPasses() override;
+  bool addInstSelector() override;
+  void addPreEmitPass() override;
+};
+
+} // end anonymous namespace
+
+TargetPassConfig *LoongArchTargetMachine::createPassConfig(PassManagerBase &PM) {
+  return new LoongArchPassConfig(*this, PM);
+}
+
+void LoongArchPassConfig::addIRPasses() {
+  TargetPassConfig::addIRPasses();
+  addPass(createAtomicExpandPass());
+}
+// Install an instruction selector pass using
+// the ISelDag to gen LoongArch code.
+bool LoongArchPassConfig::addInstSelector() {
+  addPass(createLoongArchModuleISelDagPass());
+  addPass(createLoongArchISelDag(getLoongArchTargetMachine(), getOptLevel()));
+  return false;
+}
+
+TargetTransformInfo
+LoongArchTargetMachine::getTargetTransformInfo(const Function &F) {
+  LLVM_DEBUG(errs() << "Target Transform Info Pass Added\n");
+  return TargetTransformInfo(BasicTTIImpl(this, F));
+}
+
+// Implemented by targets that want to run passes immediately before
+// machine code is emitted. return true if -print-machineinstrs should
+// print out the code after the passes.
+void LoongArchPassConfig::addPreEmitPass() {
+  // Expand pseudo instructions that are sensitive to register allocation.
+  addPass(createLoongArchExpandPseudoPass());
+
+  // Relax conditional branch instructions if they're otherwise out of
+  // range of their destination.
+  // This pass must be run after any pseudo instruction expansion
+  addPass(&BranchRelaxationPassID);
+}
diff --git a/llvm/lib/Target/LoongArch/LoongArchTargetMachine.h b/llvm/lib/Target/LoongArch/LoongArchTargetMachine.h
new file mode 100644
index 000000000000..f412e25b3d30
--- /dev/null
+++ b/llvm/lib/Target/LoongArch/LoongArchTargetMachine.h
@@ -0,0 +1,60 @@
+//===- LoongArchTargetMachine.h - Define TargetMachine for LoongArch ------*- C++ -*-===//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+//
+// This file declares the LoongArch specific subclass of TargetMachine.
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef LLVM_LIB_TARGET_LOONGARCH_LOONGARCHTARGETMACHINE_H
+#define LLVM_LIB_TARGET_LOONGARCH_LOONGARCHTARGETMACHINE_H
+
+#include "MCTargetDesc/LoongArchABIInfo.h"
+#include "LoongArchSubtarget.h"
+#include "llvm/ADT/Optional.h"
+#include "llvm/ADT/StringMap.h"
+#include "llvm/ADT/StringRef.h"
+#include "llvm/Support/CodeGen.h"
+#include "llvm/Target/TargetMachine.h"
+#include <memory>
+
+namespace llvm {
+
+class LoongArchTargetMachine : public LLVMTargetMachine {
+  std::unique_ptr<TargetLoweringObjectFile> TLOF;
+  // Selected ABI
+  LoongArchABIInfo ABI;
+
+  mutable StringMap<std::unique_ptr<LoongArchSubtarget>> SubtargetMap;
+
+public:
+  LoongArchTargetMachine(const Target &T, const Triple &TT, StringRef CPU,
+                    StringRef FS, const TargetOptions &Options,
+                    Optional<Reloc::Model> RM, Optional<CodeModel::Model> CM,
+                    CodeGenOpt::Level OL, bool JIT);
+  ~LoongArchTargetMachine() override;
+
+  TargetTransformInfo getTargetTransformInfo(const Function &F) override;
+  const LoongArchSubtarget *getSubtargetImpl(const Function &F) const override;
+
+  // Pass Pipeline Configuration
+  TargetPassConfig *createPassConfig(PassManagerBase &PM) override;
+
+  TargetLoweringObjectFile *getObjFileLowering() const override {
+    return TLOF.get();
+  }
+
+  const LoongArchABIInfo &getABI() const { return ABI; }
+
+  bool isMachineVerifierClean() const override {
+    return false;
+  }
+};
+
+} // end namespace llvm
+
+#endif // LLVM_LIB_TARGET_LOONGARCH_LOONGARCHTARGETMACHINE_H
diff --git a/llvm/lib/Target/LoongArch/LoongArchTargetObjectFile.cpp b/llvm/lib/Target/LoongArch/LoongArchTargetObjectFile.cpp
new file mode 100644
index 000000000000..9c6250d28930
--- /dev/null
+++ b/llvm/lib/Target/LoongArch/LoongArchTargetObjectFile.cpp
@@ -0,0 +1,26 @@
+//===-- LoongArchTargetObjectFile.cpp - LoongArch Object Files ----------------------===//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+
+#include "LoongArchTargetObjectFile.h"
+#include "LoongArchSubtarget.h"
+#include "LoongArchTargetMachine.h"
+#include "MCTargetDesc/LoongArchMCExpr.h"
+#include "llvm/BinaryFormat/ELF.h"
+#include "llvm/IR/DataLayout.h"
+#include "llvm/IR/DerivedTypes.h"
+#include "llvm/IR/GlobalVariable.h"
+#include "llvm/MC/MCContext.h"
+#include "llvm/MC/MCSectionELF.h"
+#include "llvm/Support/CommandLine.h"
+#include "llvm/Target/TargetMachine.h"
+using namespace llvm;
+
+void LoongArchTargetObjectFile::Initialize(MCContext &Ctx, const TargetMachine &TM){
+  TargetLoweringObjectFileELF::Initialize(Ctx, TM);
+  InitializeELF(TM.Options.UseInitArray);
+}
diff --git a/llvm/lib/Target/LoongArch/LoongArchTargetObjectFile.h b/llvm/lib/Target/LoongArch/LoongArchTargetObjectFile.h
new file mode 100644
index 000000000000..a50c57171f80
--- /dev/null
+++ b/llvm/lib/Target/LoongArch/LoongArchTargetObjectFile.h
@@ -0,0 +1,24 @@
+//===-- llvm/Target/LoongArchTargetObjectFile.h - LoongArch Object Info ---*- C++ -*-===//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef LLVM_LIB_TARGET_LOONGARCH_LOONGARCHTARGETOBJECTFILE_H
+#define LLVM_LIB_TARGET_LOONGARCH_LOONGARCHTARGETOBJECTFILE_H
+
+#include "llvm/CodeGen/TargetLoweringObjectFileImpl.h"
+
+namespace llvm {
+class LoongArchTargetMachine;
+  class LoongArchTargetObjectFile : public TargetLoweringObjectFileELF {
+
+  public:
+
+    void Initialize(MCContext &Ctx, const TargetMachine &TM) override;
+  };
+} // end namespace llvm
+
+#endif
diff --git a/llvm/lib/Target/LoongArch/LoongArchTargetStreamer.h b/llvm/lib/Target/LoongArch/LoongArchTargetStreamer.h
new file mode 100644
index 000000000000..6708303fac27
--- /dev/null
+++ b/llvm/lib/Target/LoongArch/LoongArchTargetStreamer.h
@@ -0,0 +1,149 @@
+//===-- LoongArchTargetStreamer.h - LoongArch Target Streamer ------------*- C++ -*--===//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef LLVM_LIB_TARGET_LOONGARCH_LOONGARCHTARGETSTREAMER_H
+#define LLVM_LIB_TARGET_LOONGARCH_LOONGARCHTARGETSTREAMER_H
+
+#include "MCTargetDesc/LoongArchFPABIInfo.h"
+#include "MCTargetDesc/LoongArchABIInfo.h"
+#include "llvm/ADT/Optional.h"
+#include "llvm/ADT/STLExtras.h"
+#include "llvm/MC/MCELFStreamer.h"
+#include "llvm/MC/MCRegisterInfo.h"
+#include "llvm/MC/MCStreamer.h"
+
+namespace llvm {
+
+class formatted_raw_ostream;
+
+struct LoongArchFPABIInfo;
+
+class LoongArchTargetStreamer : public MCTargetStreamer {
+public:
+  LoongArchTargetStreamer(MCStreamer &S);
+
+  virtual void setPic(bool Value) {}
+
+  virtual void emitDirectiveOptionPic0();
+  virtual void emitDirectiveOptionPic2();
+
+  virtual void emitDirectiveSetArch(StringRef Arch);
+  virtual void emitDirectiveSetLoongArch32();
+  virtual void emitDirectiveSetloongarch64();
+  virtual void emitDirectiveSetSoftFloat();
+  virtual void emitDirectiveSetHardFloat();
+
+  virtual void emitDirectiveModuleFP();
+  virtual void emitDirectiveModuleSoftFloat();
+  virtual void emitDirectiveModuleHardFloat();
+  virtual void emitDirectiveSetFp(LoongArchFPABIInfo::FpABIKind Value);
+
+  void emitR(unsigned Opcode, unsigned Reg0, SMLoc IDLoc,
+             const MCSubtargetInfo *STI);
+  void emitII(unsigned Opcode, int16_t Imm1, int16_t Imm2, SMLoc IDLoc,
+              const MCSubtargetInfo *STI);
+  void emitRX(unsigned Opcode, unsigned Reg0, MCOperand Op1, SMLoc IDLoc,
+              const MCSubtargetInfo *STI);
+  void emitRI(unsigned Opcode, unsigned Reg0, int32_t Imm, SMLoc IDLoc,
+              const MCSubtargetInfo *STI);
+  void emitRR(unsigned Opcode, unsigned Reg0, unsigned Reg1, SMLoc IDLoc,
+              const MCSubtargetInfo *STI);
+  void emitRXX(unsigned Opcode, unsigned Reg0, MCOperand Op1, MCOperand Op2,
+               SMLoc IDLoc, const MCSubtargetInfo *STI);
+  void emitRRX(unsigned Opcode, unsigned Reg0, unsigned Reg1, MCOperand Op2,
+               SMLoc IDLoc, const MCSubtargetInfo *STI);
+  void emitRRR(unsigned Opcode, unsigned Reg0, unsigned Reg1, unsigned Reg2,
+               SMLoc IDLoc, const MCSubtargetInfo *STI);
+  void emitRRI(unsigned Opcode, unsigned Reg0, unsigned Reg1, int16_t Imm,
+               SMLoc IDLoc, const MCSubtargetInfo *STI);
+  void emitRRXX(unsigned Opcode, unsigned Reg0, unsigned Reg1, MCOperand Op2,
+                MCOperand Op3, SMLoc IDLoc, const MCSubtargetInfo *STI);
+  void emitRRIII(unsigned Opcode, unsigned Reg0, unsigned Reg1, int16_t Imm0,
+                 int16_t Imm1, int16_t Imm2, SMLoc IDLoc,
+                 const MCSubtargetInfo *STI);
+  void emitAdd(unsigned DstReg, unsigned SrcReg, unsigned TrgReg, bool Is64Bit,
+                const MCSubtargetInfo *STI);
+  void emitDSLL(unsigned DstReg, unsigned SrcReg, int16_t ShiftAmount,
+                SMLoc IDLoc, const MCSubtargetInfo *STI);
+  void emitNop(SMLoc IDLoc, const MCSubtargetInfo *STI);
+
+  void forbidModuleDirective() { ModuleDirectiveAllowed = false; }
+  void reallowModuleDirective() { ModuleDirectiveAllowed = true; }
+  bool isModuleDirectiveAllowed() { return ModuleDirectiveAllowed; }
+
+  template <class PredicateLibrary>
+  void updateABIInfo(const PredicateLibrary &P) {
+    ABI = P.getABI();
+    FPABIInfo.setAllFromPredicates(P);
+  }
+
+  LoongArchFPABIInfo &getFPABIInfo() { return FPABIInfo; }
+  const LoongArchABIInfo &getABI() const {
+    assert(ABI.hasValue() && "ABI hasn't been set!");
+    return *ABI;
+  }
+
+protected:
+  llvm::Optional<LoongArchABIInfo> ABI;
+  LoongArchFPABIInfo FPABIInfo;
+
+  bool GPRInfoSet;
+
+  bool FPRInfoSet;
+
+  bool FrameInfoSet;
+  int FrameOffset;
+  unsigned FrameReg;
+  unsigned ReturnReg;
+
+private:
+  bool ModuleDirectiveAllowed;
+};
+
+// This part is for ascii assembly output
+class LoongArchTargetAsmStreamer : public LoongArchTargetStreamer {
+  formatted_raw_ostream &OS;
+
+public:
+  LoongArchTargetAsmStreamer(MCStreamer &S, formatted_raw_ostream &OS);
+
+  void emitDirectiveOptionPic0() override;
+  void emitDirectiveOptionPic2() override;
+
+  void emitDirectiveSetArch(StringRef Arch) override;
+  void emitDirectiveSetLoongArch32() override;
+  void emitDirectiveSetloongarch64() override;
+  void emitDirectiveSetSoftFloat() override;
+  void emitDirectiveSetHardFloat() override;
+
+  // FP abiflags directives
+  void emitDirectiveModuleFP() override;
+  void emitDirectiveModuleSoftFloat() override;
+  void emitDirectiveModuleHardFloat() override;
+  void emitDirectiveSetFp(LoongArchFPABIInfo::FpABIKind Value) override;
+};
+
+// This part is for ELF object output
+class LoongArchTargetELFStreamer : public LoongArchTargetStreamer {
+  const MCSubtargetInfo &STI;
+  bool Pic;
+
+public:
+  MCELFStreamer &getStreamer();
+  LoongArchTargetELFStreamer(MCStreamer &S, const MCSubtargetInfo &STI);
+
+  void setPic(bool Value) override { Pic = Value; }
+
+  void emitLabel(MCSymbol *Symbol) override;
+  void finish() override;
+
+  void emitDirectiveOptionPic0() override;
+  void emitDirectiveOptionPic2() override;
+};
+}
+#endif
diff --git a/llvm/lib/Target/LoongArch/MCTargetDesc/CMakeLists.txt b/llvm/lib/Target/LoongArch/MCTargetDesc/CMakeLists.txt
new file mode 100644
index 000000000000..5c592ddc866f
--- /dev/null
+++ b/llvm/lib/Target/LoongArch/MCTargetDesc/CMakeLists.txt
@@ -0,0 +1,13 @@
+  add_llvm_component_library(LLVMLoongArchDesc
+  LoongArchABIInfo.cpp
+  LoongArchFPABIInfo.cpp
+  LoongArchAsmBackend.cpp
+  LoongArchELFObjectWriter.cpp
+  LoongArchELFStreamer.cpp
+  LoongArchInstPrinter.cpp
+  LoongArchMCAsmInfo.cpp
+  LoongArchMCCodeEmitter.cpp
+  LoongArchMCExpr.cpp
+  LoongArchMCTargetDesc.cpp
+  LoongArchTargetStreamer.cpp
+  )
diff --git a/llvm/lib/Target/LoongArch/MCTargetDesc/LLVMBuild.txt b/llvm/lib/Target/LoongArch/MCTargetDesc/LLVMBuild.txt
new file mode 100644
index 000000000000..6f0019bbbb36
--- /dev/null
+++ b/llvm/lib/Target/LoongArch/MCTargetDesc/LLVMBuild.txt
@@ -0,0 +1,22 @@
+;===- ./lib/Target/LoongArch/MCTargetDesc/LLVMBuild.txt -------------*- Conf -*--===;
+;
+; Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+; See https://llvm.org/LICENSE.txt for license information.
+; SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+;
+;===------------------------------------------------------------------------===;
+;
+; This is an LLVMBuild description file for the components in this subdirectory.
+;
+; For more information on the LLVMBuild system, please see:
+;
+;   http://llvm.org/docs/LLVMBuild.html
+;
+;===------------------------------------------------------------------------===;
+
+[component_0]
+type = Library
+name = LoongArchDesc
+parent = LoongArch
+required_libraries = MC LoongArchInfo Support
+add_to_library_groups = LoongArch
diff --git a/llvm/lib/Target/LoongArch/MCTargetDesc/LoongArchABIInfo.cpp b/llvm/lib/Target/LoongArch/MCTargetDesc/LoongArchABIInfo.cpp
new file mode 100644
index 000000000000..133f38981954
--- /dev/null
+++ b/llvm/lib/Target/LoongArch/MCTargetDesc/LoongArchABIInfo.cpp
@@ -0,0 +1,114 @@
+//===---- LoongArchABIInfo.cpp - Information about LoongArch ABI's ------------------===//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+
+#include "LoongArchABIInfo.h"
+#include "LoongArchRegisterInfo.h"
+#include "llvm/ADT/StringRef.h"
+#include "llvm/ADT/StringSwitch.h"
+#include "llvm/MC/MCTargetOptions.h"
+
+using namespace llvm;
+
+namespace {
+static const MCPhysReg LP32IntRegs[4] = {LoongArch::A0, LoongArch::A1, LoongArch::A2, LoongArch::A3};
+
+static const MCPhysReg LoongArch64IntRegs[8] = {
+    LoongArch::A0_64, LoongArch::A1_64, LoongArch::A2_64, LoongArch::A3_64,
+    LoongArch::A4_64, LoongArch::A5_64, LoongArch::A6_64, LoongArch::A7_64};
+}
+
+ArrayRef<MCPhysReg> LoongArchABIInfo::GetByValArgRegs() const {
+  if (IsLP32())
+    return makeArrayRef(LP32IntRegs);
+  if (IsLPX32() || IsLP64D())
+    return makeArrayRef(LoongArch64IntRegs);
+  llvm_unreachable("Unhandled ABI");
+}
+
+ArrayRef<MCPhysReg> LoongArchABIInfo::GetVarArgRegs() const {
+  if (IsLP32())
+    return makeArrayRef(LP32IntRegs);
+  if (IsLPX32() || IsLP64D())
+    return makeArrayRef(LoongArch64IntRegs);
+  llvm_unreachable("Unhandled ABI");
+}
+
+unsigned LoongArchABIInfo::GetCalleeAllocdArgSizeInBytes(CallingConv::ID CC) const {
+  if (IsLP32())
+    return CC != CallingConv::Fast ? 16 : 0;
+  if (IsLPX32() || IsLP64D())
+    return 0;
+  llvm_unreachable("Unhandled ABI");
+}
+
+LoongArchABIInfo LoongArchABIInfo::computeTargetABI(const Triple &TT, StringRef CPU,
+                                          const MCTargetOptions &Options) {
+  if (Options.getABIName().startswith("lp32"))
+    return LoongArchABIInfo::LP32();
+  if (Options.getABIName().startswith("lpx32"))
+    return LoongArchABIInfo::LPX32();
+  if (Options.getABIName().startswith("lp64d"))
+    return LoongArchABIInfo::LP64D();
+  assert(Options.getABIName().empty() && "Unknown ABI option for LoongArch");
+
+  if (TT.isLoongArch64())
+    return LoongArchABIInfo::LP64D();
+  return LoongArchABIInfo::LP32();
+}
+
+unsigned LoongArchABIInfo::GetStackPtr() const {
+  return ArePtrs64bit() ? LoongArch::SP_64 : LoongArch::SP;
+}
+
+unsigned LoongArchABIInfo::GetFramePtr() const {
+  return ArePtrs64bit() ? LoongArch::FP_64 : LoongArch::FP;
+}
+
+unsigned LoongArchABIInfo::GetBasePtr() const {
+  return ArePtrs64bit() ? LoongArch::S7_64 : LoongArch::S7;
+}
+
+unsigned LoongArchABIInfo::GetNullPtr() const {
+  return ArePtrs64bit() ? LoongArch::ZERO_64 : LoongArch::ZERO;
+}
+
+unsigned LoongArchABIInfo::GetZeroReg() const {
+  return AreGprs64bit() ? LoongArch::ZERO_64 : LoongArch::ZERO;
+}
+
+unsigned LoongArchABIInfo::GetPtrAddOp() const {
+  return ArePtrs64bit() ? LoongArch::ADD_D : LoongArch::ADD_W;
+}
+
+unsigned LoongArchABIInfo::GetPtrAddiOp() const {
+  return ArePtrs64bit() ? LoongArch::ADDI_D : LoongArch::ADDI_W;
+}
+
+unsigned LoongArchABIInfo::GetPtrSubOp() const {
+  return ArePtrs64bit() ? LoongArch::SUB_D : LoongArch::SUB_W;
+}
+
+unsigned LoongArchABIInfo::GetPtrAndOp() const {
+  return ArePtrs64bit() ? LoongArch::AND : LoongArch::AND32;
+}
+
+unsigned LoongArchABIInfo::GetGPRMoveOp() const {
+  return ArePtrs64bit() ? LoongArch::OR : LoongArch::OR32;
+}
+
+unsigned LoongArchABIInfo::GetEhDataReg(unsigned I) const {
+  static const unsigned EhDataReg[] = {
+    LoongArch::A0, LoongArch::A1, LoongArch::A2, LoongArch::A3
+  };
+  static const unsigned EhDataReg64[] = {
+    LoongArch::A0_64, LoongArch::A1_64, LoongArch::A2_64, LoongArch::A3_64
+  };
+
+  return IsLP64D() ? EhDataReg64[I] : EhDataReg[I];
+}
+
diff --git a/llvm/lib/Target/LoongArch/MCTargetDesc/LoongArchABIInfo.h b/llvm/lib/Target/LoongArch/MCTargetDesc/LoongArchABIInfo.h
new file mode 100644
index 000000000000..b3fea26a2957
--- /dev/null
+++ b/llvm/lib/Target/LoongArch/MCTargetDesc/LoongArchABIInfo.h
@@ -0,0 +1,80 @@
+//===---- LoongArchABIInfo.h - Information about LoongArch ABI's --------------------===//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef LLVM_LIB_TARGET_LOONGARCH_MCTARGETDESC_LOONGARCHABIINFO_H
+#define LLVM_LIB_TARGET_LOONGARCH_MCTARGETDESC_LOONGARCHABIINFO_H
+
+#include "llvm/ADT/Triple.h"
+#include "llvm/IR/CallingConv.h"
+#include "llvm/MC/MCRegisterInfo.h"
+
+namespace llvm {
+
+template <typename T> class ArrayRef;
+class MCTargetOptions;
+class StringRef;
+class TargetRegisterClass;
+
+class LoongArchABIInfo {
+public:
+  enum class ABI { Unknown, LP32, LPX32, LP64D };
+
+protected:
+  ABI ThisABI;
+
+public:
+  LoongArchABIInfo(ABI ThisABI) : ThisABI(ThisABI) {}
+
+  static LoongArchABIInfo Unknown() { return LoongArchABIInfo(ABI::Unknown); }
+  static LoongArchABIInfo LP32() { return LoongArchABIInfo(ABI::LP32); }
+  static LoongArchABIInfo LPX32() { return LoongArchABIInfo(ABI::LPX32); }
+  static LoongArchABIInfo LP64D() { return LoongArchABIInfo(ABI::LP64D); }
+  static LoongArchABIInfo computeTargetABI(const Triple &TT, StringRef CPU,
+                                      const MCTargetOptions &Options);
+
+  bool IsKnown() const { return ThisABI != ABI::Unknown; }
+  bool IsLP32() const { return ThisABI == ABI::LP32; }
+  bool IsLPX32() const { return ThisABI == ABI::LPX32; }
+  bool IsLP64D() const { return ThisABI == ABI::LP64D; }
+  ABI GetEnumValue() const { return ThisABI; }
+
+  /// The registers to use for byval arguments.
+  ArrayRef<MCPhysReg> GetByValArgRegs() const;
+
+  /// The registers to use for the variable argument list.
+  ArrayRef<MCPhysReg> GetVarArgRegs() const;
+
+  /// Obtain the size of the area allocated by the callee for arguments.
+  /// CallingConv::FastCall affects the value for LP32.
+  unsigned GetCalleeAllocdArgSizeInBytes(CallingConv::ID CC) const;
+
+  /// Ordering of ABI's
+  /// LoongArchGenSubtargetInfo.inc will use this to resolve conflicts when given
+  /// multiple ABI options.
+  bool operator<(const LoongArchABIInfo Other) const {
+    return ThisABI < Other.GetEnumValue();
+  }
+
+  unsigned GetStackPtr() const;
+  unsigned GetFramePtr() const;
+  unsigned GetBasePtr() const;
+  unsigned GetNullPtr() const;
+  unsigned GetZeroReg() const;
+  unsigned GetPtrAddOp() const;
+  unsigned GetPtrAddiOp() const;
+  unsigned GetPtrSubOp() const;
+  unsigned GetPtrAndOp() const;
+  unsigned GetGPRMoveOp() const;
+  inline bool ArePtrs64bit() const { return IsLP64D(); }
+  inline bool AreGprs64bit() const { return IsLPX32() || IsLP64D(); }
+
+  unsigned GetEhDataReg(unsigned I) const;
+};
+}
+
+#endif
diff --git a/llvm/lib/Target/LoongArch/MCTargetDesc/LoongArchAsmBackend.cpp b/llvm/lib/Target/LoongArch/MCTargetDesc/LoongArchAsmBackend.cpp
new file mode 100644
index 000000000000..b1d9894e15b7
--- /dev/null
+++ b/llvm/lib/Target/LoongArch/MCTargetDesc/LoongArchAsmBackend.cpp
@@ -0,0 +1,233 @@
+//===-- LoongArchAsmBackend.cpp - LoongArch Asm Backend  ----------------------------===//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+//
+// This file implements the LoongArchAsmBackend class.
+//
+//===----------------------------------------------------------------------===//
+//
+
+#include "MCTargetDesc/LoongArchAsmBackend.h"
+#include "MCTargetDesc/LoongArchABIInfo.h"
+#include "MCTargetDesc/LoongArchFixupKinds.h"
+#include "MCTargetDesc/LoongArchMCExpr.h"
+#include "MCTargetDesc/LoongArchMCTargetDesc.h"
+#include "llvm/ADT/STLExtras.h"
+#include "llvm/MC/MCAsmBackend.h"
+#include "llvm/MC/MCAssembler.h"
+#include "llvm/MC/MCContext.h"
+#include "llvm/MC/MCDirectives.h"
+#include "llvm/MC/MCELFObjectWriter.h"
+#include "llvm/MC/MCFixupKindInfo.h"
+#include "llvm/MC/MCObjectWriter.h"
+#include "llvm/MC/MCSubtargetInfo.h"
+#include "llvm/MC/MCTargetOptions.h"
+#include "llvm/MC/MCValue.h"
+#include "llvm/Support/ErrorHandling.h"
+#include "llvm/Support/Format.h"
+#include "llvm/Support/MathExtras.h"
+#include "llvm/Support/raw_ostream.h"
+
+using namespace llvm;
+
+// Prepare value for the target space for it
+static unsigned adjustFixupValue(const MCFixup &Fixup, uint64_t Value,
+                                 MCContext &Ctx) {
+
+  unsigned Kind = Fixup.getKind();
+
+  // TODO: reloc
+  switch (Kind) {
+  default:
+    return 0;
+  case FK_Data_2:
+    Value &= 0xffff;
+    break;
+  case FK_Data_4:
+  case FK_Data_8:
+    break;
+  }
+
+  return Value;
+}
+
+std::unique_ptr<MCObjectTargetWriter>
+LoongArchAsmBackend::createObjectTargetWriter() const {
+  return createLoongArchELFObjectWriter(TheTriple, IsLPX32);
+}
+
+/// ApplyFixup - Apply the \p Value for given \p Fixup into the provided
+/// data fragment, at the offset specified by the fixup and following the
+/// fixup kind as appropriate.
+void LoongArchAsmBackend::applyFixup(const MCAssembler &Asm, const MCFixup &Fixup,
+                                const MCValue &Target,
+                                MutableArrayRef<char> Data, uint64_t Value,
+                                bool IsResolved,
+                                const MCSubtargetInfo *STI) const {
+  MCFixupKind Kind = Fixup.getKind();
+  MCContext &Ctx = Asm.getContext();
+  Value = adjustFixupValue(Fixup, Value, Ctx);
+
+  if (!Value)
+    return; // Doesn't change encoding.
+
+  // Where do we start in the object
+  unsigned Offset = Fixup.getOffset();
+  // Number of bytes we need to fixup
+  unsigned NumBytes = (getFixupKindInfo(Kind).TargetSize + 7) / 8;
+
+
+  // Grab current value, if any, from bits.
+  uint64_t CurVal = 0;
+
+  for (unsigned i = 0; i != NumBytes; ++i)
+    CurVal |= (uint64_t)((uint8_t)Data[Offset + i]) << (i*8);
+
+  uint64_t Mask = ((uint64_t)(-1) >>
+                    (64 - getFixupKindInfo(Kind).TargetSize));
+  CurVal |= Value & Mask;
+
+  // Write out the fixed up bytes back to the code/data bits.
+  for (unsigned i = 0; i != NumBytes; ++i)
+    Data[Offset + i] = (uint8_t)((CurVal >> (i*8)) & 0xff);
+}
+
+Optional<MCFixupKind> LoongArchAsmBackend::getFixupKind(StringRef Name) const {
+  return StringSwitch<Optional<MCFixupKind>>(Name)
+      .Case("R_LARCH_NONE", (MCFixupKind)LoongArch::fixup_LARCH_NONE)
+      .Case("R_LARCH_32", FK_Data_4)
+      .Case("R_LARCH_64", FK_Data_8)
+      .Default(MCAsmBackend::getFixupKind(Name));
+}
+
+const MCFixupKindInfo &LoongArchAsmBackend::
+getFixupKindInfo(MCFixupKind Kind) const {
+  const static MCFixupKindInfo Infos[] = {
+    // This table *must* be in same the order of fixup_* kinds in
+    // LoongArchFixupKinds.h.
+    //
+    // name                    offset  bits  flags
+    { "fixup_LARCH_NONE",         0,      0,   0 },
+    { "fixup_LARCH_SOP_PUSH_ABSOLUTE",	0,	0,	0},
+    { "fixup_LARCH_SOP_PUSH_PCREL",	0,	0,	0},
+    { "fixup_LARCH_SOP_PUSH_GPREL",	0,	0,	0},
+    { "fixup_LARCH_SOP_PUSH_TLS_TPREL",	0,	0,	0},
+    { "fixup_LARCH_SOP_PUSH_TLS_GOT",	0,	0,	0},
+    { "fixup_LARCH_SOP_PUSH_TLS_GD",	0,	0,	0},
+    { "fixup_LARCH_SOP_PUSH_PLT_PCREL",	0,	0,	0},
+    { "fixup_LARCH_32",	0,	0,	0},
+    { "fixup_LARCH_64",	0,	0,	0},
+    { "fixup_LARCH_RELATIVE",	0,	0,	0},
+    { "fixup_LARCH_COPY",	0,	0,	0},
+    { "fixup_LARCH_JUMP_SLOT",	0,	0,	0},
+    { "fixup_LARCH_TLS_DTPMOD32",	0,	0,	0},
+    { "fixup_LARCH_TLS_DTPMOD64",	0,	0,	0},
+    { "fixup_LARCH_TLS_DTPREL32",	0,	0,	0},
+    { "fixup_LARCH_TLS_DTPREL64",	0,	0,	0},
+    { "fixup_LARCH_TLS_TPREL32",	0,	0,	0},
+    { "fixup_LARCH_TLS_TPREL64",	0,	0,	0},
+    { "fixup_LARCH_IRELATIVE",	0,	0,	0},
+    { "fixup_LARCH_MARK_LA",	0,	0,	0},
+    { "fixup_LARCH_MARK_PCREL",	0,	0,	0},
+    { "fixup_LARCH_SOP_PUSH_DUP",	0,	0,	0},
+    { "fixup_LARCH_SOP_ASSERT",	0,	0,	0},
+    { "fixup_LARCH_SOP_NOT",	0,	0,	0},
+    { "fixup_LARCH_SOP_SUB",	0,	0,	0},
+    { "fixup_LARCH_SOP_SL",	0,	0,	0},
+    { "fixup_LARCH_SOP_SR",	0,	0,	0},
+    { "fixup_LARCH_SOP_ADD",	0,	0,	0},
+    { "fixup_LARCH_SOP_AND",	0,	0,	0},
+    { "fixup_LARCH_SOP_IF_ELSE",	0,	0,	0},
+    { "fixup_LARCH_SOP_POP_32_S_10_5",	0,	0,	0},
+    { "fixup_LARCH_SOP_POP_32_U_10_12",	0,	0,	0},
+    { "fixup_LARCH_SOP_POP_32_S_10_12",	0,	0,	0},
+    { "fixup_LARCH_SOP_POP_32_S_10_16",	0,	0,	0},
+    { "fixup_LARCH_SOP_POP_32_S_10_16_S2",	0,	0,	0},
+    { "fixup_LARCH_SOP_POP_32_S_5_20",	0,	0,	0},
+    { "fixup_LARCH_SOP_POP_32_S_0_5_10_16_S2",	0,	0,	0},
+    { "fixup_LARCH_SOP_POP_32_S_0_10_10_16_S2",	0,	0,	0},
+    { "fixup_LARCH_SOP_POP_32_U",	0,	0,	0},
+    { "fixup_LARCH_ADD8",	0,	0,	0},
+    { "fixup_LARCH_ADD16",	0,	0,	0},
+    { "fixup_LARCH_ADD24",	0,	0,	0},
+    { "fixup_LARCH_ADD32",	0,	0,	0},
+    { "fixup_LARCH_ADD64",	0,	0,	0},
+    { "fixup_LARCH_SUB8",	0,	0,	0},
+    { "fixup_LARCH_SUB16",	0,	0,	0},
+    { "fixup_LARCH_SUB24",	0,	0,	0},
+    { "fixup_LARCH_SUB32",	0,	0,	0},
+    { "fixup_LARCH_SUB64",	0,	0,	0},
+  };
+
+  if (Kind < FirstTargetFixupKind)
+    return MCAsmBackend::getFixupKindInfo(Kind);
+
+  assert(unsigned(Kind - FirstTargetFixupKind) < getNumFixupKinds() &&
+          "Invalid kind!");
+
+  return Infos[Kind - FirstTargetFixupKind];
+}
+
+/// WriteNopData - Write an (optimal) nop sequence of Count bytes
+/// to the given output. If the target cannot generate such a sequence,
+/// it should return an error.
+///
+/// \return - True on success.
+bool LoongArchAsmBackend::writeNopData(raw_ostream &OS, uint64_t Count) const {
+  // Check for a less than instruction size number of bytes
+  if ((Count % 4) != 0)
+    return false;
+
+  // The nop on LoongArch is andi r0, r0, 0.
+  for (; Count >= 4; Count -= 4)
+    OS.write("\x03\x40\0\0", 4);
+
+  return true;
+}
+
+bool LoongArchAsmBackend::shouldForceRelocation(const MCAssembler &Asm,
+                                           const MCFixup &Fixup,
+                                           const MCValue &Target) {
+  const unsigned FixupKind = Fixup.getKind();
+  switch (FixupKind) {
+  default:
+    return false;
+  // All these relocations require special processing
+  // at linking time. Delegate this work to a linker.
+  case LoongArch::fixup_LARCH_SOP_PUSH_PLT_PCREL:
+  case LoongArch::fixup_LARCH_SOP_PUSH_PCREL:
+  case LoongArch::fixup_LARCH_SOP_PUSH_GPREL:
+  case LoongArch::fixup_LARCH_SOP_PUSH_TLS_GD:
+  case LoongArch::fixup_LARCH_SOP_PUSH_TLS_GOT:
+  case LoongArch::fixup_LARCH_SOP_PUSH_TLS_TPREL:
+  case LoongArch::fixup_LARCH_SOP_PUSH_ABSOLUTE:
+  case LoongArch::fixup_LARCH_SOP_IF_ELSE:
+  case LoongArch::fixup_LARCH_SOP_ADD:
+  case LoongArch::fixup_LARCH_SOP_SUB:
+  case LoongArch::fixup_LARCH_SOP_AND:
+  case LoongArch::fixup_LARCH_SOP_SL:
+  case LoongArch::fixup_LARCH_SOP_SR:
+  case LoongArch::fixup_LARCH_SOP_POP_32_S_10_5:
+  case LoongArch::fixup_LARCH_SOP_POP_32_S_5_20:
+  case LoongArch::fixup_LARCH_SOP_POP_32_S_10_12:
+  case LoongArch::fixup_LARCH_SOP_POP_32_U_10_12:
+  case LoongArch::fixup_LARCH_SOP_POP_32_S_10_16_S2:
+  case LoongArch::fixup_LARCH_SOP_POP_32_S_0_5_10_16_S2:
+  case LoongArch::fixup_LARCH_SOP_POP_32_S_0_10_10_16_S2:
+    return true;
+  }
+}
+
+MCAsmBackend *llvm::createLoongArchAsmBackend(const Target &T,
+                                              const MCSubtargetInfo &STI,
+                                              const MCRegisterInfo &MRI,
+                                              const MCTargetOptions &Options) {
+  LoongArchABIInfo ABI = LoongArchABIInfo::computeTargetABI(
+                           STI.getTargetTriple(), STI.getCPU(), Options);
+  return new LoongArchAsmBackend(T, MRI, STI.getTargetTriple(), STI.getCPU(),
+                                 ABI.IsLPX32());
+}
diff --git a/llvm/lib/Target/LoongArch/MCTargetDesc/LoongArchAsmBackend.h b/llvm/lib/Target/LoongArch/MCTargetDesc/LoongArchAsmBackend.h
new file mode 100644
index 000000000000..575dd915e9dd
--- /dev/null
+++ b/llvm/lib/Target/LoongArch/MCTargetDesc/LoongArchAsmBackend.h
@@ -0,0 +1,90 @@
+//===-- LoongArchAsmBackend.h - LoongArch Asm Backend  ------------------------------===//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+//
+// This file defines the LoongArchAsmBackend class.
+//
+//===----------------------------------------------------------------------===//
+//
+
+#ifndef LLVM_LIB_TARGET_LOONGARCH_MCTARGETDESC_LOONGARCHASMBACKEND_H
+#define LLVM_LIB_TARGET_LOONGARCH_MCTARGETDESC_LOONGARCHASMBACKEND_H
+
+#include "MCTargetDesc/LoongArchFixupKinds.h"
+#include "llvm/ADT/Triple.h"
+#include "llvm/MC/MCAsmBackend.h"
+
+namespace llvm {
+
+class MCAssembler;
+struct MCFixupKindInfo;
+class MCObjectWriter;
+class MCRegisterInfo;
+class MCSymbolELF;
+class Target;
+
+class LoongArchAsmBackend : public MCAsmBackend {
+  Triple TheTriple;
+  bool IsLPX32;
+
+public:
+  LoongArchAsmBackend(const Target &T, const MCRegisterInfo &MRI, const Triple &TT,
+                 StringRef CPU, bool LPX32)
+      : MCAsmBackend(support::little),
+        TheTriple(TT), IsLPX32(LPX32) {
+    assert(TT.isLittleEndian());
+  }
+
+  std::unique_ptr<MCObjectTargetWriter>
+  createObjectTargetWriter() const override;
+
+  void applyFixup(const MCAssembler &Asm, const MCFixup &Fixup,
+                  const MCValue &Target, MutableArrayRef<char> Data,
+                  uint64_t Value, bool IsResolved,
+                  const MCSubtargetInfo *STI) const override;
+
+  Optional<MCFixupKind> getFixupKind(StringRef Name) const override;
+  const MCFixupKindInfo &getFixupKindInfo(MCFixupKind Kind) const override;
+
+  unsigned getNumFixupKinds() const override {
+    return LoongArch::NumTargetFixupKinds;
+  }
+
+  /// @name Target Relaxation Interfaces
+  /// @{
+
+  /// MayNeedRelaxation - Check whether the given instruction may need
+  /// relaxation.
+  ///
+  /// \param Inst - The instruction to test.
+  bool mayNeedRelaxation(const MCInst &Inst,
+                         const MCSubtargetInfo &STI) const override {
+    return false;
+  }
+
+  /// fixupNeedsRelaxation - Target specific predicate for whether a given
+  /// fixup requires the associated instruction to be relaxed.
+   bool fixupNeedsRelaxation(const MCFixup &Fixup, uint64_t Value,
+                             const MCRelaxableFragment *DF,
+                             const MCAsmLayout &Layout) const override {
+    // FIXME.
+    llvm_unreachable("RelaxInstruction() unimplemented");
+    return false;
+  }
+
+  /// @}
+
+  bool writeNopData(raw_ostream &OS, uint64_t Count) const override;
+
+  bool shouldForceRelocation(const MCAssembler &Asm, const MCFixup &Fixup,
+                             const MCValue &Target) override;
+
+}; // class LoongArchAsmBackend
+
+} // namespace
+
+#endif
diff --git a/llvm/lib/Target/LoongArch/MCTargetDesc/LoongArchBaseInfo.h b/llvm/lib/Target/LoongArch/MCTargetDesc/LoongArchBaseInfo.h
new file mode 100644
index 000000000000..707333c18fbd
--- /dev/null
+++ b/llvm/lib/Target/LoongArch/MCTargetDesc/LoongArchBaseInfo.h
@@ -0,0 +1,128 @@
+//===-- LoongArchBaseInfo.h - Top level definitions for LoongArch MC ------*- C++ -*-===//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+//
+// This file contains small standalone helper functions and enum definitions for
+// the LoongArch target useful for the compiler back-end and the MC libraries.
+//
+//===----------------------------------------------------------------------===//
+#ifndef LLVM_LIB_TARGET_LOONGARCH_MCTARGETDESC_LOONGARCHBASEINFO_H
+#define LLVM_LIB_TARGET_LOONGARCH_MCTARGETDESC_LOONGARCHBASEINFO_H
+
+#include "LoongArchFixupKinds.h"
+#include "LoongArchMCTargetDesc.h"
+#include "llvm/MC/MCExpr.h"
+#include "llvm/Support/DataTypes.h"
+#include "llvm/Support/ErrorHandling.h"
+
+namespace llvm {
+
+/// LoongArchII - This namespace holds all of the target specific flags that
+/// instruction info tracks.
+///
+namespace LoongArchII {
+  /// Target Operand Flag enum.
+  enum TOF {
+    //===------------------------------------------------------------------===//
+    // LoongArch Specific MachineOperand flags.
+
+    MO_NO_FLAG,
+
+    /// MO_ABS_XXX - Represents the hi or low part of an absolute symbol
+    /// address.
+    MO_ABS_HI,
+    MO_ABS_LO,
+    MO_ABS_HIGHER,
+    MO_ABS_HIGHEST,
+
+    /// MO_PCREL_XXX - Represents the hi or low part of an pc relative symbol
+    /// address.
+    MO_PCREL_HI,
+    MO_PCREL_LO,
+    // with tmp reg
+    MO_PCREL_RRHI,
+    MO_PCREL_RRLO,
+    MO_PCREL_RRHIGHER,
+    MO_PCREL_RRHIGHEST,
+
+    // LArch Tls gd and ld
+    MO_TLSGD_HI,
+    MO_TLSGD_LO,
+    // with tmp reg
+    MO_TLSGD_RRHI,
+    MO_TLSGD_RRLO,
+    MO_TLSGD_RRHIGHER,
+    MO_TLSGD_RRHIGHEST,
+
+    // LArch thread tprel (ie/le)
+    // LArch Tls ie
+    MO_TLSIE_HI,
+    MO_TLSIE_LO,
+    // with tmp reg
+    MO_TLSIE_RRHI,
+    MO_TLSIE_RRLO,
+    MO_TLSIE_RRHIGHER,
+    MO_TLSIE_RRHIGHEST,
+    // LArch Tls le
+    MO_TLSLE_HI,
+    MO_TLSLE_LO,
+    MO_TLSLE_HIGHER,
+    MO_TLSLE_HIGHEST,
+
+    // Loongarch got
+    MO_GOT_HI,
+    MO_GOT_LO,
+    // with tmp reg
+    MO_GOT_RRHI,
+    MO_GOT_RRLO,
+    MO_GOT_RRHIGHER,
+    MO_GOT_RRHIGHEST,
+
+    MO_CALL_HI,
+    MO_CALL_LO,
+  };
+
+  enum {
+    //===------------------------------------------------------------------===//
+    // Instruction encodings.  These are the standard/most common forms for
+    // LoongArch instructions.
+    //
+
+    // Pseudo - This represents an instruction that is a pseudo instruction
+    // or one that has not been implemented yet.  It is illegal to code generate
+    // it, but tolerated for intermediate implementation stages.
+    Pseudo   = 0,
+
+    /// FrmR - This form is for instructions of the format R.
+    FrmR  = 1,
+    /// FrmI - This form is for instructions of the format I.
+    FrmI  = 2,
+    /// FrmJ - This form is for instructions of the format J.
+    FrmJ  = 3,
+    /// FrmFR - This form is for instructions of the format FR.
+    FrmFR = 4,
+    /// FrmFI - This form is for instructions of the format FI.
+    FrmFI = 5,
+    /// FrmOther - This form is for instructions that have no specific format.
+    FrmOther = 6,
+
+    FormMask = 15,
+    /// IsCTI - Instruction is a Control Transfer Instruction.
+    IsCTI = 1 << 4,
+    /// HasForbiddenSlot - Instruction has a forbidden slot.
+    HasForbiddenSlot = 1 << 5,
+    /// IsPCRelativeLoad - A Load instruction with implicit source register
+    ///                    ($pc) with explicit offset and destination register
+    IsPCRelativeLoad = 1 << 6,
+    /// HasFCCRegOperand - Instruction uses an $fcc<x> register.
+    HasFCCRegOperand = 1 << 7
+
+  };
+}
+}
+
+#endif
diff --git a/llvm/lib/Target/LoongArch/MCTargetDesc/LoongArchELFObjectWriter.cpp b/llvm/lib/Target/LoongArch/MCTargetDesc/LoongArchELFObjectWriter.cpp
new file mode 100644
index 000000000000..e00b9af9d49e
--- /dev/null
+++ b/llvm/lib/Target/LoongArch/MCTargetDesc/LoongArchELFObjectWriter.cpp
@@ -0,0 +1,186 @@
+//===-- LoongArchELFObjectWriter.cpp - LoongArch ELF Writer -------------------------===//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+
+#include "MCTargetDesc/LoongArchFixupKinds.h"
+#include "MCTargetDesc/LoongArchMCTargetDesc.h"
+#include "llvm/ADT/STLExtras.h"
+#include "llvm/BinaryFormat/ELF.h"
+#include "llvm/MC/MCContext.h"
+#include "llvm/MC/MCELFObjectWriter.h"
+#include "llvm/MC/MCFixup.h"
+#include "llvm/MC/MCObjectWriter.h"
+#include "llvm/MC/MCSymbolELF.h"
+#include "llvm/Support/Casting.h"
+#include "llvm/Support/Compiler.h"
+#include "llvm/Support/Debug.h"
+#include "llvm/Support/ErrorHandling.h"
+#include "llvm/Support/MathExtras.h"
+#include "llvm/Support/raw_ostream.h"
+#include <algorithm>
+#include <cassert>
+#include <cstdint>
+#include <iterator>
+#include <list>
+#include <utility>
+
+#define DEBUG_TYPE "loongarch-elf-object-writer"
+
+using namespace llvm;
+
+namespace {
+
+class LoongArchELFObjectWriter : public MCELFObjectTargetWriter {
+public:
+  LoongArchELFObjectWriter(uint8_t OSABI, bool HasRelocationAddend, bool Is64);
+
+  ~LoongArchELFObjectWriter() override = default;
+
+  unsigned getRelocType(MCContext &Ctx, const MCValue &Target,
+                        const MCFixup &Fixup, bool IsPCRel) const override;
+  bool needsRelocateWithSymbol(const MCSymbol &Sym,
+                               unsigned Type) const override {
+    return true;
+  }
+};
+
+} // end anonymous namespace
+
+LoongArchELFObjectWriter::LoongArchELFObjectWriter(uint8_t OSABI,
+                                         bool HasRelocationAddend, bool Is64)
+    : MCELFObjectTargetWriter(Is64, OSABI, ELF::EM_LOONGARCH, HasRelocationAddend) {}
+
+unsigned LoongArchELFObjectWriter::getRelocType(MCContext &Ctx,
+                                           const MCValue &Target,
+                                           const MCFixup &Fixup,
+                                           bool IsPCRel) const {
+  // Determine the type of the relocation.
+  ///XXX:Reloc
+  unsigned Kind = (unsigned)Fixup.getKind();
+
+  switch (Kind) {
+    default:
+      return ELF::R_LARCH_NONE;
+      //llvm_unreachable("invalid fixup kind!");
+    case FK_Data_4:
+    case LoongArch::fixup_LARCH_32:
+      return ELF::R_LARCH_32;
+    case FK_GPRel_4:
+    case FK_Data_8:
+    case LoongArch::fixup_LARCH_64:
+      return ELF::R_LARCH_64;
+    case LoongArch::fixup_LARCH_NONE:
+      return ELF::R_LARCH_NONE;
+    case LoongArch::fixup_LARCH_RELATIVE:
+      return ELF::R_LARCH_RELATIVE;
+    case LoongArch::fixup_LARCH_COPY:
+      return ELF::R_LARCH_COPY;
+    case LoongArch::fixup_LARCH_JUMP_SLOT:
+      return ELF::R_LARCH_JUMP_SLOT;
+    case LoongArch::fixup_LARCH_TLS_DTPMOD32:
+      return ELF::R_LARCH_TLS_DTPMOD32;
+    case LoongArch::fixup_LARCH_TLS_DTPMOD64:
+      return ELF::R_LARCH_TLS_DTPMOD64;
+    case LoongArch::fixup_LARCH_TLS_DTPREL32:
+      return ELF::R_LARCH_TLS_DTPREL32;
+    case LoongArch::fixup_LARCH_TLS_DTPREL64:
+      return ELF::R_LARCH_TLS_DTPREL64;
+    case LoongArch::fixup_LARCH_TLS_TPREL32:
+      return ELF::R_LARCH_TLS_TPREL32;
+    case LoongArch::fixup_LARCH_TLS_TPREL64:
+      return ELF::R_LARCH_TLS_TPREL64;
+    case LoongArch::fixup_LARCH_IRELATIVE:
+      return ELF::R_LARCH_IRELATIVE;
+    case LoongArch::fixup_LARCH_MARK_LA:
+      return ELF::R_LARCH_MARK_LA;
+    case LoongArch::fixup_LARCH_MARK_PCREL:
+      return ELF::R_LARCH_MARK_PCREL;
+    case LoongArch::fixup_LARCH_SOP_PUSH_PCREL:
+      return ELF::R_LARCH_SOP_PUSH_PCREL;
+    case LoongArch::fixup_LARCH_SOP_PUSH_ABSOLUTE:
+      return ELF::R_LARCH_SOP_PUSH_ABSOLUTE;
+    case LoongArch::fixup_LARCH_SOP_PUSH_DUP:
+      return ELF::R_LARCH_SOP_PUSH_DUP;
+    case LoongArch::fixup_LARCH_SOP_PUSH_GPREL:
+      return ELF::R_LARCH_SOP_PUSH_GPREL;
+    case LoongArch::fixup_LARCH_SOP_PUSH_TLS_TPREL:
+      return ELF::R_LARCH_SOP_PUSH_TLS_TPREL;
+    case LoongArch::fixup_LARCH_SOP_PUSH_TLS_GOT:
+      return ELF::R_LARCH_SOP_PUSH_TLS_GOT;
+    case LoongArch::fixup_LARCH_SOP_PUSH_TLS_GD:
+      return ELF::R_LARCH_SOP_PUSH_TLS_GD;
+    case LoongArch::fixup_LARCH_SOP_PUSH_PLT_PCREL:
+      return ELF::R_LARCH_SOP_PUSH_PLT_PCREL;
+    case LoongArch::fixup_LARCH_SOP_ASSERT:
+      return ELF::R_LARCH_SOP_ASSERT;
+    case LoongArch::fixup_LARCH_SOP_NOT:
+      return ELF::R_LARCH_SOP_NOT;
+    case LoongArch::fixup_LARCH_SOP_SUB:
+      return ELF::R_LARCH_SOP_SUB;
+    case LoongArch::fixup_LARCH_SOP_SL:
+      return ELF::R_LARCH_SOP_SL;
+    case LoongArch::fixup_LARCH_SOP_SR:
+      return ELF::R_LARCH_SOP_SR;
+    case LoongArch::fixup_LARCH_SOP_ADD:
+      return ELF::R_LARCH_SOP_ADD;
+    case LoongArch::fixup_LARCH_SOP_AND:
+      return ELF::R_LARCH_SOP_AND;
+    case LoongArch::fixup_LARCH_SOP_IF_ELSE:
+      return ELF::R_LARCH_SOP_IF_ELSE;
+    case LoongArch::fixup_LARCH_SOP_POP_32_S_10_5:
+      return ELF::R_LARCH_SOP_POP_32_S_10_5;
+    case LoongArch::fixup_LARCH_SOP_POP_32_U_10_12:
+      return ELF::R_LARCH_SOP_POP_32_U_10_12;
+    case LoongArch::fixup_LARCH_SOP_POP_32_S_10_12:
+      return ELF::R_LARCH_SOP_POP_32_S_10_12;
+    case LoongArch::fixup_LARCH_SOP_POP_32_S_10_16:
+      return ELF::R_LARCH_SOP_POP_32_S_10_16;
+    case LoongArch::fixup_LARCH_SOP_POP_32_S_10_16_S2:
+      return ELF::R_LARCH_SOP_POP_32_S_10_16_S2;
+    case LoongArch::fixup_LARCH_SOP_POP_32_S_5_20:
+      return ELF::R_LARCH_SOP_POP_32_S_5_20;
+    case LoongArch::fixup_LARCH_SOP_POP_32_S_0_5_10_16_S2:
+      return ELF::R_LARCH_SOP_POP_32_S_0_5_10_16_S2;
+    case LoongArch::fixup_LARCH_SOP_POP_32_S_0_10_10_16_S2:
+      return ELF::R_LARCH_SOP_POP_32_S_0_10_10_16_S2;
+    case LoongArch::fixup_LARCH_SOP_POP_32_U:
+      return ELF::R_LARCH_SOP_POP_32_U;
+    case LoongArch::fixup_LARCH_ADD8:
+      return ELF::R_LARCH_ADD8;
+    case LoongArch::fixup_LARCH_ADD16:
+      return ELF::R_LARCH_ADD16;
+    case LoongArch::fixup_LARCH_ADD24:
+      return ELF::R_LARCH_ADD24;
+    case LoongArch::fixup_LARCH_ADD32:
+      return ELF::R_LARCH_ADD32;
+    case LoongArch::fixup_LARCH_ADD64:
+      return ELF::R_LARCH_ADD64;
+    case LoongArch::fixup_LARCH_SUB8:
+      return ELF::R_LARCH_SUB8;
+    case LoongArch::fixup_LARCH_SUB16:
+      return ELF::R_LARCH_SUB16;
+    case LoongArch::fixup_LARCH_SUB24:
+      return ELF::R_LARCH_SUB24;
+    case LoongArch::fixup_LARCH_SUB32:
+      return ELF::R_LARCH_SUB32;
+    case LoongArch::fixup_LARCH_SUB64:
+      return ELF::R_LARCH_SUB64;
+    case LoongArch::fixup_LARCH_GNU_VTINHERIT:
+      return ELF::R_LARCH_GNU_VTINHERIT;
+    case LoongArch::fixup_LARCH_GNU_VTENTRY:
+      return ELF::R_LARCH_GNU_VTENTRY;
+  }
+}
+
+std::unique_ptr<MCObjectTargetWriter>
+llvm::createLoongArchELFObjectWriter(const Triple &TT, bool IsLPX32) {
+  uint8_t OSABI = MCELFObjectTargetWriter::getOSABI(TT.getOS());
+  bool IsLP64 = TT.isArch64Bit() && !IsLPX32;
+  bool HasRelocationAddend = TT.isArch64Bit();
+  return std::make_unique<LoongArchELFObjectWriter>(OSABI, HasRelocationAddend,
+                                                IsLP64);
+}
diff --git a/llvm/lib/Target/LoongArch/MCTargetDesc/LoongArchELFStreamer.cpp b/llvm/lib/Target/LoongArch/MCTargetDesc/LoongArchELFStreamer.cpp
new file mode 100644
index 000000000000..02540332e944
--- /dev/null
+++ b/llvm/lib/Target/LoongArch/MCTargetDesc/LoongArchELFStreamer.cpp
@@ -0,0 +1,132 @@
+//===-------- LoongArchELFStreamer.cpp - ELF Object Output ---------------------===//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+
+#include "LoongArchELFStreamer.h"
+#include "LoongArchFixupKinds.h"
+#include "LoongArchTargetStreamer.h"
+#include "llvm/BinaryFormat/ELF.h"
+#include "llvm/MC/MCAsmBackend.h"
+#include "llvm/MC/MCAssembler.h"
+#include "llvm/MC/MCCodeEmitter.h"
+#include "llvm/MC/MCContext.h"
+#include "llvm/MC/MCDwarf.h"
+#include "llvm/MC/MCInst.h"
+#include "llvm/MC/MCObjectWriter.h"
+#include "llvm/MC/MCSymbolELF.h"
+#include "llvm/MC/MCValue.h"
+#include "llvm/Support/Casting.h"
+
+using namespace llvm;
+
+static std::pair<unsigned, unsigned> getRelocPairForSize(unsigned Size) {
+  switch (Size) {
+  default:
+    llvm_unreachable("unsupported fixup size");
+  case 1:
+    return std::make_pair(LoongArch::fixup_LARCH_ADD8,
+                          LoongArch::fixup_LARCH_SUB8);
+  case 2:
+    return std::make_pair(LoongArch::fixup_LARCH_ADD16,
+                          LoongArch::fixup_LARCH_SUB16);
+  case 4:
+    return std::make_pair(LoongArch::fixup_LARCH_ADD32,
+                          LoongArch::fixup_LARCH_SUB32);
+  case 8:
+    return std::make_pair(LoongArch::fixup_LARCH_ADD64,
+                          LoongArch::fixup_LARCH_SUB64);
+  }
+}
+
+static bool requiresFixups(MCContext &C, const MCExpr *Value,
+                           const MCExpr *&LHS, const MCExpr *&RHS) {
+  const auto *MBE = dyn_cast<MCBinaryExpr>(Value);
+  if (MBE == nullptr)
+    return false;
+
+  MCValue E;
+  if (!Value->evaluateAsRelocatable(E, nullptr, nullptr))
+    return false;
+  if (E.getSymA() == nullptr || E.getSymB() == nullptr)
+    return false;
+
+  const auto &A = E.getSymA()->getSymbol();
+  const auto &B = E.getSymB()->getSymbol();
+
+  if (A.getName().empty() && B.getName().empty())
+    return false;
+
+  if (!A.isInSection() && !B.isInSection() &&
+      !A.getName().empty() && !B.getName().empty())
+    return false;
+
+  LHS =
+      MCBinaryExpr::create(MCBinaryExpr::Add, MCSymbolRefExpr::create(&A, C),
+                           MCConstantExpr::create(E.getConstant(), C), C);
+  RHS = E.getSymB();
+
+  return (A.isInSection() ? A.getSection().hasInstructions()
+                          : !A.getName().empty()) ||
+         (B.isInSection() ? B.getSection().hasInstructions()
+                          : !B.getName().empty());
+}
+
+
+LoongArchELFStreamer::LoongArchELFStreamer(MCContext &Context,
+                                 std::unique_ptr<MCAsmBackend> MAB,
+                                 std::unique_ptr<MCObjectWriter> OW,
+                                 std::unique_ptr<MCCodeEmitter> Emitter)
+    : MCELFStreamer(Context, std::move(MAB), std::move(OW),
+                    std::move(Emitter)) {
+  }
+
+void LoongArchELFStreamer::emitCFIStartProcImpl(MCDwarfFrameInfo &Frame) {
+  Frame.Begin = getContext().createTempSymbol();
+  MCELFStreamer::emitLabel(Frame.Begin);
+}
+
+MCSymbol *LoongArchELFStreamer::emitCFILabel() {
+  MCSymbol *Label = getContext().createTempSymbol("cfi", true);
+  MCELFStreamer::emitLabel(Label);
+  return Label;
+}
+
+void LoongArchELFStreamer::emitCFIEndProcImpl(MCDwarfFrameInfo &Frame) {
+  Frame.End = getContext().createTempSymbol();
+  MCELFStreamer::emitLabel(Frame.End);
+}
+
+void LoongArchELFStreamer::emitValueImpl(const MCExpr *Value, unsigned Size,
+                                    SMLoc Loc) {
+  const MCExpr *A, *B;
+  if (!requiresFixups(getContext(), Value, A, B))
+    return MCELFStreamer::emitValueImpl(Value, Size, Loc);
+
+  MCStreamer::emitValueImpl(Value, Size, Loc);
+
+  MCDataFragment *DF = getOrCreateDataFragment();
+  flushPendingLabels(DF, DF->getContents().size());
+  MCDwarfLineEntry::Make(this, getCurrentSectionOnly());
+
+  unsigned Add, Sub;
+  std::tie(Add, Sub) = getRelocPairForSize(Size);
+
+  DF->getFixups().push_back(MCFixup::create(
+      DF->getContents().size(), A, static_cast<MCFixupKind>(Add), Loc));
+  DF->getFixups().push_back(MCFixup::create(
+      DF->getContents().size(), B, static_cast<MCFixupKind>(Sub), Loc));
+
+  DF->getContents().resize(DF->getContents().size() + Size, 0);
+}
+
+MCELFStreamer *llvm::createLoongArchELFStreamer(
+    MCContext &Context, std::unique_ptr<MCAsmBackend> MAB,
+    std::unique_ptr<MCObjectWriter> OW, std::unique_ptr<MCCodeEmitter> Emitter,
+    bool RelaxAll) {
+  return new LoongArchELFStreamer(Context, std::move(MAB), std::move(OW),
+                             std::move(Emitter));
+}
diff --git a/llvm/lib/Target/LoongArch/MCTargetDesc/LoongArchELFStreamer.h b/llvm/lib/Target/LoongArch/MCTargetDesc/LoongArchELFStreamer.h
new file mode 100644
index 000000000000..875cebcb7400
--- /dev/null
+++ b/llvm/lib/Target/LoongArch/MCTargetDesc/LoongArchELFStreamer.h
@@ -0,0 +1,53 @@
+//===- LoongArchELFStreamer.h - ELF Object Output --------------------*- C++ -*-===//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+//
+// This is a custom MCELFStreamer which allows us to insert some hooks before
+// emitting data into an actual object file.
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef LLVM_LIB_TARGET_LOONGARCH_MCTARGETDESC_LOONGARCHELFSTREAMER_H
+#define LLVM_LIB_TARGET_LOONGARCH_MCTARGETDESC_LOONGARCHELFSTREAMER_H
+
+#include "llvm/ADT/SmallVector.h"
+#include "llvm/MC/MCELFStreamer.h"
+#include <memory>
+
+namespace llvm {
+
+class MCAsmBackend;
+class MCCodeEmitter;
+class MCContext;
+class MCSubtargetInfo;
+struct MCDwarfFrameInfo;
+
+class LoongArchELFStreamer : public MCELFStreamer {
+
+public:
+  LoongArchELFStreamer(MCContext &Context, std::unique_ptr<MCAsmBackend> MAB,
+                  std::unique_ptr<MCObjectWriter> OW,
+                  std::unique_ptr<MCCodeEmitter> Emitter);
+
+  /// Overriding these functions allows us to dismiss all labels.
+  void emitValueImpl(const MCExpr *Value, unsigned Size, SMLoc Loc) override;
+
+  // Overriding these functions allows us to avoid recording of these labels
+  // in emitLabel.
+  void emitCFIStartProcImpl(MCDwarfFrameInfo &Frame) override;
+  void emitCFIEndProcImpl(MCDwarfFrameInfo &Frame) override;
+  MCSymbol *emitCFILabel() override;
+};
+
+MCELFStreamer *createLoongArchELFStreamer(MCContext &Context,
+                                     std::unique_ptr<MCAsmBackend> MAB,
+                                     std::unique_ptr<MCObjectWriter> OW,
+                                     std::unique_ptr<MCCodeEmitter> Emitter,
+                                     bool RelaxAll);
+} // end namespace llvm
+
+#endif // LLVM_LIB_TARGET_LOONGARCH_MCTARGETDESC_LOONGARCHELFSTREAMER_H
diff --git a/llvm/lib/Target/LoongArch/MCTargetDesc/LoongArchFPABIInfo.cpp b/llvm/lib/Target/LoongArch/MCTargetDesc/LoongArchFPABIInfo.cpp
new file mode 100644
index 000000000000..74874ca09b6f
--- /dev/null
+++ b/llvm/lib/Target/LoongArch/MCTargetDesc/LoongArchFPABIInfo.cpp
@@ -0,0 +1,25 @@
+//===-- LoongArchFPABIInfo.cpp - Information about LoongArch Floating Point ABI's --===//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+
+#include "MCTargetDesc/LoongArchFPABIInfo.h"
+#include "llvm/ADT/StringRef.h"
+#include "llvm/MC/MCStreamer.h"
+#include "llvm/Support/ErrorHandling.h"
+
+using namespace llvm;
+
+StringRef LoongArchFPABIInfo::getFpABIString(FpABIKind Value) {
+  switch (Value) {
+  case FpABIKind::S32:
+    return "32";
+  case FpABIKind::S64:
+    return "64";
+  default:
+    llvm_unreachable("unsupported fp abi value");
+  }
+}
diff --git a/llvm/lib/Target/LoongArch/MCTargetDesc/LoongArchFPABIInfo.h b/llvm/lib/Target/LoongArch/MCTargetDesc/LoongArchFPABIInfo.h
new file mode 100644
index 000000000000..511bd0e37dce
--- /dev/null
+++ b/llvm/lib/Target/LoongArch/MCTargetDesc/LoongArchFPABIInfo.h
@@ -0,0 +1,61 @@
+//===--- LoongArchFPABIInfo.h - Information about LoongArch Folating Point ABI's ---===//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef LLVM_LIB_TARGET_LOONGARCH_MCTARGETDESC_LOONGARCHFPABIINFO_H
+#define LLVM_LIB_TARGET_LOONGARCH_MCTARGETDESC_LOONGARCHFPABIINFO_H
+
+#include "llvm/ADT/StringRef.h"
+#include "llvm/Support/ErrorHandling.h"
+#include <cstdint>
+
+namespace llvm {
+
+class MCStreamer;
+
+struct LoongArchFPABIInfo {
+  // Internal representation of the fp_abi related values used in .module.
+  enum class FpABIKind { ANY, S32, S64, SOFT };
+
+protected:
+  // The floating-point ABI.
+  FpABIKind FpABI = FpABIKind::ANY;
+
+public:
+  LoongArchFPABIInfo() = default;
+
+  FpABIKind getFpABI() { return FpABI; }
+  void setFpABI(FpABIKind Value, bool IsABI32Bit) {
+    FpABI = Value;
+  }
+
+  StringRef getFpABIString(FpABIKind Value);
+
+  template <class PredicateLibrary>
+  void setFpAbiFromPredicates(const PredicateLibrary &P) {
+    FpABI = FpABIKind::ANY;
+    if (P.useSoftFloat())
+      FpABI = FpABIKind::SOFT;
+    else if (P.isABI_LPX32() || P.isABI_LP64D())
+      FpABI = FpABIKind::S64;
+    else if (P.isABI_LP32()) {
+      if (P.isFP64bit())
+        FpABI = FpABIKind::S64;
+      else
+        FpABI = FpABIKind::S32;
+    }
+  }
+
+  template <class PredicateLibrary>
+  void setAllFromPredicates(const PredicateLibrary &P) {
+    setFpAbiFromPredicates(P);
+  }
+};
+
+} // end namespace llvm
+
+#endif // LLVM_LIB_TARGET_LOONGARCH_MCTARGETDESC_LOONGARCHFPABIINFO_H
diff --git a/llvm/lib/Target/LoongArch/MCTargetDesc/LoongArchFixupKinds.h b/llvm/lib/Target/LoongArch/MCTargetDesc/LoongArchFixupKinds.h
new file mode 100644
index 000000000000..e0e1200d8bad
--- /dev/null
+++ b/llvm/lib/Target/LoongArch/MCTargetDesc/LoongArchFixupKinds.h
@@ -0,0 +1,90 @@
+//===-- LoongArchFixupKinds.h - LoongArch Specific Fixup Entries ----------*- C++ -*-===//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef LLVM_LIB_TARGET_LOONGARCH_MCTARGETDESC_LOONGARCHFIXUPKINDS_H
+#define LLVM_LIB_TARGET_LOONGARCH_MCTARGETDESC_LOONGARCHFIXUPKINDS_H
+
+#include "llvm/MC/MCFixup.h"
+
+namespace llvm {
+namespace LoongArch {
+  // Although most of the current fixup types reflect a unique relocation
+  // one can have multiple fixup types for a given relocation and thus need
+  // to be uniquely named.
+  //
+  // This table *must* be in the same order of
+  // MCFixupKindInfo Infos[LoongArch::NumTargetFixupKinds]
+  // in LoongArchAsmBackend.cpp.
+  //
+  enum Fixups {
+    // R_LARCH_NONE.
+    fixup_LARCH_NONE = FirstTargetFixupKind,
+
+    // reloc_hint
+    fixup_LARCH_SOP_PUSH_ABSOLUTE,
+    fixup_LARCH_SOP_PUSH_PCREL,
+    fixup_LARCH_SOP_PUSH_GPREL,
+    fixup_LARCH_SOP_PUSH_TLS_TPREL,
+    fixup_LARCH_SOP_PUSH_TLS_GOT,
+    fixup_LARCH_SOP_PUSH_TLS_GD,
+    fixup_LARCH_SOP_PUSH_PLT_PCREL,
+    // fixup methods
+    fixup_LARCH_32,
+    fixup_LARCH_64,
+    fixup_LARCH_RELATIVE,
+    fixup_LARCH_COPY,
+    fixup_LARCH_JUMP_SLOT,
+    fixup_LARCH_TLS_DTPMOD32,
+    fixup_LARCH_TLS_DTPMOD64,
+    fixup_LARCH_TLS_DTPREL32,
+    fixup_LARCH_TLS_DTPREL64,
+    fixup_LARCH_TLS_TPREL32,
+    fixup_LARCH_TLS_TPREL64,
+    fixup_LARCH_IRELATIVE,
+    fixup_LARCH_MARK_LA,
+    fixup_LARCH_MARK_PCREL,
+    fixup_LARCH_SOP_PUSH_DUP,
+    fixup_LARCH_SOP_ASSERT,
+    fixup_LARCH_SOP_NOT,
+    fixup_LARCH_SOP_SUB,
+    fixup_LARCH_SOP_SL,
+    fixup_LARCH_SOP_SR,
+    fixup_LARCH_SOP_ADD,
+    fixup_LARCH_SOP_AND,
+    fixup_LARCH_SOP_IF_ELSE,
+    fixup_LARCH_SOP_POP_32_S_10_5,
+    fixup_LARCH_SOP_POP_32_U_10_12,
+    fixup_LARCH_SOP_POP_32_S_10_12,
+    fixup_LARCH_SOP_POP_32_S_10_16,
+    fixup_LARCH_SOP_POP_32_S_10_16_S2,
+    fixup_LARCH_SOP_POP_32_S_5_20,
+    fixup_LARCH_SOP_POP_32_S_0_5_10_16_S2,
+    fixup_LARCH_SOP_POP_32_S_0_10_10_16_S2,
+    fixup_LARCH_SOP_POP_32_U,
+    fixup_LARCH_ADD8,
+    fixup_LARCH_ADD16,
+    fixup_LARCH_ADD24,
+    fixup_LARCH_ADD32,
+    fixup_LARCH_ADD64,
+    fixup_LARCH_SUB8,
+    fixup_LARCH_SUB16,
+    fixup_LARCH_SUB24,
+    fixup_LARCH_SUB32,
+    fixup_LARCH_SUB64,
+    fixup_LARCH_GNU_VTINHERIT,
+    fixup_LARCH_GNU_VTENTRY,
+
+    // Marker
+    LastTargetFixupKind,
+    NumTargetFixupKinds = LastTargetFixupKind - FirstTargetFixupKind
+  };
+} // namespace LoongArch
+} // namespace llvm
+
+
+#endif
diff --git a/llvm/lib/Target/LoongArch/MCTargetDesc/LoongArchInstPrinter.cpp b/llvm/lib/Target/LoongArch/MCTargetDesc/LoongArchInstPrinter.cpp
new file mode 100644
index 000000000000..4d096cc4c74f
--- /dev/null
+++ b/llvm/lib/Target/LoongArch/MCTargetDesc/LoongArchInstPrinter.cpp
@@ -0,0 +1,245 @@
+//===-- LoongArchInstPrinter.cpp - Convert LoongArch MCInst to assembly syntax ------===//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+//
+// This class prints an LoongArch MCInst to a .s file.
+//
+//===----------------------------------------------------------------------===//
+
+#include "LoongArchInstPrinter.h"
+#include "MCTargetDesc/LoongArchMCExpr.h"
+#include "LoongArchInstrInfo.h"
+#include "MCTargetDesc/LoongArchMCTargetDesc.h"
+#include "llvm/ADT/StringExtras.h"
+#include "llvm/MC/MCExpr.h"
+#include "llvm/MC/MCInst.h"
+#include "llvm/MC/MCInstrInfo.h"
+#include "llvm/MC/MCSymbol.h"
+#include "llvm/Support/ErrorHandling.h"
+#include "llvm/Support/raw_ostream.h"
+using namespace llvm;
+
+#define DEBUG_TYPE "asm-printer"
+
+#define PRINT_ALIAS_INSTR
+#include "LoongArchGenAsmWriter.inc"
+
+template<unsigned R>
+static bool isReg(const MCInst &MI, unsigned OpNo) {
+  assert(MI.getOperand(OpNo).isReg() && "Register operand expected.");
+  return MI.getOperand(OpNo).getReg() == R;
+}
+
+const char* LoongArch::LoongArchFCCToString(LoongArch::CondCode CC) {
+  switch (CC) {
+  case FCOND_T:
+  case FCOND_F:   return "caf";
+  case FCOND_OR:
+  case FCOND_UN:  return "cun";
+  case FCOND_UNE:
+  case FCOND_OEQ: return "ceq";
+  case FCOND_ONE:
+  case FCOND_UEQ: return "cueq";
+  case FCOND_UGE:
+  case FCOND_OLT: return "clt";
+  case FCOND_OGE:
+  case FCOND_ULT: return "cult";
+  case FCOND_UGT:
+  case FCOND_OLE: return "cle";
+  case FCOND_OGT:
+  case FCOND_ULE: return "cule";
+  case FCOND_ST:
+  case FCOND_SF:  return "saf";
+  case FCOND_GLE:
+  case FCOND_NGLE:return "sun";
+  case FCOND_SEQ: return "seq";
+  case FCOND_SNE: return "sne";
+  case FCOND_GL:
+  case FCOND_NGL: return "sueq";
+  case FCOND_NLT:
+  case FCOND_LT:  return "slt";
+  case FCOND_GE:
+  case FCOND_NGE: return "sult";
+  case FCOND_NLE:
+  case FCOND_LE:  return "sle";
+  case FCOND_GT:
+  case FCOND_NGT: return "sule";
+  case FCOND_CNE:  return "cne";
+  case FCOND_COR:  return "cor";
+  case FCOND_SOR:  return "sor";
+  case FCOND_CUNE:  return "cune";
+  case FCOND_SUNE:  return "sune";
+  }
+  llvm_unreachable("Impossible condition code!");
+}
+
+void LoongArchInstPrinter::printRegName(raw_ostream &OS, unsigned RegNo) const {
+  OS << '$' << StringRef(getRegisterName(RegNo)).lower();
+}
+
+void LoongArchInstPrinter::printInst(const MCInst *MI, uint64_t Address,
+                                     StringRef Annot,
+                                     const MCSubtargetInfo &STI,
+                                     raw_ostream &O) {
+  switch (MI->getOpcode()) {
+  default:
+    break;
+  case LoongArch::PCADDU12I_ri:
+  case LoongArch::PCADDU12I_rii:
+  case LoongArch::LU12I_W_ri:
+    printLoadAddr(MI, O);
+    return;
+  case LoongArch::ADD_D_rrr:
+  case LoongArch::LDX_D_rrr:
+  case LoongArch::ADDI_D_rri:
+  case LoongArch::ADDI_D_rrii:
+  case LoongArch::LD_D_rri:
+  case LoongArch::LD_D_rrii:
+  case LoongArch::ORI_rri:
+  case LoongArch::ORI_rrii:
+  case LoongArch::LU32I_D_ri:
+  case LoongArch::LU32I_D_rii:
+  case LoongArch::LU52I_D_rri:
+  case LoongArch::LU52I_D_rrii:
+    O << "\t# la expanded slot";
+    return;
+  }
+
+  // Try to print any aliases first.
+  if (!printAliasInstr(MI, Address, O) && !printAlias(*MI, O))
+    printInstruction(MI, Address, O);
+  printAnnotation(O, Annot);
+}
+
+void LoongArchInstPrinter::printOperand(const MCInst *MI, unsigned OpNo,
+                                   raw_ostream &O) {
+  const MCOperand &Op = MI->getOperand(OpNo);
+  if (Op.isReg()) {
+    printRegName(O, Op.getReg());
+    return;
+  }
+
+  if (Op.isImm()) {
+    O << formatImm(Op.getImm());
+    return;
+  }
+
+  assert(Op.isExpr() && "unknown operand kind in printOperand");
+  Op.getExpr()->print(O, &MAI, true);
+}
+
+template <unsigned Bits, unsigned Offset>
+void LoongArchInstPrinter::printUImm(const MCInst *MI, int opNum, raw_ostream &O) {
+  const MCOperand &MO = MI->getOperand(opNum);
+  if (MO.isImm()) {
+    uint64_t Imm = MO.getImm();
+    Imm -= Offset;
+    Imm &= (1 << Bits) - 1;
+    Imm += Offset;
+    O << formatImm(Imm);
+    return;
+  }
+
+  printOperand(MI, opNum, O);
+}
+
+void LoongArchInstPrinter::
+printMemOperand(const MCInst *MI, int opNum, raw_ostream &O) {
+  // Load/Store memory operands -- $reg, imm
+  printOperand(MI, opNum, O);
+  O << ", ";
+  printOperand(MI, opNum+1, O);
+}
+
+void LoongArchInstPrinter::
+printMemOperandEA(const MCInst *MI, int opNum, raw_ostream &O) {
+  // when using stack locations for not load/store instructions
+  // print the same way as all normal 3 operand instructions.
+  printOperand(MI, opNum, O);
+  O << ", ";
+  printOperand(MI, opNum+1, O);
+}
+
+void LoongArchInstPrinter::
+printFCCOperand(const MCInst *MI, int opNum, raw_ostream &O) {
+  const MCOperand& MO = MI->getOperand(opNum);
+  O << LoongArchFCCToString((LoongArch::CondCode)MO.getImm());
+}
+
+bool LoongArchInstPrinter::printAlias(const char *Str, const MCInst &MI,
+                                 unsigned OpNo, raw_ostream &OS) {
+  OS << "\t" << Str << "\t";
+  if(MI.getOpcode() == LoongArch::JIRL) {
+    printOperand(&MI, OpNo, OS);
+    OS << "@plt";
+  }else
+    printOperand(&MI, OpNo, OS);
+  return true;
+}
+
+bool LoongArchInstPrinter::printAlias(const char *Str, const MCInst &MI,
+                                 unsigned OpNo0, unsigned OpNo1,
+                                 raw_ostream &OS) {
+  printAlias(Str, MI, OpNo0, OS);
+  OS << ", ";
+  printOperand(&MI, OpNo1, OS);
+  return true;
+}
+
+bool LoongArchInstPrinter::printAlias(const MCInst &MI, raw_ostream &OS) {
+  switch (MI.getOpcode()) {
+    case LoongArch::OR:
+      // or $r0, $r1, $zero => move $r0, $r1
+      return isReg<LoongArch::ZERO>(MI, 2) && printAlias("move", MI, 0, 1, OS);
+  default: return false;
+  }
+}
+
+void LoongArchInstPrinter::
+printRegisterList(const MCInst *MI, int opNum, raw_ostream &O) {
+  // - 2 because register List is always first operand of instruction and it is
+  // always followed by memory operand (base + offset).
+  for (int i = opNum, e = MI->getNumOperands() - 2; i != e; ++i) {
+    if (i != opNum)
+      O << ", ";
+    printRegName(O, MI->getOperand(i).getReg());
+  }
+}
+
+void LoongArchInstPrinter::
+printLoadAddr(const MCInst *MI, raw_ostream &O) {
+  const MCOperand &Op = MI->getOperand(1);
+  const MCExpr *Expr = Op.getExpr();
+  const LoongArchMCExpr *LoongArchExpr = cast<LoongArchMCExpr>(Expr);
+  switch (LoongArchExpr->getKind()) {
+    default:
+      llvm_unreachable("invalid handled!");
+      return;
+    case LoongArchMCExpr::MEK_ABS_HI:
+      O << "\tla.abs\t";
+      break;
+    case LoongArchMCExpr::MEK_GOT_HI:
+      O << "\tla.got\t";
+      break;
+    case LoongArchMCExpr::MEK_PCREL_HI:
+      O << "\tla.pcrel\t";
+      break;
+    case LoongArchMCExpr::MEK_TLSGD_HI:
+      O << "\tla.tls.gd\t";
+      break;
+    case LoongArchMCExpr::MEK_TLSIE_HI:
+      O << "\tla.tls.ie\t";
+      break;
+    case LoongArchMCExpr::MEK_TLSLE_HI:
+      O << "\tla.tls.le\t";
+      break;
+  }
+  printRegName(O, MI->getOperand(0).getReg());
+  O << ", ";
+  Expr->print(O, nullptr);
+  return;
+}
diff --git a/llvm/lib/Target/LoongArch/MCTargetDesc/LoongArchInstPrinter.h b/llvm/lib/Target/LoongArch/MCTargetDesc/LoongArchInstPrinter.h
new file mode 100644
index 000000000000..ecc2835101ff
--- /dev/null
+++ b/llvm/lib/Target/LoongArch/MCTargetDesc/LoongArchInstPrinter.h
@@ -0,0 +1,117 @@
+//=== LoongArchInstPrinter.h - Convert LoongArch MCInst to assembly syntax -*- C++ -*-==//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+//
+// This class prints a LoongArch MCInst to a .s file.
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef LLVM_LIB_TARGET_LOONGARCH_INSTPRINTER_LOONGARCHINSTPRINTER_H
+#define LLVM_LIB_TARGET_LOONGARCH_INSTPRINTER_LOONGARCHINSTPRINTER_H
+#include "llvm/MC/MCInstPrinter.h"
+
+namespace llvm {
+
+namespace LoongArch {
+// LoongArch Branch Codes
+enum FPBranchCode {
+  BRANCH_F,
+  BRANCH_T,
+  BRANCH_INVALID
+};
+
+// LoongArch Condition Codes
+enum CondCode {
+  FCOND_F = 0x0,
+  FCOND_SF,
+  FCOND_OLT,
+  FCOND_LT,
+  FCOND_OEQ,
+  FCOND_SEQ,
+  FCOND_OLE,
+  FCOND_LE,
+  FCOND_UN,
+  FCOND_NGLE,
+  FCOND_ULT,
+  FCOND_NGE,
+  FCOND_UEQ,
+  FCOND_NGL,
+  FCOND_ULE,
+  FCOND_NGT,
+  FCOND_CNE,
+  FCOND_SNE,
+  FCOND_COR = 0x14,
+  FCOND_SOR = 0x15,
+  FCOND_CUNE = 0x18,
+  FCOND_SUNE = 0x19,
+
+  // To be used with float branch False
+  // This conditions have the same mnemonic as the
+  // above ones, but are used with a branch False;
+  FCOND_T,
+  FCOND_UNE,
+  FCOND_ST,
+  FCOND_UGE,
+  FCOND_NLT,
+  FCOND_UGT,
+  FCOND_NLE,
+  FCOND_OR,
+  FCOND_GLE,
+  FCOND_OGE,
+  FCOND_GE,
+  FCOND_ONE,
+  FCOND_GL,
+  FCOND_OGT,
+  FCOND_GT
+};
+
+const char *LoongArchFCCToString(LoongArch::CondCode CC);
+} // end namespace LoongArch
+
+class LoongArchInstPrinter : public MCInstPrinter {
+public:
+  LoongArchInstPrinter(const MCAsmInfo &MAI, const MCInstrInfo &MII,
+                  const MCRegisterInfo &MRI)
+    : MCInstPrinter(MAI, MII, MRI) {}
+
+  // Autogenerated by tblgen.
+  void printInstruction(const MCInst *MI, uint64_t Address, raw_ostream &O);
+  static const char *getRegisterName(unsigned RegNo);
+
+  void printRegName(raw_ostream &OS, unsigned RegNo) const override;
+  void printInst(const MCInst *MI, uint64_t Address, StringRef Annot,
+                 const MCSubtargetInfo &STI, raw_ostream &O) override;
+
+  bool printAliasInstr(const MCInst *MI, uint64_t Address, raw_ostream &OS);
+  void printCustomAliasOperand(const MCInst *MI, uint64_t Address,
+                               unsigned OpIdx, unsigned PrintMethodIdx,
+                               raw_ostream &O);
+
+private:
+  void printOperand(const MCInst *MI, unsigned OpNo, raw_ostream &O);
+  void printOperand(const MCInst *MI, uint64_t /*Address*/, unsigned OpNum,
+                    raw_ostream &O) {
+    printOperand(MI, OpNum, O);
+  }
+  template <unsigned Bits, unsigned Offset = 0>
+  void printUImm(const MCInst *MI, int opNum, raw_ostream &O);
+  void printMemOperand(const MCInst *MI, int opNum, raw_ostream &O);
+  void printMemOperandEA(const MCInst *MI, int opNum, raw_ostream &O);
+  void printFCCOperand(const MCInst *MI, int opNum, raw_ostream &O);
+
+  bool printAlias(const char *Str, const MCInst &MI, unsigned OpNo,
+                  raw_ostream &OS);
+  bool printAlias(const char *Str, const MCInst &MI, unsigned OpNo0,
+                  unsigned OpNo1, raw_ostream &OS);
+  bool printAlias(const MCInst &MI, raw_ostream &OS);
+  void printSaveRestore(const MCInst *MI, raw_ostream &O);
+  void printRegisterList(const MCInst *MI, int opNum, raw_ostream &O);
+  void printLoadAddr(const MCInst *MI, raw_ostream &O);
+};
+} // end namespace llvm
+
+#endif
diff --git a/llvm/lib/Target/LoongArch/MCTargetDesc/LoongArchMCAsmInfo.cpp b/llvm/lib/Target/LoongArch/MCTargetDesc/LoongArchMCAsmInfo.cpp
new file mode 100644
index 000000000000..9211cb261df7
--- /dev/null
+++ b/llvm/lib/Target/LoongArch/MCTargetDesc/LoongArchMCAsmInfo.cpp
@@ -0,0 +1,37 @@
+//===-- LoongArchMCAsmInfo.cpp - LoongArch Asm Properties ---------------------------===//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+//
+// This file contains the declarations of the LoongArchMCAsmInfo properties.
+//
+//===----------------------------------------------------------------------===//
+
+#include "LoongArchMCAsmInfo.h"
+#include "llvm/ADT/Triple.h"
+
+using namespace llvm;
+
+void LoongArchMCAsmInfo::anchor() { }
+
+LoongArchMCAsmInfo::LoongArchMCAsmInfo(const Triple &TheTriple,
+                                       const MCTargetOptions &Options) {
+
+  if (TheTriple.isLoongArch64())
+    CodePointerSize = CalleeSaveStackSlotSize = 8;
+
+  AlignmentIsInBytes          = false;
+  Data16bitsDirective         = "\t.harf\t";
+  Data32bitsDirective         = "\t.word\t";
+  Data64bitsDirective         = "\t.dword\t";
+  CommentString               = "#";
+  ZeroDirective               = "\t.space\t";
+  SupportsDebugInformation = true;
+  ExceptionsType = ExceptionHandling::DwarfCFI;
+  DwarfRegNumForCFI = true;
+  //HasLoongArchExpressions = true;
+  UseIntegratedAssembler = true;
+}
diff --git a/llvm/lib/Target/LoongArch/MCTargetDesc/LoongArchMCAsmInfo.h b/llvm/lib/Target/LoongArch/MCTargetDesc/LoongArchMCAsmInfo.h
new file mode 100644
index 000000000000..244db58dbab9
--- /dev/null
+++ b/llvm/lib/Target/LoongArch/MCTargetDesc/LoongArchMCAsmInfo.h
@@ -0,0 +1,31 @@
+//===-- LoongArchMCAsmInfo.h - LoongArch Asm Info ------------------------*- C++ -*--===//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+//
+// This file contains the declaration of the LoongArchMCAsmInfo class.
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef LLVM_LIB_TARGET_LOONGARCH_MCTARGETDESC_LOONGARCHMCASMINFO_H
+#define LLVM_LIB_TARGET_LOONGARCH_MCTARGETDESC_LOONGARCHMCASMINFO_H
+
+#include "llvm/MC/MCAsmInfoELF.h"
+
+namespace llvm {
+class Triple;
+
+class LoongArchMCAsmInfo : public MCAsmInfoELF {
+  void anchor() override;
+
+public:
+  explicit LoongArchMCAsmInfo(const Triple &TheTriple,
+                              const MCTargetOptions &Options);
+};
+
+} // namespace llvm
+
+#endif
diff --git a/llvm/lib/Target/LoongArch/MCTargetDesc/LoongArchMCCodeEmitter.cpp b/llvm/lib/Target/LoongArch/MCTargetDesc/LoongArchMCCodeEmitter.cpp
new file mode 100644
index 000000000000..3958cc30f6d5
--- /dev/null
+++ b/llvm/lib/Target/LoongArch/MCTargetDesc/LoongArchMCCodeEmitter.cpp
@@ -0,0 +1,1421 @@
+//===-- LoongArchMCCodeEmitter.cpp - Convert LoongArch Code to Machine Code ---------===//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+//
+// This file implements the LoongArchMCCodeEmitter class.
+//
+//===----------------------------------------------------------------------===//
+
+#include "LoongArchMCCodeEmitter.h"
+#include "MCTargetDesc/LoongArchFixupKinds.h"
+#include "MCTargetDesc/LoongArchMCExpr.h"
+#include "MCTargetDesc/LoongArchMCTargetDesc.h"
+#include "MCTargetDesc/LoongArchInstPrinter.h"
+#include "llvm/ADT/APFloat.h"
+#include "llvm/ADT/APInt.h"
+#include "llvm/ADT/SmallVector.h"
+#include "llvm/MC/MCContext.h"
+#include "llvm/MC/MCExpr.h"
+#include "llvm/MC/MCFixup.h"
+#include "llvm/MC/MCInst.h"
+#include "llvm/MC/MCInstrDesc.h"
+#include "llvm/MC/MCInstrInfo.h"
+#include "llvm/MC/MCRegisterInfo.h"
+#include "llvm/MC/MCSubtargetInfo.h"
+#include "llvm/Support/Casting.h"
+#include "llvm/Support/ErrorHandling.h"
+#include "llvm/Support/raw_ostream.h"
+#include <cassert>
+#include <cstdint>
+
+using namespace llvm;
+
+#define DEBUG_TYPE "mccodeemitter"
+
+#define GET_INSTRMAP_INFO
+#include "LoongArchGenInstrInfo.inc"
+#undef GET_INSTRMAP_INFO
+
+namespace llvm {
+
+MCCodeEmitter *createLoongArchMCCodeEmitter(const MCInstrInfo &MCII,
+                                         const MCRegisterInfo &MRI,
+                                         MCContext &Ctx) {
+  return new LoongArchMCCodeEmitter(MCII, Ctx);
+}
+
+} // end namespace llvm
+
+void LoongArchMCCodeEmitter::EmitByte(unsigned char C, raw_ostream &OS) const {
+  OS << (char)C;
+}
+
+void LoongArchMCCodeEmitter::EmitInstruction(uint64_t Val, unsigned Size,
+                                        const MCSubtargetInfo &STI,
+                                        raw_ostream &OS) const {
+  for (unsigned i = 0; i < Size; ++i) {
+    unsigned Shift = i * 8;
+    EmitByte((Val >> Shift) & 0xff, OS);
+  }
+}
+
+/// encodeInstruction - Emit the instruction.
+/// Size the instruction with Desc.getSize().
+void LoongArchMCCodeEmitter::
+encodeInstruction(const MCInst &MI, raw_ostream &OS,
+                  SmallVectorImpl<MCFixup> &Fixups,
+                  const MCSubtargetInfo &STI) const
+{
+  MCInst TmpInst = MI;
+
+  uint32_t Binary = getBinaryCodeForInstr(TmpInst, Fixups, STI);
+
+  const MCInstrDesc &Desc = MCII.get(TmpInst.getOpcode());
+
+  // Get byte count of instruction
+  unsigned Size = Desc.getSize();
+  if (!Size)
+    llvm_unreachable("Desc.getSize() returns 0");
+
+  EmitInstruction(Binary, Size, STI, OS);
+}
+
+/// getBranchTargetOpValue - Return binary encoding of the branch
+/// target operand. If the machine operand requires relocation,
+/// record the relocation and return zero.
+unsigned LoongArchMCCodeEmitter::
+getBranchTargetOpValue(const MCInst &MI, unsigned OpNo,
+                       SmallVectorImpl<MCFixup> &Fixups,
+                       const MCSubtargetInfo &STI) const {
+  const MCOperand &MO = MI.getOperand(OpNo);
+
+  // If the destination is an immediate, divide by 4.
+  if (MO.isImm()) return MO.getImm() >> 2;
+
+  assert(MO.isExpr() &&
+         "getBranchTargetOpValue expects only expressions or immediates");
+
+  // XXX: brtarget reloc EncoderMethod.
+  const MCExpr *Expr = MO.getExpr();
+  int64_t Value = 0x0;
+  const MCConstantExpr *tmpExpr = MCConstantExpr::create(Value, Ctx);
+  Fixups.push_back(MCFixup::create(0, Expr,
+        MCFixupKind(LoongArch::fixup_LARCH_SOP_PUSH_PCREL)));
+  switch (MI.getOpcode()) {
+  default:
+    llvm_unreachable("Unhandled reloc instruction!");
+    break;
+  case LoongArch::BEQZ:
+  case LoongArch::BEQZ32:
+  case LoongArch::BNEZ:
+  case LoongArch::BNEZ32:
+  case LoongArch::BCEQZ:
+  case LoongArch::BCNEZ:
+    Fixups.push_back(MCFixup::create(0, tmpExpr,
+        MCFixupKind(LoongArch::fixup_LARCH_SOP_POP_32_S_0_5_10_16_S2)));
+    break;
+  case LoongArch::BEQ:
+  case LoongArch::BEQ32:
+  case LoongArch::BNE:
+  case LoongArch::BNE32:
+  case LoongArch::BLT:
+  case LoongArch::BLT32:
+  case LoongArch::BGE:
+  case LoongArch::BGE32:
+  case LoongArch::BLTU:
+  case LoongArch::BLTU32:
+  case LoongArch::BGEU:
+  case LoongArch::BGEU32:
+    Fixups.push_back(MCFixup::create(0, tmpExpr,
+        MCFixupKind(LoongArch::fixup_LARCH_SOP_POP_32_S_10_16_S2)));
+    break;
+  }
+  return 0;
+}
+
+/// getJumpTargetOpValue - Return binary encoding of the jump
+/// target operand. If the machine operand requires relocation,
+/// record the relocation and return zero.
+unsigned LoongArchMCCodeEmitter::
+getJumpTargetOpValue(const MCInst &MI, unsigned OpNo,
+                     SmallVectorImpl<MCFixup> &Fixups,
+                     const MCSubtargetInfo &STI) const {
+  const MCOperand &MO = MI.getOperand(OpNo);
+  // If the destination is an immediate, divide by 4.
+  if (MO.isImm()) return MO.getImm()>>2;
+
+  assert(MO.isExpr() &&
+         "getJumpTargetOpValue expects only expressions or an immediate");
+
+  const MCExpr *Expr = MO.getExpr();
+  int64_t Value = 0x0;
+  const MCConstantExpr *tmpExpr = MCConstantExpr::create(Value, Ctx);
+  Fixups.push_back(MCFixup::create(0, Expr,
+        MCFixupKind(LoongArch::fixup_LARCH_SOP_PUSH_PLT_PCREL)));
+  if (MI.getOpcode() == LoongArch::JIRL)
+    Fixups.push_back(MCFixup::create(0, tmpExpr,
+          MCFixupKind(LoongArch::fixup_LARCH_SOP_POP_32_S_10_16_S2)));
+  else // B or BL
+    Fixups.push_back(MCFixup::create(0, tmpExpr,
+          MCFixupKind(LoongArch::fixup_LARCH_SOP_POP_32_S_0_10_10_16_S2)));
+  return 0;
+}
+
+unsigned LoongArchMCCodeEmitter::
+getSImm11Lsl1Encoding(const MCInst &MI, unsigned OpNo,
+                     SmallVectorImpl<MCFixup> &Fixups,
+                     const MCSubtargetInfo &STI) const {
+  const MCOperand &MO = MI.getOperand(OpNo);
+  if (MO.isImm()) {
+    unsigned Value = MO.getImm();
+    return Value >> 1;
+  }
+
+  return 0;
+}
+
+unsigned LoongArchMCCodeEmitter::
+getSImm10Lsl2Encoding(const MCInst &MI, unsigned OpNo,
+                     SmallVectorImpl<MCFixup> &Fixups,
+                     const MCSubtargetInfo &STI) const {
+  const MCOperand &MO = MI.getOperand(OpNo);
+  if (MO.isImm()) {
+    unsigned Value = MO.getImm();
+    return Value >> 2;
+  }
+
+  return 0;
+}
+
+unsigned LoongArchMCCodeEmitter::
+getSImm9Lsl3Encoding(const MCInst &MI, unsigned OpNo,
+                     SmallVectorImpl<MCFixup> &Fixups,
+                     const MCSubtargetInfo &STI) const {
+  const MCOperand &MO = MI.getOperand(OpNo);
+  if (MO.isImm()) {
+    unsigned Value = MO.getImm();
+    return Value >> 3;
+  }
+
+  return 0;
+}
+
+unsigned LoongArchMCCodeEmitter::
+getSImm8Lsl1Encoding(const MCInst &MI, unsigned OpNo,
+                     SmallVectorImpl<MCFixup> &Fixups,
+                     const MCSubtargetInfo &STI) const {
+  const MCOperand &MO = MI.getOperand(OpNo);
+  if (MO.isImm()) {
+    unsigned Value = MO.getImm();
+    return Value >> 1;
+  }
+
+  return 0;
+}
+
+unsigned LoongArchMCCodeEmitter::
+getSImm8Lsl2Encoding(const MCInst &MI, unsigned OpNo,
+                     SmallVectorImpl<MCFixup> &Fixups,
+                     const MCSubtargetInfo &STI) const {
+  const MCOperand &MO = MI.getOperand(OpNo);
+  if (MO.isImm()) {
+    unsigned Value = MO.getImm();
+    return Value >> 2;
+  }
+
+  return 0;
+}
+
+unsigned LoongArchMCCodeEmitter::
+getSImm8Lsl3Encoding(const MCInst &MI, unsigned OpNo,
+                     SmallVectorImpl<MCFixup> &Fixups,
+                     const MCSubtargetInfo &STI) const {
+  const MCOperand &MO = MI.getOperand(OpNo);
+  if (MO.isImm()) {
+    unsigned Value = MO.getImm();
+    return Value >> 3;
+  }
+
+  return 0;
+}
+
+unsigned LoongArchMCCodeEmitter::
+getExprOpValue(const MCInst &MI, const MCExpr *Expr,
+               SmallVectorImpl<MCFixup> &Fixups,
+               const MCSubtargetInfo &STI) const {
+  int64_t Res;
+
+  if (Expr->evaluateAsAbsolute(Res))
+    return Res;
+
+  MCExpr::ExprKind Kind = Expr->getKind();
+  if (Kind == MCExpr::Constant) {
+    return cast<MCConstantExpr>(Expr)->getValue();
+  }
+
+  if (Kind == MCExpr::Binary) {
+    unsigned Res = getExprOpValue(MI, cast<MCBinaryExpr>(Expr)->getLHS(), Fixups, STI);
+    Res += getExprOpValue(MI, cast<MCBinaryExpr>(Expr)->getRHS(), Fixups, STI);
+    return Res;
+  }
+
+  if (Kind == MCExpr::Target) {
+    int64_t Value = 0x0;
+    const LoongArchMCExpr *LoongArchExpr = cast<LoongArchMCExpr>(Expr);
+    const MCExpr *BinExpr = nullptr;
+    const MCExpr *GOTExpr = nullptr;
+    const MCSymbol *GOTSymbol = Ctx.getOrCreateSymbol(StringRef("_GLOBAL_OFFSET_TABLE_"));
+
+    LoongArch::Fixups FixupKind = LoongArch::Fixups(0);
+    switch (LoongArchExpr->getKind()) {
+    case LoongArchMCExpr::MEK_None:
+    case LoongArchMCExpr::MEK_Special:
+      llvm_unreachable("Unhandled fixup kind!");
+      Fixups.push_back(MCFixup::create(0, LoongArchExpr, MCFixupKind(FixupKind)));
+      break;
+    case LoongArchMCExpr::MEK_PLT:
+      Value = 0x0;
+      FixupKind = LoongArch::fixup_LARCH_SOP_PUSH_PLT_PCREL;
+      Fixups.push_back(MCFixup::create(0, LoongArchExpr, MCFixupKind(FixupKind)));
+      if (MI.getOpcode() == LoongArch::JIRL)
+        Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+              MCFixupKind(LoongArch::fixup_LARCH_SOP_POP_32_S_10_16_S2)));
+      else // B or BL
+        Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+              MCFixupKind(LoongArch::fixup_LARCH_SOP_POP_32_S_0_10_10_16_S2)));
+      break;
+    case LoongArchMCExpr::MEK_CALL_HI:
+      FixupKind = LoongArch::fixup_LARCH_SOP_PUSH_PLT_PCREL;
+      Fixups.push_back(MCFixup::create(0, LoongArchExpr, MCFixupKind(FixupKind)));
+
+      Value = 0x20000;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_PUSH_ABSOLUTE)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_ADD)));
+      Value = 0x12;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_PUSH_ABSOLUTE)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_SR)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_POP_32_S_5_20)));
+
+      break;
+    case LoongArchMCExpr::MEK_CALL_LO:
+      FixupKind = LoongArch::fixup_LARCH_SOP_PUSH_PLT_PCREL;
+      Fixups.push_back(MCFixup::create(0, LoongArchExpr, MCFixupKind(FixupKind)));
+
+      Value = 0x4;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_PUSH_ABSOLUTE)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_ADD)));
+      Fixups.push_back(MCFixup::create(0, LoongArchExpr, MCFixupKind(FixupKind)));
+      Value = 0x20004;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_PUSH_ABSOLUTE)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_ADD)));
+      Value = 0x12;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_PUSH_ABSOLUTE)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_SR)));
+      Value = 0x12;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_PUSH_ABSOLUTE)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_SL)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_SUB)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_POP_32_S_10_16_S2)));
+      break;
+    case LoongArchMCExpr::MEK_GOT_HI:
+      FixupKind = LoongArch::fixup_LARCH_SOP_PUSH_PCREL;
+      GOTExpr = MCSymbolRefExpr::create(GOTSymbol,
+                                        MCSymbolRefExpr::VK_None, Ctx);
+      Value = 0x800;
+      BinExpr = MCBinaryExpr::createAdd(GOTExpr, MCConstantExpr::create(Value, Ctx), Ctx);
+      Fixups.push_back(MCFixup::create(0, BinExpr, MCFixupKind(FixupKind)));
+
+      FixupKind = LoongArch::fixup_LARCH_SOP_PUSH_GPREL;
+      Fixups.push_back(MCFixup::create(0, LoongArchExpr, MCFixupKind(FixupKind)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_ADD)));
+      Value = 0xc;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_PUSH_ABSOLUTE)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_SR)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_POP_32_S_5_20)));
+      break;
+    case LoongArchMCExpr::MEK_GOT_LO:
+      FixupKind = LoongArch::fixup_LARCH_SOP_PUSH_PCREL;
+      GOTExpr = MCSymbolRefExpr::create(GOTSymbol,
+                                        MCSymbolRefExpr::VK_None, Ctx);
+      Value = 0x4;
+      BinExpr = MCBinaryExpr::createAdd(GOTExpr, MCConstantExpr::create(Value, Ctx), Ctx);
+      Fixups.push_back(MCFixup::create(0, BinExpr, MCFixupKind(FixupKind)));
+      FixupKind = LoongArch::fixup_LARCH_SOP_PUSH_GPREL;
+      Fixups.push_back(MCFixup::create(0, LoongArchExpr, MCFixupKind(FixupKind)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_ADD)));
+      FixupKind = LoongArch::fixup_LARCH_SOP_PUSH_PCREL;
+      Value = 0x804;
+      BinExpr = MCBinaryExpr::createAdd(GOTExpr, MCConstantExpr::create(Value, Ctx), Ctx);
+      Fixups.push_back(MCFixup::create(0, BinExpr, MCFixupKind(FixupKind)));
+      FixupKind = LoongArch::fixup_LARCH_SOP_PUSH_GPREL;
+      Fixups.push_back(MCFixup::create(0, LoongArchExpr, MCFixupKind(FixupKind)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_ADD)));
+      Value = 0xc;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_PUSH_ABSOLUTE)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_SR)));
+      Value = 0xc;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_PUSH_ABSOLUTE)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_SL)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_SUB)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_POP_32_S_10_12)));
+      break;
+    case LoongArchMCExpr::MEK_GOT_RRHI:
+      FixupKind = LoongArch::fixup_LARCH_SOP_PUSH_PCREL;
+      GOTExpr = MCSymbolRefExpr::create(GOTSymbol,
+                                        MCSymbolRefExpr::VK_None, Ctx);
+      Fixups.push_back(MCFixup::create(0, GOTExpr, MCFixupKind(FixupKind)));
+      FixupKind = LoongArch::fixup_LARCH_SOP_PUSH_GPREL;
+      Fixups.push_back(MCFixup::create(0, LoongArchExpr, MCFixupKind(FixupKind)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_ADD)));
+      FixupKind = LoongArch::fixup_LARCH_SOP_PUSH_PCREL;
+      Value = 0x80000000;
+      BinExpr = MCBinaryExpr::createAdd(GOTExpr, MCConstantExpr::create(Value, Ctx), Ctx);
+      Fixups.push_back(MCFixup::create(0, BinExpr, MCFixupKind(FixupKind)));
+      FixupKind = LoongArch::fixup_LARCH_SOP_PUSH_GPREL;
+      Fixups.push_back(MCFixup::create(0, LoongArchExpr, MCFixupKind(FixupKind)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_ADD)));
+      Value = 0x20;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_PUSH_ABSOLUTE)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_SR)));
+      Value = 0x20;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_PUSH_ABSOLUTE)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_SL)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_SUB)));
+      Value = 0x20;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_PUSH_ABSOLUTE)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_SL)));
+      Value = 0x2c;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_PUSH_ABSOLUTE)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_SR)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_POP_32_S_5_20)));
+      break;
+    case LoongArchMCExpr::MEK_GOT_RRLO:
+      FixupKind = LoongArch::fixup_LARCH_SOP_PUSH_PCREL;
+      GOTExpr = MCSymbolRefExpr::create(GOTSymbol,
+                                        MCSymbolRefExpr::VK_None, Ctx);
+      Value = 0x4;
+      BinExpr = MCBinaryExpr::createAdd(GOTExpr, MCConstantExpr::create(Value, Ctx), Ctx);
+      Fixups.push_back(MCFixup::create(0, BinExpr, MCFixupKind(FixupKind)));
+      FixupKind = LoongArch::fixup_LARCH_SOP_PUSH_GPREL;
+      Fixups.push_back(MCFixup::create(0, LoongArchExpr, MCFixupKind(FixupKind)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_ADD)));
+      FixupKind = LoongArch::fixup_LARCH_SOP_PUSH_PCREL;
+      Value = 0x80000004;
+      BinExpr = MCBinaryExpr::createAdd(GOTExpr, MCConstantExpr::create(Value, Ctx), Ctx);
+      Fixups.push_back(MCFixup::create(0, BinExpr, MCFixupKind(FixupKind)));
+      FixupKind = LoongArch::fixup_LARCH_SOP_PUSH_GPREL;
+      Fixups.push_back(MCFixup::create(0, LoongArchExpr, MCFixupKind(FixupKind)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_ADD)));
+      Value = 0x20;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_PUSH_ABSOLUTE)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_SR)));
+      Value = 0x20;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_PUSH_ABSOLUTE)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_SL)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_SUB)));
+      Value = 0xfff;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_PUSH_ABSOLUTE)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_AND)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_POP_32_U_10_12)));
+      break;
+    case LoongArchMCExpr::MEK_GOT_RRHIGHER:
+      FixupKind = LoongArch::fixup_LARCH_SOP_PUSH_PCREL;
+      GOTExpr = MCSymbolRefExpr::create(GOTSymbol,
+                                        MCSymbolRefExpr::VK_None, Ctx);
+      Value = 0x80000008;
+      BinExpr = MCBinaryExpr::createAdd(GOTExpr, MCConstantExpr::create(Value, Ctx), Ctx);
+      Fixups.push_back(MCFixup::create(0, BinExpr, MCFixupKind(FixupKind)));
+      FixupKind = LoongArch::fixup_LARCH_SOP_PUSH_GPREL;
+      Fixups.push_back(MCFixup::create(0, LoongArchExpr, MCFixupKind(FixupKind)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_ADD)));
+      Value = 0xc;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_PUSH_ABSOLUTE)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_SL)));
+      Value = 0x2c;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_PUSH_ABSOLUTE)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_SR)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_POP_32_S_5_20)));
+      break;
+    case LoongArchMCExpr::MEK_GOT_RRHIGHEST:
+      FixupKind = LoongArch::fixup_LARCH_SOP_PUSH_PCREL;
+      GOTExpr = MCSymbolRefExpr::create(GOTSymbol,
+                                        MCSymbolRefExpr::VK_None, Ctx);
+      Value = 0x8000000c;
+      BinExpr = MCBinaryExpr::createAdd(GOTExpr, MCConstantExpr::create(Value, Ctx), Ctx);
+      Fixups.push_back(MCFixup::create(0, BinExpr, MCFixupKind(FixupKind)));
+      FixupKind = LoongArch::fixup_LARCH_SOP_PUSH_GPREL;
+      Fixups.push_back(MCFixup::create(0, LoongArchExpr, MCFixupKind(FixupKind)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_ADD)));
+      Value = 0x34;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_PUSH_ABSOLUTE)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_SR)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_POP_32_S_10_12)));
+      break;
+    case LoongArchMCExpr::MEK_ABS_HI:
+      FixupKind = LoongArch::fixup_LARCH_MARK_LA;
+      Fixups.push_back(MCFixup::create(0, LoongArchExpr, MCFixupKind(FixupKind)));
+      FixupKind = LoongArch::fixup_LARCH_SOP_PUSH_ABSOLUTE;
+      Fixups.push_back(MCFixup::create(0, LoongArchExpr, MCFixupKind(FixupKind)));
+      Value = 0x20;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_PUSH_ABSOLUTE)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_SL)));
+      Value = 0x2c;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_PUSH_ABSOLUTE)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_SR)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_POP_32_S_5_20)));
+      break;
+    case LoongArchMCExpr::MEK_ABS_LO:
+      FixupKind = LoongArch::fixup_LARCH_SOP_PUSH_ABSOLUTE;
+      Fixups.push_back(MCFixup::create(0, LoongArchExpr, MCFixupKind(FixupKind)));
+      Value = 0xfff;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_PUSH_ABSOLUTE)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_AND)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_POP_32_U_10_12)));
+      break;
+    case LoongArchMCExpr::MEK_ABS_HIGHER:
+      FixupKind = LoongArch::fixup_LARCH_SOP_PUSH_ABSOLUTE;
+      Fixups.push_back(MCFixup::create(0, LoongArchExpr, MCFixupKind(FixupKind)));
+      Value = 0xc;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_PUSH_ABSOLUTE)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_SL)));
+      Value = 0x2c;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_PUSH_ABSOLUTE)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_SR)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_POP_32_S_5_20)));
+      break;
+    case LoongArchMCExpr::MEK_ABS_HIGHEST:
+      FixupKind = LoongArch::fixup_LARCH_SOP_PUSH_ABSOLUTE;
+      Fixups.push_back(MCFixup::create(0, LoongArchExpr, MCFixupKind(FixupKind)));
+      Value = 0x34;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_PUSH_ABSOLUTE)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_SR)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_POP_32_S_10_12)));
+      break;
+    case LoongArchMCExpr::MEK_PCREL_HI:
+      FixupKind = LoongArch::fixup_LARCH_SOP_PUSH_PCREL;
+      Value = 0x800;
+      BinExpr = MCBinaryExpr::createAdd(Expr, MCConstantExpr::create(Value, Ctx), Ctx);
+      Fixups.push_back(MCFixup::create(0, BinExpr, MCFixupKind(FixupKind)));
+      Value = 0xc;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_PUSH_ABSOLUTE)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_SR)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_POP_32_S_5_20)));
+      break;
+    case LoongArchMCExpr::MEK_PCREL_LO:
+      FixupKind = LoongArch::fixup_LARCH_SOP_PUSH_PCREL;
+      Value = 0x4;
+      BinExpr = MCBinaryExpr::createAdd(Expr, MCConstantExpr::create(Value, Ctx), Ctx);
+      Fixups.push_back(MCFixup::create(0, BinExpr, MCFixupKind(FixupKind)));
+      FixupKind = LoongArch::fixup_LARCH_SOP_PUSH_PCREL;
+      Value = 0x804;
+      BinExpr = MCBinaryExpr::createAdd(Expr, MCConstantExpr::create(Value, Ctx), Ctx);
+      Fixups.push_back(MCFixup::create(0, BinExpr, MCFixupKind(FixupKind)));
+      Value = 0xc;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_PUSH_ABSOLUTE)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_SR)));
+      Value = 0xc;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_PUSH_ABSOLUTE)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_SL)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_SUB)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_POP_32_S_10_12)));
+      break;
+    case LoongArchMCExpr::MEK_PCREL_RRHI:
+      FixupKind = LoongArch::fixup_LARCH_SOP_PUSH_PCREL;
+      Fixups.push_back(MCFixup::create(0, LoongArchExpr, MCFixupKind(FixupKind)));
+      FixupKind = LoongArch::fixup_LARCH_SOP_PUSH_PCREL;
+      Value = 0x80000000;
+      BinExpr = MCBinaryExpr::createAdd(LoongArchExpr, MCConstantExpr::create(Value, Ctx), Ctx);
+      Fixups.push_back(MCFixup::create(0, BinExpr, MCFixupKind(FixupKind)));
+      Value = 0x20;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_PUSH_ABSOLUTE)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_SR)));
+      Value = 0x20;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_PUSH_ABSOLUTE)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_SL)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_SUB)));
+      Value = 0x20;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_PUSH_ABSOLUTE)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_SL)));
+      Value = 0x2c;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_PUSH_ABSOLUTE)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_SR)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_POP_32_S_5_20)));
+      break;
+    case LoongArchMCExpr::MEK_PCREL_RRLO:
+      FixupKind = LoongArch::fixup_LARCH_SOP_PUSH_PCREL;
+      Value = 0x4;
+      BinExpr = MCBinaryExpr::createAdd(LoongArchExpr, MCConstantExpr::create(Value, Ctx), Ctx);
+      Fixups.push_back(MCFixup::create(0, BinExpr, MCFixupKind(FixupKind)));
+      FixupKind = LoongArch::fixup_LARCH_SOP_PUSH_PCREL;
+      Value = 0x80000004;
+      BinExpr = MCBinaryExpr::createAdd(LoongArchExpr, MCConstantExpr::create(Value, Ctx), Ctx);
+      Fixups.push_back(MCFixup::create(0, BinExpr, MCFixupKind(FixupKind)));
+      Value = 0x20;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_PUSH_ABSOLUTE)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_SR)));
+      Value = 0x20;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_PUSH_ABSOLUTE)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_SL)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_SUB)));
+      Value = 0xfff;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_PUSH_ABSOLUTE)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_AND)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_POP_32_U_10_12)));
+      break;
+    case LoongArchMCExpr::MEK_PCREL_RRHIGHER:
+      FixupKind = LoongArch::fixup_LARCH_SOP_PUSH_PCREL;
+      Value = 0x80000008;
+      BinExpr = MCBinaryExpr::createAdd(LoongArchExpr, MCConstantExpr::create(Value, Ctx), Ctx);
+      Fixups.push_back(MCFixup::create(0, BinExpr, MCFixupKind(FixupKind)));
+      Value = 0xc;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_PUSH_ABSOLUTE)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_SL)));
+      Value = 0x2c;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_PUSH_ABSOLUTE)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_SR)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_POP_32_S_5_20)));
+      break;
+    case LoongArchMCExpr::MEK_PCREL_RRHIGHEST:
+      FixupKind = LoongArch::fixup_LARCH_SOP_PUSH_PCREL;
+      Value = 0x8000000c;
+      BinExpr = MCBinaryExpr::createAdd(LoongArchExpr, MCConstantExpr::create(Value, Ctx), Ctx);
+      Fixups.push_back(MCFixup::create(0, BinExpr, MCFixupKind(FixupKind)));
+      Value = 0x34;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_PUSH_ABSOLUTE)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_SR)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_POP_32_S_10_12)));
+      break;
+    case LoongArchMCExpr::MEK_TLSGD_HI:
+      FixupKind = LoongArch::fixup_LARCH_SOP_PUSH_PCREL;
+      GOTExpr = MCSymbolRefExpr::create(GOTSymbol,
+                                        MCSymbolRefExpr::VK_None, Ctx);
+      Value = 0x800;
+      BinExpr = MCBinaryExpr::createAdd(GOTExpr, MCConstantExpr::create(Value, Ctx), Ctx);
+      Fixups.push_back(MCFixup::create(0, BinExpr, MCFixupKind(FixupKind)));
+      FixupKind = LoongArch::fixup_LARCH_SOP_PUSH_TLS_GD;
+      Fixups.push_back(MCFixup::create(0, LoongArchExpr, MCFixupKind(FixupKind)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_ADD)));
+      Value = 0xc;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_PUSH_ABSOLUTE)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_SR)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_POP_32_S_5_20)));
+      break;
+    case LoongArchMCExpr::MEK_TLSGD_LO:
+      FixupKind = LoongArch::fixup_LARCH_SOP_PUSH_PCREL;
+      GOTExpr = MCSymbolRefExpr::create(GOTSymbol,
+                                        MCSymbolRefExpr::VK_None, Ctx);
+      Value = 0x4;
+      BinExpr = MCBinaryExpr::createAdd(GOTExpr, MCConstantExpr::create(Value, Ctx), Ctx);
+      Fixups.push_back(MCFixup::create(0, BinExpr, MCFixupKind(FixupKind)));
+      FixupKind = LoongArch::fixup_LARCH_SOP_PUSH_TLS_GD;
+      Fixups.push_back(MCFixup::create(0, LoongArchExpr, MCFixupKind(FixupKind)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_ADD)));
+      FixupKind = LoongArch::fixup_LARCH_SOP_PUSH_PCREL;
+      Value = 0x804;
+      BinExpr = MCBinaryExpr::createAdd(GOTExpr, MCConstantExpr::create(Value, Ctx), Ctx);
+      Fixups.push_back(MCFixup::create(0, BinExpr, MCFixupKind(FixupKind)));
+      FixupKind = LoongArch::fixup_LARCH_SOP_PUSH_TLS_GD;
+      Fixups.push_back(MCFixup::create(0, LoongArchExpr, MCFixupKind(FixupKind)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_ADD)));
+      Value = 0xc;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_PUSH_ABSOLUTE)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_SR)));
+      Value = 0xc;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_PUSH_ABSOLUTE)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_SL)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_SUB)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_POP_32_S_10_12)));
+      break;
+    case LoongArchMCExpr::MEK_TLSGD_RRHI:
+      FixupKind = LoongArch::fixup_LARCH_SOP_PUSH_PCREL;
+      GOTExpr = MCSymbolRefExpr::create(GOTSymbol,
+                                        MCSymbolRefExpr::VK_None, Ctx);
+      Fixups.push_back(MCFixup::create(0, GOTExpr, MCFixupKind(FixupKind)));
+      FixupKind = LoongArch::fixup_LARCH_SOP_PUSH_TLS_GD;
+      Fixups.push_back(MCFixup::create(0, LoongArchExpr, MCFixupKind(FixupKind)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_ADD)));
+      FixupKind = LoongArch::fixup_LARCH_SOP_PUSH_PCREL;
+      Value = 0x80000000;
+      BinExpr = MCBinaryExpr::createAdd(GOTExpr, MCConstantExpr::create(Value, Ctx), Ctx);
+      Fixups.push_back(MCFixup::create(0, BinExpr, MCFixupKind(FixupKind)));
+      FixupKind = LoongArch::fixup_LARCH_SOP_PUSH_TLS_GD;
+      Fixups.push_back(MCFixup::create(0, LoongArchExpr, MCFixupKind(FixupKind)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_ADD)));
+      Value = 0x20;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_PUSH_ABSOLUTE)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_SR)));
+      Value = 0x20;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_PUSH_ABSOLUTE)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_SL)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_SUB)));
+      Value = 0x20;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_PUSH_ABSOLUTE)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_SL)));
+      Value = 0x2c;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_PUSH_ABSOLUTE)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_SR)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_POP_32_S_5_20)));
+      break;
+    case LoongArchMCExpr::MEK_TLSGD_RRLO:
+      FixupKind = LoongArch::fixup_LARCH_SOP_PUSH_PCREL;
+      GOTExpr = MCSymbolRefExpr::create(GOTSymbol,
+                                        MCSymbolRefExpr::VK_None, Ctx);
+      Value = 0x4;
+      BinExpr = MCBinaryExpr::createAdd(GOTExpr, MCConstantExpr::create(Value, Ctx), Ctx);
+      Fixups.push_back(MCFixup::create(0, BinExpr, MCFixupKind(FixupKind)));
+      FixupKind = LoongArch::fixup_LARCH_SOP_PUSH_TLS_GD;
+      Fixups.push_back(MCFixup::create(0, LoongArchExpr, MCFixupKind(FixupKind)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_ADD)));
+      FixupKind = LoongArch::fixup_LARCH_SOP_PUSH_PCREL;
+      Value = 0x80000004;
+      BinExpr = MCBinaryExpr::createAdd(GOTExpr, MCConstantExpr::create(Value, Ctx), Ctx);
+      Fixups.push_back(MCFixup::create(0, BinExpr, MCFixupKind(FixupKind)));
+      FixupKind = LoongArch::fixup_LARCH_SOP_PUSH_TLS_GD;
+      Fixups.push_back(MCFixup::create(0, LoongArchExpr, MCFixupKind(FixupKind)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_ADD)));
+      Value = 0x20;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_PUSH_ABSOLUTE)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_SR)));
+      Value = 0x20;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_PUSH_ABSOLUTE)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_SL)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_SUB)));
+      Value = 0xfff;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_PUSH_ABSOLUTE)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_AND)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_POP_32_U_10_12)));
+      break;
+    case LoongArchMCExpr::MEK_TLSGD_RRHIGHER:
+      FixupKind = LoongArch::fixup_LARCH_SOP_PUSH_PCREL;
+      GOTExpr = MCSymbolRefExpr::create(GOTSymbol,
+                                        MCSymbolRefExpr::VK_None, Ctx);
+      Value = 0x80000008;
+      BinExpr = MCBinaryExpr::createAdd(GOTExpr, MCConstantExpr::create(Value, Ctx), Ctx);
+      Fixups.push_back(MCFixup::create(0, BinExpr, MCFixupKind(FixupKind)));
+      FixupKind = LoongArch::fixup_LARCH_SOP_PUSH_TLS_GD;
+      Fixups.push_back(MCFixup::create(0, LoongArchExpr, MCFixupKind(FixupKind)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_ADD)));
+      Value = 0xc;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_PUSH_ABSOLUTE)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_SL)));
+      Value = 0x2c;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_PUSH_ABSOLUTE)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_SR)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_POP_32_S_5_20)));
+      break;
+    case LoongArchMCExpr::MEK_TLSGD_RRHIGHEST:
+      FixupKind = LoongArch::fixup_LARCH_SOP_PUSH_PCREL;
+      GOTExpr = MCSymbolRefExpr::create(GOTSymbol,
+                                        MCSymbolRefExpr::VK_None, Ctx);
+      Value = 0x8000000c;
+      BinExpr = MCBinaryExpr::createAdd(GOTExpr, MCConstantExpr::create(Value, Ctx), Ctx);
+      Fixups.push_back(MCFixup::create(0, BinExpr, MCFixupKind(FixupKind)));
+      FixupKind = LoongArch::fixup_LARCH_SOP_PUSH_TLS_GD;
+      Fixups.push_back(MCFixup::create(0, LoongArchExpr, MCFixupKind(FixupKind)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_ADD)));
+      Value = 0x34;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_PUSH_ABSOLUTE)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_SR)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_POP_32_S_10_12)));
+      break;
+    case LoongArchMCExpr::MEK_TLSIE_HI:
+      FixupKind = LoongArch::fixup_LARCH_SOP_PUSH_PCREL;
+      GOTExpr = MCSymbolRefExpr::create(GOTSymbol,
+                                        MCSymbolRefExpr::VK_None, Ctx);
+      Value = 0x800;
+      BinExpr = MCBinaryExpr::createAdd(GOTExpr, MCConstantExpr::create(Value, Ctx), Ctx);
+      Fixups.push_back(MCFixup::create(0, BinExpr, MCFixupKind(FixupKind)));
+      FixupKind = LoongArch::fixup_LARCH_SOP_PUSH_TLS_GOT;
+      Fixups.push_back(MCFixup::create(0, LoongArchExpr, MCFixupKind(FixupKind)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_ADD)));
+      Value = 0xc;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_PUSH_ABSOLUTE)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_SR)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_POP_32_S_5_20)));
+      break;
+    case LoongArchMCExpr::MEK_TLSIE_LO:
+      FixupKind = LoongArch::fixup_LARCH_SOP_PUSH_PCREL;
+      GOTExpr = MCSymbolRefExpr::create(GOTSymbol,
+                                        MCSymbolRefExpr::VK_None, Ctx);
+      Value = 0x4;
+      BinExpr = MCBinaryExpr::createAdd(GOTExpr, MCConstantExpr::create(Value, Ctx), Ctx);
+      Fixups.push_back(MCFixup::create(0, BinExpr, MCFixupKind(FixupKind)));
+      FixupKind = LoongArch::fixup_LARCH_SOP_PUSH_TLS_GOT;
+      Fixups.push_back(MCFixup::create(0, LoongArchExpr, MCFixupKind(FixupKind)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_ADD)));
+      FixupKind = LoongArch::fixup_LARCH_SOP_PUSH_PCREL;
+      Value = 0x804;
+      BinExpr = MCBinaryExpr::createAdd(GOTExpr, MCConstantExpr::create(Value, Ctx), Ctx);
+      Fixups.push_back(MCFixup::create(0, BinExpr, MCFixupKind(FixupKind)));
+      FixupKind = LoongArch::fixup_LARCH_SOP_PUSH_TLS_GOT;
+      Fixups.push_back(MCFixup::create(0, LoongArchExpr, MCFixupKind(FixupKind)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_ADD)));
+      Value = 0xc;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_PUSH_ABSOLUTE)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_SR)));
+      Value = 0xc;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_PUSH_ABSOLUTE)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_SL)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_SUB)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_POP_32_S_10_12)));
+      break;
+    case LoongArchMCExpr::MEK_TLSIE_RRHI:
+      FixupKind = LoongArch::fixup_LARCH_SOP_PUSH_PCREL;
+      GOTExpr = MCSymbolRefExpr::create(GOTSymbol,
+                                        MCSymbolRefExpr::VK_None, Ctx);
+      Fixups.push_back(MCFixup::create(0, GOTExpr, MCFixupKind(FixupKind)));
+      FixupKind = LoongArch::fixup_LARCH_SOP_PUSH_TLS_GOT;
+      Fixups.push_back(MCFixup::create(0, LoongArchExpr, MCFixupKind(FixupKind)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_ADD)));
+      FixupKind = LoongArch::fixup_LARCH_SOP_PUSH_PCREL;
+      Value = 0x80000000;
+      BinExpr = MCBinaryExpr::createAdd(GOTExpr, MCConstantExpr::create(Value, Ctx), Ctx);
+      Fixups.push_back(MCFixup::create(0, BinExpr, MCFixupKind(FixupKind)));
+      FixupKind = LoongArch::fixup_LARCH_SOP_PUSH_TLS_GOT;
+      Fixups.push_back(MCFixup::create(0, LoongArchExpr, MCFixupKind(FixupKind)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_ADD)));
+      Value = 0x20;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_PUSH_ABSOLUTE)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_SR)));
+      Value = 0x20;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_PUSH_ABSOLUTE)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_SL)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_SUB)));
+      Value = 0x20;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_PUSH_ABSOLUTE)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_SL)));
+      Value = 0x2c;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_PUSH_ABSOLUTE)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_SR)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_POP_32_S_5_20)));
+      break;
+    case LoongArchMCExpr::MEK_TLSIE_RRLO:
+      FixupKind = LoongArch::fixup_LARCH_SOP_PUSH_PCREL;
+      GOTExpr = MCSymbolRefExpr::create(GOTSymbol,
+                                        MCSymbolRefExpr::VK_None, Ctx);
+      Value = 0x4;
+      BinExpr = MCBinaryExpr::createAdd(GOTExpr, MCConstantExpr::create(Value, Ctx), Ctx);
+      Fixups.push_back(MCFixup::create(0, BinExpr, MCFixupKind(FixupKind)));
+      FixupKind = LoongArch::fixup_LARCH_SOP_PUSH_TLS_GOT;
+      Fixups.push_back(MCFixup::create(0, LoongArchExpr, MCFixupKind(FixupKind)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_ADD)));
+      FixupKind = LoongArch::fixup_LARCH_SOP_PUSH_PCREL;
+      Value = 0x80000004;
+      BinExpr = MCBinaryExpr::createAdd(GOTExpr, MCConstantExpr::create(Value, Ctx), Ctx);
+      Fixups.push_back(MCFixup::create(0, BinExpr, MCFixupKind(FixupKind)));
+      FixupKind = LoongArch::fixup_LARCH_SOP_PUSH_TLS_GOT;
+      Fixups.push_back(MCFixup::create(0, LoongArchExpr, MCFixupKind(FixupKind)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_ADD)));
+      Value = 0x20;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_PUSH_ABSOLUTE)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_SR)));
+      Value = 0x20;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_PUSH_ABSOLUTE)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_SL)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_SUB)));
+      Value = 0xfff;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_PUSH_ABSOLUTE)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_AND)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_POP_32_U_10_12)));
+      break;
+    case LoongArchMCExpr::MEK_TLSIE_RRHIGHER:
+      FixupKind = LoongArch::fixup_LARCH_SOP_PUSH_PCREL;
+      GOTExpr = MCSymbolRefExpr::create(GOTSymbol,
+                                        MCSymbolRefExpr::VK_None, Ctx);
+      Value = 0x80000008;
+      BinExpr = MCBinaryExpr::createAdd(GOTExpr, MCConstantExpr::create(Value, Ctx), Ctx);
+      Fixups.push_back(MCFixup::create(0, BinExpr, MCFixupKind(FixupKind)));
+      FixupKind = LoongArch::fixup_LARCH_SOP_PUSH_TLS_GOT;
+      Fixups.push_back(MCFixup::create(0, LoongArchExpr, MCFixupKind(FixupKind)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_ADD)));
+      Value = 0xc;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_PUSH_ABSOLUTE)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_SL)));
+      Value = 0x2c;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_PUSH_ABSOLUTE)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_SR)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_POP_32_S_5_20)));
+      break;
+    case LoongArchMCExpr::MEK_TLSIE_RRHIGHEST:
+      FixupKind = LoongArch::fixup_LARCH_SOP_PUSH_PCREL;
+      GOTExpr = MCSymbolRefExpr::create(GOTSymbol,
+                                        MCSymbolRefExpr::VK_None, Ctx);
+      Value = 0x8000000c;
+      BinExpr = MCBinaryExpr::createAdd(GOTExpr, MCConstantExpr::create(Value, Ctx), Ctx);
+      Fixups.push_back(MCFixup::create(0, BinExpr, MCFixupKind(FixupKind)));
+      FixupKind = LoongArch::fixup_LARCH_SOP_PUSH_TLS_GOT;
+      Fixups.push_back(MCFixup::create(0, LoongArchExpr, MCFixupKind(FixupKind)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_ADD)));
+      Value = 0x34;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_PUSH_ABSOLUTE)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_SR)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_POP_32_S_10_12)));
+      break;
+    case LoongArchMCExpr::MEK_TLSLE_HI:
+      FixupKind = LoongArch::fixup_LARCH_SOP_PUSH_TLS_TPREL;
+      Fixups.push_back(MCFixup::create(0, LoongArchExpr, MCFixupKind(FixupKind)));
+
+      Value = 0x20;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_PUSH_ABSOLUTE)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_SL)));
+      Value = 0x2c;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_PUSH_ABSOLUTE)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_SR)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_POP_32_S_5_20)));
+      break;
+    case LoongArchMCExpr::MEK_TLSLE_LO:
+      FixupKind = LoongArch::fixup_LARCH_SOP_PUSH_TLS_TPREL;
+      Fixups.push_back(MCFixup::create(0, LoongArchExpr, MCFixupKind(FixupKind)));
+      Value = 0xfff;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_PUSH_ABSOLUTE)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_AND)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_POP_32_U_10_12)));
+      break;
+    case LoongArchMCExpr::MEK_TLSLE_HIGHER:
+      FixupKind = LoongArch::fixup_LARCH_SOP_PUSH_TLS_TPREL;
+      Fixups.push_back(MCFixup::create(0, LoongArchExpr, MCFixupKind(FixupKind)));
+      Value = 0xc;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_PUSH_ABSOLUTE)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_SL)));
+      Value = 0x2c;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_PUSH_ABSOLUTE)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_SR)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_POP_32_S_5_20)));
+      break;
+    case LoongArchMCExpr::MEK_TLSLE_HIGHEST:
+      FixupKind = LoongArch::fixup_LARCH_SOP_PUSH_TLS_TPREL;
+      Fixups.push_back(MCFixup::create(0, LoongArchExpr, MCFixupKind(FixupKind)));
+      Value = 0x34;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_PUSH_ABSOLUTE)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_SR)));
+      Value = 0x0;
+      Fixups.push_back(MCFixup::create(0, MCConstantExpr::create(Value, Ctx),
+            MCFixupKind(LoongArch::fixup_LARCH_SOP_POP_32_S_10_12)));
+      break;
+    }
+    return 0;
+  }
+
+  if (Kind == MCExpr::SymbolRef) {
+    LoongArch::Fixups FixupKind = LoongArch::Fixups(0);
+
+    switch(cast<MCSymbolRefExpr>(Expr)->getKind()) {
+    default: llvm_unreachable("Unknown fixup kind!");
+      break;
+    }
+    Fixups.push_back(MCFixup::create(0, Expr, MCFixupKind(FixupKind)));
+    return 0;
+  }
+  return 0;
+}
+
+/// getMachineOpValue - Return binary encoding of operand. If the machine
+/// operand requires relocation, record the relocation and return zero.
+unsigned LoongArchMCCodeEmitter::
+getMachineOpValue(const MCInst &MI, const MCOperand &MO,
+                  SmallVectorImpl<MCFixup> &Fixups,
+                  const MCSubtargetInfo &STI) const {
+  if (MO.isReg()) {
+    unsigned Reg = MO.getReg();
+    unsigned RegNo = Ctx.getRegisterInfo()->getEncodingValue(Reg);
+    return RegNo;
+  } else if (MO.isImm()) {
+    return static_cast<unsigned>(MO.getImm());
+  } else if (MO.isFPImm()) {
+    return static_cast<unsigned>(APFloat(MO.getFPImm())
+        .bitcastToAPInt().getHiBits(32).getLimitedValue());
+  }
+  // MO must be an Expr.
+  assert(MO.isExpr());
+  return getExprOpValue(MI, MO.getExpr(),Fixups, STI);
+}
+
+/// Return binary encoding of memory related operand.
+/// If the offset operand requires relocation, record the relocation.
+template <unsigned ShiftAmount>
+unsigned LoongArchMCCodeEmitter::getMemEncoding(const MCInst &MI, unsigned OpNo,
+                                                SmallVectorImpl<MCFixup> &Fixups,
+                                                const MCSubtargetInfo &STI) const {
+  // Base register is encoded in bits 16-12, offset is encoded in bits 11-0.
+  assert(MI.getOperand(OpNo).isReg());
+  unsigned RegBits = getMachineOpValue(MI, MI.getOperand(OpNo),Fixups, STI) << 12;
+  unsigned OffBits = getMachineOpValue(MI, MI.getOperand(OpNo+1), Fixups, STI);
+
+  // Apply the scale factor if there is one.
+  OffBits >>= ShiftAmount;
+
+  return (OffBits & 0xFFF) | RegBits;
+}
+
+unsigned LoongArchMCCodeEmitter::getMemEncoding10l2(const MCInst &MI, unsigned OpNo,
+                                                SmallVectorImpl<MCFixup> &Fixups,
+                                                const MCSubtargetInfo &STI) const {
+  // Base register is encoded in bits 16-12, offset is encoded in bits 11-0.
+  assert(MI.getOperand(OpNo).isReg());
+  unsigned RegBits = getMachineOpValue(MI, MI.getOperand(OpNo),Fixups, STI) << 10;
+  unsigned OffBits = getMachineOpValue(MI, MI.getOperand(OpNo+1), Fixups, STI);
+
+  // Apply the scale factor if there is one.
+  OffBits >>= 2;
+
+  return (OffBits & 0x3FF) | RegBits;
+}
+
+unsigned LoongArchMCCodeEmitter::getMemEncoding11l1(const MCInst &MI, unsigned OpNo,
+                                                SmallVectorImpl<MCFixup> &Fixups,
+                                                const MCSubtargetInfo &STI) const {
+  // Base register is encoded in bits 16-12, offset is encoded in bits 11-0.
+  assert(MI.getOperand(OpNo).isReg());
+  unsigned RegBits = getMachineOpValue(MI, MI.getOperand(OpNo),Fixups, STI) << 11;
+  unsigned OffBits = getMachineOpValue(MI, MI.getOperand(OpNo+1), Fixups, STI);
+
+  // Apply the scale factor if there is one.
+  OffBits >>= 1;
+
+  return (OffBits & 0x7FF) | RegBits;
+}
+
+unsigned LoongArchMCCodeEmitter::getMemEncoding9l3(const MCInst &MI, unsigned OpNo,
+                                                SmallVectorImpl<MCFixup> &Fixups,
+                                                const MCSubtargetInfo &STI) const {
+  // Base register is encoded in bits 16-12, offset is encoded in bits 11-0.
+  assert(MI.getOperand(OpNo).isReg());
+  unsigned RegBits = getMachineOpValue(MI, MI.getOperand(OpNo),Fixups, STI) << 9;
+  unsigned OffBits = getMachineOpValue(MI, MI.getOperand(OpNo+1), Fixups, STI);
+
+  // Apply the scale factor if there is one.
+  OffBits >>= 3;
+
+  return (OffBits & 0x1FF) | RegBits;
+}
+
+/// Return binary encoding of simm14 memory related operand. Such as LL/SC instructions.
+/// If the offset operand requires relocation, record the relocation.
+template <unsigned ShiftAmount>
+unsigned LoongArchMCCodeEmitter::getSimm14MemEncoding(const MCInst &MI, unsigned OpNo,
+                                                      SmallVectorImpl<MCFixup> &Fixups,
+                                                      const MCSubtargetInfo &STI) const {
+  // Base register is encoded in bits 18-14, offset is encoded in bits 13-0.
+  assert(MI.getOperand(OpNo).isReg());
+  unsigned RegBits = getMachineOpValue(MI, MI.getOperand(OpNo),Fixups, STI) << 14;
+  unsigned OffBits = getMachineOpValue(MI, MI.getOperand(OpNo+1), Fixups, STI);
+
+  // Apply the scale factor if there is one.
+  OffBits >>= ShiftAmount;
+
+  return (OffBits & 0x3FFF) | RegBits;
+}
+
+unsigned
+LoongArchMCCodeEmitter::getFCMPEncoding(const MCInst &MI, unsigned OpNo,
+                                       SmallVectorImpl<MCFixup> &Fixups,
+                                       const MCSubtargetInfo &STI) const {
+  const MCOperand& MO = MI.getOperand(OpNo);
+  switch((LoongArch::CondCode)MO.getImm()){
+  case LoongArch::FCOND_T:
+    return 0x0;
+  case LoongArch::FCOND_OR:
+    return 0x8;
+  case LoongArch::FCOND_UNE:
+    return 0x4;
+  case LoongArch::FCOND_ONE:
+    return 0xC;
+  case LoongArch::FCOND_UGE:
+    return 0x2;
+  case LoongArch::FCOND_OGE:
+    return 0xA;
+  case LoongArch::FCOND_UGT:
+    return 0x6;
+  case LoongArch::FCOND_OGT:
+    return 0xE;
+  case LoongArch::FCOND_ST:
+    return 0x1;
+  case LoongArch::FCOND_GLE:
+    return 0x9;
+  case LoongArch::FCOND_GL:
+    return 0xD;
+  case LoongArch::FCOND_NLT:
+    return 0x3;
+  case LoongArch::FCOND_GE:
+    return 0xB;
+  case LoongArch::FCOND_NLE:
+    return 0x7;
+  case LoongArch::FCOND_GT:
+    return 0xF;
+  default:
+    return MO.getImm();
+  }
+}
+
+template <unsigned Bits, int Offset>
+unsigned
+LoongArchMCCodeEmitter::getUImmWithOffsetEncoding(const MCInst &MI, unsigned OpNo,
+                                             SmallVectorImpl<MCFixup> &Fixups,
+                                             const MCSubtargetInfo &STI) const {
+  assert(MI.getOperand(OpNo).isImm());
+  unsigned Value = getMachineOpValue(MI, MI.getOperand(OpNo), Fixups, STI);
+  Value -= Offset;
+  return Value;
+}
+
+#include "LoongArchGenMCCodeEmitter.inc"
diff --git a/llvm/lib/Target/LoongArch/MCTargetDesc/LoongArchMCCodeEmitter.h b/llvm/lib/Target/LoongArch/MCTargetDesc/LoongArchMCCodeEmitter.h
new file mode 100644
index 000000000000..cb9321640d93
--- /dev/null
+++ b/llvm/lib/Target/LoongArch/MCTargetDesc/LoongArchMCCodeEmitter.h
@@ -0,0 +1,142 @@
+//===- LoongArchMCCodeEmitter.h - Convert LoongArch Code to Machine Code --*- C++ -*-===//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+//
+// This file defines the LoongArchMCCodeEmitter class.
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef LLVM_LIB_TARGET_LOONGARCH_MCTARGETDESC_LOONGARCHMCCODEEMITTER_H
+#define LLVM_LIB_TARGET_LOONGARCH_MCTARGETDESC_LOONGARCHMCCODEEMITTER_H
+
+#include "llvm/MC/MCCodeEmitter.h"
+#include "llvm/ADT/StringRef.h"
+#include <cstdint>
+#include <map>
+
+namespace llvm {
+
+class MCContext;
+class MCExpr;
+class MCFixup;
+class MCInst;
+class MCInstrInfo;
+class MCOperand;
+class MCSubtargetInfo;
+class raw_ostream;
+
+class LoongArchMCCodeEmitter : public MCCodeEmitter {
+  const MCInstrInfo &MCII;
+  MCContext &Ctx;
+
+public:
+  LoongArchMCCodeEmitter(const MCInstrInfo &mcii, MCContext &Ctx_)
+      : MCII(mcii), Ctx(Ctx_) {}
+  LoongArchMCCodeEmitter(const LoongArchMCCodeEmitter &) = delete;
+  LoongArchMCCodeEmitter &operator=(const LoongArchMCCodeEmitter &) = delete;
+  ~LoongArchMCCodeEmitter() override = default;
+
+  void EmitByte(unsigned char C, raw_ostream &OS) const;
+
+  void EmitInstruction(uint64_t Val, unsigned Size, const MCSubtargetInfo &STI,
+                       raw_ostream &OS) const;
+
+  void encodeInstruction(const MCInst &MI, raw_ostream &OS,
+                         SmallVectorImpl<MCFixup> &Fixups,
+                         const MCSubtargetInfo &STI) const override;
+
+  // getBinaryCodeForInstr - TableGen'erated function for getting the
+  // binary encoding for an instruction.
+  uint64_t getBinaryCodeForInstr(const MCInst &MI,
+                                 SmallVectorImpl<MCFixup> &Fixups,
+                                 const MCSubtargetInfo &STI) const;
+
+  // getJumpTargetOpValue - Return binary encoding of the jump
+  // target operand. If the machine operand requires relocation,
+  // record the relocation and return zero.
+  unsigned getJumpTargetOpValue(const MCInst &MI, unsigned OpNo,
+                                SmallVectorImpl<MCFixup> &Fixups,
+                                const MCSubtargetInfo &STI) const;
+
+  // getBranchTargetOpValue - Return binary encoding of the branch
+  // target operand. If the machine operand requires relocation,
+  // record the relocation and return zero.
+  unsigned getBranchTargetOpValue(const MCInst &MI, unsigned OpNo,
+                                  SmallVectorImpl<MCFixup> &Fixups,
+                                  const MCSubtargetInfo &STI) const;
+
+  // getMachineOpValue - Return binary encoding of operand. If the machin
+  // operand requires relocation, record the relocation and return zero.
+  unsigned getMachineOpValue(const MCInst &MI, const MCOperand &MO,
+                             SmallVectorImpl<MCFixup> &Fixups,
+                             const MCSubtargetInfo &STI) const;
+
+  template <unsigned ShiftAmount = 0>
+  unsigned getMemEncoding(const MCInst &MI, unsigned OpNo,
+                          SmallVectorImpl<MCFixup> &Fixups,
+                          const MCSubtargetInfo &STI) const;
+
+  unsigned getMemEncoding10l2(const MCInst &MI, unsigned OpNo,
+                          SmallVectorImpl<MCFixup> &Fixups,
+                          const MCSubtargetInfo &STI) const;
+
+  unsigned getMemEncoding11l1(const MCInst &MI, unsigned OpNo,
+                          SmallVectorImpl<MCFixup> &Fixups,
+                          const MCSubtargetInfo &STI) const;
+
+  unsigned getMemEncoding9l3(const MCInst &MI, unsigned OpNo,
+                          SmallVectorImpl<MCFixup> &Fixups,
+                          const MCSubtargetInfo &STI) const;
+
+  template <unsigned ShiftAmount = 0>
+  unsigned getSimm14MemEncoding(const MCInst &MI, unsigned OpNo,
+                                SmallVectorImpl<MCFixup> &Fixups,
+                                const MCSubtargetInfo &STI) const;
+
+  unsigned getFCMPEncoding(const MCInst &MI, unsigned OpNo,
+                          SmallVectorImpl<MCFixup> &Fixups,
+                          const MCSubtargetInfo &STI) const;
+
+  /// Subtract Offset then encode as a N-bit unsigned integer.
+  template <unsigned Bits, int Offset>
+  unsigned getUImmWithOffsetEncoding(const MCInst &MI, unsigned OpNo,
+                                     SmallVectorImpl<MCFixup> &Fixups,
+                                     const MCSubtargetInfo &STI) const;
+
+  unsigned getExprOpValue(const MCInst &MI, const MCExpr *Expr,
+                          SmallVectorImpl<MCFixup> &Fixups,
+                          const MCSubtargetInfo &STI) const;
+
+  unsigned getSImm11Lsl1Encoding(const MCInst &MI, unsigned OpNo,
+                                SmallVectorImpl<MCFixup> &Fixups,
+                                const MCSubtargetInfo &STI) const;
+
+  unsigned getSImm10Lsl2Encoding(const MCInst &MI, unsigned OpNo,
+                                SmallVectorImpl<MCFixup> &Fixups,
+                                const MCSubtargetInfo &STI) const;
+
+  unsigned getSImm9Lsl3Encoding(const MCInst &MI, unsigned OpNo,
+                                SmallVectorImpl<MCFixup> &Fixups,
+                                const MCSubtargetInfo &STI) const;
+
+  unsigned getSImm8Lsl1Encoding(const MCInst &MI, unsigned OpNo,
+                                SmallVectorImpl<MCFixup> &Fixups,
+                                const MCSubtargetInfo &STI) const;
+
+  unsigned getSImm8Lsl2Encoding(const MCInst &MI, unsigned OpNo,
+                                SmallVectorImpl<MCFixup> &Fixups,
+                                const MCSubtargetInfo &STI) const;
+
+  unsigned getSImm8Lsl3Encoding(const MCInst &MI, unsigned OpNo,
+                                SmallVectorImpl<MCFixup> &Fixups,
+                                const MCSubtargetInfo &STI) const;
+
+};
+
+} // end namespace llvm
+
+#endif // LLVM_LIB_TARGET_LOONGARCH_MCTARGETDESC_LOONGARCHMCCODEEMITTER_H
diff --git a/llvm/lib/Target/LoongArch/MCTargetDesc/LoongArchMCExpr.cpp b/llvm/lib/Target/LoongArch/MCTargetDesc/LoongArchMCExpr.cpp
new file mode 100644
index 000000000000..1af027f15547
--- /dev/null
+++ b/llvm/lib/Target/LoongArch/MCTargetDesc/LoongArchMCExpr.cpp
@@ -0,0 +1,158 @@
+//===-- LoongArchMCExpr.cpp - LoongArch specific MC expression classes --------------===//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+
+#include "LoongArchMCExpr.h"
+#include "llvm/BinaryFormat/ELF.h"
+#include "llvm/MC/MCAsmInfo.h"
+#include "llvm/MC/MCAssembler.h"
+#include "llvm/MC/MCContext.h"
+#include "llvm/MC/MCStreamer.h"
+#include "llvm/MC/MCSymbolELF.h"
+#include "llvm/MC/MCValue.h"
+#include "llvm/Support/Casting.h"
+#include "llvm/Support/ErrorHandling.h"
+#include "llvm/Support/MathExtras.h"
+#include "llvm/Support/raw_ostream.h"
+#include <cstdint>
+
+using namespace llvm;
+
+#define DEBUG_TYPE "loongarchmcexpr"
+
+const LoongArchMCExpr *LoongArchMCExpr::create(LoongArchMCExpr::LoongArchExprKind Kind,
+                                     const MCExpr *Expr, MCContext &Ctx) {
+  return new (Ctx) LoongArchMCExpr(Kind, Expr);
+}
+
+void LoongArchMCExpr::printImpl(raw_ostream &OS, const MCAsmInfo *MAI) const {
+  int64_t AbsVal;
+  if (Expr->evaluateAsAbsolute(AbsVal))
+    OS << AbsVal;
+  else
+    Expr->print(OS, MAI, true);
+}
+
+bool
+LoongArchMCExpr::evaluateAsRelocatableImpl(MCValue &Res,
+                                      const MCAsmLayout *Layout,
+                                      const MCFixup *Fixup) const {
+  if (!getSubExpr()->evaluateAsRelocatable(Res, Layout, Fixup))
+    return false;
+
+  if (Res.getRefKind() != MCSymbolRefExpr::VK_None)
+    return false;
+
+  // evaluateAsAbsolute() and evaluateAsValue() require that we evaluate the
+  // %hi/%lo/etc. here. Fixup is a null pointer when either of these is the
+  // caller.
+  if (Res.isAbsolute() && Fixup == nullptr) {
+    int64_t AbsVal = Res.getConstant();
+    switch (Kind) {
+    default:
+      break;
+    case MEK_None:
+    case MEK_Special:
+      llvm_unreachable("MEK_None and MEK_Special are invalid");
+    }
+    Res = MCValue::get(AbsVal);
+    return true;
+  }
+
+  // We want to defer it for relocatable expressions since the constant is
+  // applied to the whole symbol value.
+  //
+  // The value of getKind() that is given to MCValue is only intended to aid
+  // debugging when inspecting MCValue objects. It shouldn't be relied upon
+  // for decision making.
+  Res = MCValue::get(Res.getSymA(), Res.getSymB(), Res.getConstant(), getKind());
+
+  return true;
+}
+
+void LoongArchMCExpr::visitUsedExpr(MCStreamer &Streamer) const {
+  Streamer.visitUsedExpr(*getSubExpr());
+}
+
+static void fixELFSymbolsInTLSFixupsImpl(const MCExpr *Expr, MCAssembler &Asm) {
+  switch (Expr->getKind()) {
+  case MCExpr::Target:
+    fixELFSymbolsInTLSFixupsImpl(cast<LoongArchMCExpr>(Expr)->getSubExpr(), Asm);
+    break;
+  case MCExpr::Constant:
+    break;
+  case MCExpr::Binary: {
+    const MCBinaryExpr *BE = cast<MCBinaryExpr>(Expr);
+    fixELFSymbolsInTLSFixupsImpl(BE->getLHS(), Asm);
+    fixELFSymbolsInTLSFixupsImpl(BE->getRHS(), Asm);
+    break;
+  }
+  case MCExpr::SymbolRef: {
+    // We're known to be under a TLS fixup, so any symbol should be
+    // modified. There should be only one.
+    const MCSymbolRefExpr &SymRef = *cast<MCSymbolRefExpr>(Expr);
+    cast<MCSymbolELF>(SymRef.getSymbol()).setType(ELF::STT_TLS);
+    break;
+  }
+  case MCExpr::Unary:
+    fixELFSymbolsInTLSFixupsImpl(cast<MCUnaryExpr>(Expr)->getSubExpr(), Asm);
+    break;
+  }
+}
+
+void LoongArchMCExpr::fixELFSymbolsInTLSFixups(MCAssembler &Asm) const {
+  switch (getKind()) {
+  default:
+    break;
+  case MEK_None:
+  case MEK_Special:
+    llvm_unreachable("MEK_None and MEK_Special are invalid");
+    break;
+  case MEK_CALL_HI:
+  case MEK_CALL_LO:
+  case MEK_GOT_HI:
+  case MEK_GOT_LO:
+  case MEK_GOT_RRHI:
+  case MEK_GOT_RRLO:
+  case MEK_GOT_RRHIGHER:
+  case MEK_GOT_RRHIGHEST:
+  case MEK_ABS_HI:
+  case MEK_ABS_LO:
+  case MEK_ABS_HIGHER:
+  case MEK_ABS_HIGHEST:
+  case MEK_PCREL_HI:
+  case MEK_PCREL_LO:
+  case MEK_PCREL_RRHI:
+  case MEK_PCREL_RRHIGHER:
+  case MEK_PCREL_RRHIGHEST:
+  case MEK_PCREL_RRLO:
+  case MEK_PLT:
+    // If we do have nested target-specific expressions, they will be in
+    // a consecutive chain.
+    if (const LoongArchMCExpr *E = dyn_cast<const LoongArchMCExpr>(getSubExpr()))
+      E->fixELFSymbolsInTLSFixups(Asm);
+    break;
+  case MEK_TLSGD_HI:
+  case MEK_TLSGD_LO:
+  case MEK_TLSGD_RRHI:
+  case MEK_TLSGD_RRHIGHER:
+  case MEK_TLSGD_RRHIGHEST:
+  case MEK_TLSGD_RRLO:
+  case MEK_TLSLE_HI:
+  case MEK_TLSLE_HIGHER:
+  case MEK_TLSLE_HIGHEST:
+  case MEK_TLSLE_LO:
+  case MEK_TLSIE_HI:
+  case MEK_TLSIE_LO:
+  case MEK_TLSIE_RRHI:
+  case MEK_TLSIE_RRHIGHER:
+  case MEK_TLSIE_RRHIGHEST:
+  case MEK_TLSIE_RRLO:
+    fixELFSymbolsInTLSFixupsImpl(getSubExpr(), Asm);
+    break;
+  }
+}
diff --git a/llvm/lib/Target/LoongArch/MCTargetDesc/LoongArchMCExpr.h b/llvm/lib/Target/LoongArch/MCTargetDesc/LoongArchMCExpr.h
new file mode 100644
index 000000000000..7851d478e913
--- /dev/null
+++ b/llvm/lib/Target/LoongArch/MCTargetDesc/LoongArchMCExpr.h
@@ -0,0 +1,97 @@
+//===- LoongArchMCExpr.h - LoongArch specific MC expression classes -------*- C++ -*-===//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef LLVM_LIB_TARGET_LOONGARCH_MCTARGETDESC_LOONGARCHMCEXPR_H
+#define LLVM_LIB_TARGET_LOONGARCH_MCTARGETDESC_LOONGARCHMCEXPR_H
+
+#include "llvm/MC/MCAsmLayout.h"
+#include "llvm/MC/MCExpr.h"
+#include "llvm/MC/MCValue.h"
+
+namespace llvm {
+
+class LoongArchMCExpr : public MCTargetExpr {
+public:
+  enum LoongArchExprKind {
+    MEK_None,
+    MEK_CALL_HI,
+    MEK_CALL_LO,
+    MEK_GOT_HI,
+    MEK_GOT_LO,
+    MEK_GOT_RRHI,
+    MEK_GOT_RRHIGHER,
+    MEK_GOT_RRHIGHEST,
+    MEK_GOT_RRLO,
+    MEK_ABS_HI,
+    MEK_ABS_HIGHER,
+    MEK_ABS_HIGHEST,
+    MEK_ABS_LO,
+    MEK_PCREL_HI,
+    MEK_PCREL_LO,
+    MEK_PCREL_RRHI,
+    MEK_PCREL_RRHIGHER,
+    MEK_PCREL_RRHIGHEST,
+    MEK_PCREL_RRLO,
+    MEK_TLSLE_HI,
+    MEK_TLSLE_HIGHER,
+    MEK_TLSLE_HIGHEST,
+    MEK_TLSLE_LO,
+    MEK_TLSIE_HI,
+    MEK_TLSIE_LO,
+    MEK_TLSIE_RRHI,
+    MEK_TLSIE_RRHIGHER,
+    MEK_TLSIE_RRHIGHEST,
+    MEK_TLSIE_RRLO,
+    MEK_TLSGD_HI,
+    MEK_TLSGD_LO,
+    MEK_TLSGD_RRHI,
+    MEK_TLSGD_RRHIGHER,
+    MEK_TLSGD_RRHIGHEST,
+    MEK_TLSGD_RRLO,
+    MEK_PLT,
+    MEK_Special,
+  };
+
+private:
+  const LoongArchExprKind Kind;
+  const MCExpr *Expr;
+
+  explicit LoongArchMCExpr(LoongArchExprKind Kind, const MCExpr *Expr)
+      : Kind(Kind), Expr(Expr) {}
+
+public:
+  static const LoongArchMCExpr *create(LoongArchExprKind Kind, const MCExpr *Expr,
+                                  MCContext &Ctx);
+  static const LoongArchMCExpr *createGpOff(LoongArchExprKind Kind, const MCExpr *Expr,
+                                       MCContext &Ctx);
+
+  /// Get the kind of this expression.
+  LoongArchExprKind getKind() const { return Kind; }
+
+  /// Get the child of this expression.
+  const MCExpr *getSubExpr() const { return Expr; }
+
+  void printImpl(raw_ostream &OS, const MCAsmInfo *MAI) const override;
+  bool evaluateAsRelocatableImpl(MCValue &Res, const MCAsmLayout *Layout,
+                                 const MCFixup *Fixup) const override;
+  void visitUsedExpr(MCStreamer &Streamer) const override;
+
+  MCFragment *findAssociatedFragment() const override {
+    return getSubExpr()->findAssociatedFragment();
+  }
+
+  void fixELFSymbolsInTLSFixups(MCAssembler &Asm) const override;
+
+  static bool classof(const MCExpr *E) {
+    return E->getKind() == MCExpr::Target;
+  }
+};
+
+} // end namespace llvm
+
+#endif // LLVM_LIB_TARGET_LOONGARCH_MCTARGETDESC_LOONGARCHMCEXPR_H
diff --git a/llvm/lib/Target/LoongArch/MCTargetDesc/LoongArchMCTargetDesc.cpp b/llvm/lib/Target/LoongArch/MCTargetDesc/LoongArchMCTargetDesc.cpp
new file mode 100644
index 000000000000..b80f164a199d
--- /dev/null
+++ b/llvm/lib/Target/LoongArch/MCTargetDesc/LoongArchMCTargetDesc.cpp
@@ -0,0 +1,192 @@
+//===-- LoongArchMCTargetDesc.cpp - LoongArch Target Descriptions -------------------===//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+//
+// This file provides LoongArch specific target descriptions.
+//
+//===----------------------------------------------------------------------===//
+
+#include "LoongArchMCTargetDesc.h"
+#include "LoongArchTargetStreamer.h"
+#include "MCTargetDesc/LoongArchAsmBackend.h"
+#include "MCTargetDesc/LoongArchELFStreamer.h"
+#include "MCTargetDesc/LoongArchInstPrinter.h"
+#include "MCTargetDesc/LoongArchMCAsmInfo.h"
+#include "TargetInfo/LoongArchTargetInfo.h"
+#include "llvm/ADT/Triple.h"
+#include "llvm/MC/MCCodeEmitter.h"
+#include "llvm/MC/MCELFStreamer.h"
+#include "llvm/MC/MCInstrAnalysis.h"
+#include "llvm/MC/MCInstrInfo.h"
+#include "llvm/MC/MCObjectWriter.h"
+#include "llvm/MC/MCRegisterInfo.h"
+#include "llvm/MC/MCSubtargetInfo.h"
+#include "llvm/MC/MCSymbol.h"
+#include "llvm/MC/MachineLocation.h"
+#include "llvm/Support/ErrorHandling.h"
+#include "llvm/Support/FormattedStream.h"
+#include "llvm/Support/TargetRegistry.h"
+
+using namespace llvm;
+
+#define GET_INSTRINFO_MC_DESC
+#include "LoongArchGenInstrInfo.inc"
+
+#define GET_SUBTARGETINFO_MC_DESC
+#include "LoongArchGenSubtargetInfo.inc"
+
+#define GET_REGINFO_MC_DESC
+#include "LoongArchGenRegisterInfo.inc"
+
+/// Select the LoongArch CPU for the given triple and cpu name.
+/// FIXME: Merge with the copy in LoongArchSubtarget.cpp
+StringRef LoongArch_MC::selectLoongArchCPU(const Triple &TT, StringRef CPU) {
+  if (CPU.empty() || CPU == "generic") {
+    if (TT.isLoongArch32())
+      CPU = "loongarch32"; //FIXME
+    else
+      CPU = "la464";
+  }
+  return CPU;
+}
+
+static MCInstrInfo *createLoongArchMCInstrInfo() {
+  MCInstrInfo *X = new MCInstrInfo();
+  InitLoongArchMCInstrInfo(X);
+  return X;
+}
+
+static MCRegisterInfo *createLoongArchMCRegisterInfo(const Triple &TT) {
+  MCRegisterInfo *X = new MCRegisterInfo();
+  InitLoongArchMCRegisterInfo(X, LoongArch::RA);
+  return X;
+}
+
+static MCSubtargetInfo *createLoongArchMCSubtargetInfo(const Triple &TT,
+                                                  StringRef CPU, StringRef FS) {
+  CPU = LoongArch_MC::selectLoongArchCPU(TT, CPU);
+  return createLoongArchMCSubtargetInfoImpl(TT, CPU, FS);
+}
+
+static MCAsmInfo *createLoongArchMCAsmInfo(const MCRegisterInfo &MRI,
+                                           const Triple &TT,
+                                           const MCTargetOptions &Options) {
+  MCAsmInfo *MAI = new LoongArchMCAsmInfo(TT, Options);
+
+  unsigned SP = MRI.getDwarfRegNum(LoongArch::SP, true);
+  MCCFIInstruction Inst = MCCFIInstruction::createDefCfaRegister(nullptr, SP);
+  MAI->addInitialFrameState(Inst);
+
+  return MAI;
+}
+
+static MCInstPrinter *createLoongArchMCInstPrinter(const Triple &T,
+                                                   unsigned SyntaxVariant,
+                                                   const MCAsmInfo &MAI,
+                                                   const MCInstrInfo &MII,
+                                                   const MCRegisterInfo &MRI) {
+  return new LoongArchInstPrinter(MAI, MII, MRI);
+}
+
+static MCStreamer *createMCStreamer(const Triple &T, MCContext &Context,
+                                    std::unique_ptr<MCAsmBackend> &&MAB,
+                                    std::unique_ptr<MCObjectWriter> &&OW,
+                                    std::unique_ptr<MCCodeEmitter> &&Emitter,
+                                    bool RelaxAll) {
+  MCStreamer *S;
+  S = createLoongArchELFStreamer(Context, std::move(MAB), std::move(OW),
+                              std::move(Emitter), RelaxAll);
+  return S;
+}
+
+static MCTargetStreamer *createLoongArchAsmTargetStreamer(MCStreamer &S,
+                                                     formatted_raw_ostream &OS,
+                                                     MCInstPrinter *InstPrint,
+                                                     bool isVerboseAsm) {
+  return new LoongArchTargetAsmStreamer(S, OS);
+}
+
+static MCTargetStreamer *createLoongArchNullTargetStreamer(MCStreamer &S) {
+  return new LoongArchTargetStreamer(S);
+}
+
+static MCTargetStreamer *
+createLoongArchObjectTargetStreamer(MCStreamer &S, const MCSubtargetInfo &STI) {
+  return new LoongArchTargetELFStreamer(S, STI);
+}
+
+namespace {
+
+class LoongArchMCInstrAnalysis : public MCInstrAnalysis {
+public:
+  LoongArchMCInstrAnalysis(const MCInstrInfo *Info) : MCInstrAnalysis(Info) {}
+
+  bool evaluateBranch(const MCInst &Inst, uint64_t Addr, uint64_t Size,
+                      uint64_t &Target) const override {
+    unsigned NumOps = Inst.getNumOperands();
+    if (NumOps == 0)
+      return false;
+    switch (Info->get(Inst.getOpcode()).OpInfo[NumOps - 1].OperandType) {
+    case MCOI::OPERAND_UNKNOWN:
+    case MCOI::OPERAND_IMMEDIATE:
+      Target = Inst.getOperand(NumOps - 1).getImm();
+      return true;
+    case MCOI::OPERAND_PCREL:
+      // b, beq ...
+      Target = Addr + Inst.getOperand(NumOps - 1).getImm();
+      return true;
+    default:
+      return false;
+    }
+  }
+};
+}
+
+static MCInstrAnalysis *createLoongArchMCInstrAnalysis(const MCInstrInfo *Info) {
+  return new LoongArchMCInstrAnalysis(Info);
+}
+
+extern "C" LLVM_EXTERNAL_VISIBILITY void LLVMInitializeLoongArchTargetMC() {
+  for (Target *T : {&getTheLoongArch32Target(), &getTheLoongArch64Target()}) {
+    // Register the MC asm info.
+    RegisterMCAsmInfoFn X(*T, createLoongArchMCAsmInfo);
+
+    // Register the MC instruction info.
+    TargetRegistry::RegisterMCInstrInfo(*T, createLoongArchMCInstrInfo);
+
+    // Register the MC register info.
+    TargetRegistry::RegisterMCRegInfo(*T, createLoongArchMCRegisterInfo);
+
+    // Register the elf streamer.
+    TargetRegistry::RegisterELFStreamer(*T, createMCStreamer);
+
+    // Register the asm target streamer.
+    TargetRegistry::RegisterAsmTargetStreamer(*T, createLoongArchAsmTargetStreamer);
+
+    TargetRegistry::RegisterNullTargetStreamer(*T,
+                                               createLoongArchNullTargetStreamer);
+
+    // Register the MC subtarget info.
+    TargetRegistry::RegisterMCSubtargetInfo(*T, createLoongArchMCSubtargetInfo);
+
+    // Register the MC instruction analyzer.
+    TargetRegistry::RegisterMCInstrAnalysis(*T, createLoongArchMCInstrAnalysis);
+
+    // Register the MCInstPrinter.
+    TargetRegistry::RegisterMCInstPrinter(*T, createLoongArchMCInstPrinter);
+
+    TargetRegistry::RegisterObjectTargetStreamer(
+        *T, createLoongArchObjectTargetStreamer);
+
+    // Register the asm backend.
+    TargetRegistry::RegisterMCAsmBackend(*T, createLoongArchAsmBackend);
+  }
+
+  // Register the MC Code Emitter
+  for (Target *T : {&getTheLoongArch32Target(), &getTheLoongArch64Target()})
+    TargetRegistry::RegisterMCCodeEmitter(*T, createLoongArchMCCodeEmitter);
+}
diff --git a/llvm/lib/Target/LoongArch/MCTargetDesc/LoongArchMCTargetDesc.h b/llvm/lib/Target/LoongArch/MCTargetDesc/LoongArchMCTargetDesc.h
new file mode 100644
index 000000000000..c348a10a0617
--- /dev/null
+++ b/llvm/lib/Target/LoongArch/MCTargetDesc/LoongArchMCTargetDesc.h
@@ -0,0 +1,68 @@
+//===-- LoongArchMCTargetDesc.h - LoongArch Target Descriptions -----------*- C++ -*-===//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+//
+// This file provides LoongArch specific target descriptions.
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef LLVM_LIB_TARGET_LOONGARCH_MCTARGETDESC_LOONGARCHMCTARGETDESC_H
+#define LLVM_LIB_TARGET_LOONGARCH_MCTARGETDESC_LOONGARCHMCTARGETDESC_H
+
+#include "llvm/Support/DataTypes.h"
+
+#include <memory>
+
+namespace llvm {
+class MCAsmBackend;
+class MCCodeEmitter;
+class MCContext;
+class MCInstrInfo;
+class MCObjectTargetWriter;
+class MCRegisterInfo;
+class MCSubtargetInfo;
+class MCTargetOptions;
+class StringRef;
+class Target;
+class Triple;
+class raw_ostream;
+class raw_pwrite_stream;
+
+Target &getTheLoongArch32Target();
+Target &getTheLoongArch64Target();
+
+MCCodeEmitter *createLoongArchMCCodeEmitter(const MCInstrInfo &MCII,
+                                            const MCRegisterInfo &MRI,
+                                            MCContext &Ctx);
+
+MCAsmBackend *createLoongArchAsmBackend(const Target &T,
+                                        const MCSubtargetInfo &STI,
+                                        const MCRegisterInfo &MRI,
+                                        const MCTargetOptions &Options);
+
+std::unique_ptr<MCObjectTargetWriter>
+createLoongArchELFObjectWriter(const Triple &TT, bool IsLPX32);
+
+namespace LoongArch_MC {
+StringRef selectLoongArchCPU(const Triple &TT, StringRef CPU);
+}
+
+} // End llvm namespace
+
+// Defines symbolic names for LoongArch registers.  This defines a mapping from
+// register name to register number.
+#define GET_REGINFO_ENUM
+#include "LoongArchGenRegisterInfo.inc"
+
+// Defines symbolic names for the LoongArch instructions.
+#define GET_INSTRINFO_ENUM
+#include "LoongArchGenInstrInfo.inc"
+
+#define GET_SUBTARGETINFO_ENUM
+#include "LoongArchGenSubtargetInfo.inc"
+
+#endif
diff --git a/llvm/lib/Target/LoongArch/MCTargetDesc/LoongArchTargetStreamer.cpp b/llvm/lib/Target/LoongArch/MCTargetDesc/LoongArchTargetStreamer.cpp
new file mode 100644
index 000000000000..80d9966defdf
--- /dev/null
+++ b/llvm/lib/Target/LoongArch/MCTargetDesc/LoongArchTargetStreamer.cpp
@@ -0,0 +1,376 @@
+//===-- LoongArchTargetStreamer.cpp - LoongArch Target Streamer Methods -------------===//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+//
+// This file provides LoongArch specific target streamer methods.
+//
+//===----------------------------------------------------------------------===//
+
+#include "LoongArchABIInfo.h"
+#include "LoongArchELFStreamer.h"
+#include "LoongArchInstPrinter.h"
+#include "LoongArchMCExpr.h"
+#include "LoongArchMCTargetDesc.h"
+#include "LoongArchTargetObjectFile.h"
+#include "LoongArchTargetStreamer.h"
+#include "llvm/BinaryFormat/ELF.h"
+#include "llvm/MC/MCContext.h"
+#include "llvm/MC/MCSectionELF.h"
+#include "llvm/MC/MCSubtargetInfo.h"
+#include "llvm/MC/MCSymbolELF.h"
+#include "llvm/Support/CommandLine.h"
+#include "llvm/Support/ErrorHandling.h"
+#include "llvm/Support/FormattedStream.h"
+
+using namespace llvm;
+
+namespace {
+static cl::opt<bool> RoundSectionSizes(
+    "loongarch-round-section-sizes", cl::init(false),
+    cl::desc("Round section sizes up to the section alignment"), cl::Hidden);
+} // end anonymous namespace
+
+LoongArchTargetStreamer::LoongArchTargetStreamer(MCStreamer &S)
+    : MCTargetStreamer(S), ModuleDirectiveAllowed(true) {
+  GPRInfoSet = FPRInfoSet = FrameInfoSet = false;
+}
+void LoongArchTargetStreamer::emitDirectiveOptionPic0() {}
+void LoongArchTargetStreamer::emitDirectiveOptionPic2() {}
+void LoongArchTargetStreamer::emitDirectiveSetArch(StringRef Arch) {
+  forbidModuleDirective();
+}
+void LoongArchTargetStreamer::emitDirectiveSetLoongArch32() { forbidModuleDirective(); }
+void LoongArchTargetStreamer::emitDirectiveSetloongarch64() { forbidModuleDirective(); }
+void LoongArchTargetStreamer::emitDirectiveSetSoftFloat() {
+  forbidModuleDirective();
+}
+void LoongArchTargetStreamer::emitDirectiveSetHardFloat() {
+  forbidModuleDirective();
+}
+void LoongArchTargetStreamer::emitDirectiveModuleFP() {}
+
+void LoongArchTargetStreamer::emitDirectiveModuleSoftFloat() {}
+void LoongArchTargetStreamer::emitDirectiveModuleHardFloat() {}
+void LoongArchTargetStreamer::emitDirectiveSetFp(
+    LoongArchFPABIInfo::FpABIKind Value) {
+  forbidModuleDirective();
+}
+
+void LoongArchTargetStreamer::emitR(unsigned Opcode, unsigned Reg0, SMLoc IDLoc,
+                               const MCSubtargetInfo *STI) {
+  MCInst TmpInst;
+  TmpInst.setOpcode(Opcode);
+  TmpInst.addOperand(MCOperand::createReg(Reg0));
+  TmpInst.setLoc(IDLoc);
+  getStreamer().emitInstruction(TmpInst, *STI);
+}
+
+void LoongArchTargetStreamer::emitRXX(unsigned Opcode, unsigned Reg0, MCOperand Op1,
+                                MCOperand Op2, SMLoc IDLoc, const MCSubtargetInfo *STI) {
+  MCInst TmpInst;
+  TmpInst.setOpcode(Opcode);
+  TmpInst.addOperand(MCOperand::createReg(Reg0));
+  TmpInst.addOperand(Op1);
+  TmpInst.addOperand(Op2);
+  TmpInst.setLoc(IDLoc);
+  getStreamer().emitInstruction(TmpInst, *STI);
+}
+
+void LoongArchTargetStreamer::emitRRXX(unsigned Opcode, unsigned Reg0, unsigned Reg1,
+                                 MCOperand Op2, MCOperand Op3, SMLoc IDLoc,
+                                 const MCSubtargetInfo *STI) {
+  MCInst TmpInst;
+  TmpInst.setOpcode(Opcode);
+  TmpInst.addOperand(MCOperand::createReg(Reg0));
+  TmpInst.addOperand(MCOperand::createReg(Reg1));
+  TmpInst.addOperand(Op2);
+  TmpInst.addOperand(Op3);
+  TmpInst.setLoc(IDLoc);
+  getStreamer().emitInstruction(TmpInst, *STI);
+}
+
+void LoongArchTargetStreamer::emitRX(unsigned Opcode, unsigned Reg0, MCOperand Op1,
+                                SMLoc IDLoc, const MCSubtargetInfo *STI) {
+  MCInst TmpInst;
+  TmpInst.setOpcode(Opcode);
+  TmpInst.addOperand(MCOperand::createReg(Reg0));
+  TmpInst.addOperand(Op1);
+  TmpInst.setLoc(IDLoc);
+  getStreamer().emitInstruction(TmpInst, *STI);
+}
+
+void LoongArchTargetStreamer::emitRI(unsigned Opcode, unsigned Reg0, int32_t Imm,
+                                SMLoc IDLoc, const MCSubtargetInfo *STI) {
+  emitRX(Opcode, Reg0, MCOperand::createImm(Imm), IDLoc, STI);
+}
+
+void LoongArchTargetStreamer::emitRR(unsigned Opcode, unsigned Reg0, unsigned Reg1,
+                                SMLoc IDLoc, const MCSubtargetInfo *STI) {
+  emitRX(Opcode, Reg0, MCOperand::createReg(Reg1), IDLoc, STI);
+}
+
+void LoongArchTargetStreamer::emitII(unsigned Opcode, int16_t Imm1, int16_t Imm2,
+                                SMLoc IDLoc, const MCSubtargetInfo *STI) {
+  MCInst TmpInst;
+  TmpInst.setOpcode(Opcode);
+  TmpInst.addOperand(MCOperand::createImm(Imm1));
+  TmpInst.addOperand(MCOperand::createImm(Imm2));
+  TmpInst.setLoc(IDLoc);
+  getStreamer().emitInstruction(TmpInst, *STI);
+}
+
+void LoongArchTargetStreamer::emitRRX(unsigned Opcode, unsigned Reg0, unsigned Reg1,
+                                 MCOperand Op2, SMLoc IDLoc,
+                                 const MCSubtargetInfo *STI) {
+  MCInst TmpInst;
+  TmpInst.setOpcode(Opcode);
+  TmpInst.addOperand(MCOperand::createReg(Reg0));
+  TmpInst.addOperand(MCOperand::createReg(Reg1));
+  TmpInst.addOperand(Op2);
+  TmpInst.setLoc(IDLoc);
+  getStreamer().emitInstruction(TmpInst, *STI);
+}
+
+void LoongArchTargetStreamer::emitRRR(unsigned Opcode, unsigned Reg0, unsigned Reg1,
+                                 unsigned Reg2, SMLoc IDLoc,
+                                 const MCSubtargetInfo *STI) {
+  emitRRX(Opcode, Reg0, Reg1, MCOperand::createReg(Reg2), IDLoc, STI);
+}
+
+void LoongArchTargetStreamer::emitRRI(unsigned Opcode, unsigned Reg0, unsigned Reg1,
+                                 int16_t Imm, SMLoc IDLoc,
+                                 const MCSubtargetInfo *STI) {
+  emitRRX(Opcode, Reg0, Reg1, MCOperand::createImm(Imm), IDLoc, STI);
+}
+
+void LoongArchTargetStreamer::emitRRIII(unsigned Opcode, unsigned Reg0,
+                                   unsigned Reg1, int16_t Imm0, int16_t Imm1,
+                                   int16_t Imm2, SMLoc IDLoc,
+                                   const MCSubtargetInfo *STI) {
+  MCInst TmpInst;
+  TmpInst.setOpcode(Opcode);
+  TmpInst.addOperand(MCOperand::createReg(Reg0));
+  TmpInst.addOperand(MCOperand::createReg(Reg1));
+  TmpInst.addOperand(MCOperand::createImm(Imm0));
+  TmpInst.addOperand(MCOperand::createImm(Imm1));
+  TmpInst.addOperand(MCOperand::createImm(Imm2));
+  TmpInst.setLoc(IDLoc);
+  getStreamer().emitInstruction(TmpInst, *STI);
+}
+
+void LoongArchTargetStreamer::emitAdd(unsigned DstReg, unsigned SrcReg,
+                                  unsigned TrgReg, bool Is64Bit,
+                                  const MCSubtargetInfo *STI) {
+  emitRRR(Is64Bit ? LoongArch::ADD_D : LoongArch::ADD_W, DstReg, SrcReg, TrgReg, SMLoc(),
+          STI);
+}
+
+void LoongArchTargetStreamer::emitDSLL(unsigned DstReg, unsigned SrcReg,
+                                  int16_t ShiftAmount, SMLoc IDLoc,
+                                  const MCSubtargetInfo *STI) {
+  if (ShiftAmount >= 32) {
+    emitRRI(LoongArch::SLLI_D, DstReg, SrcReg, ShiftAmount - 32, IDLoc, STI);
+    return;
+  }
+
+  emitRRI(LoongArch::SLLI_D, DstReg, SrcReg, ShiftAmount, IDLoc, STI);
+}
+
+void LoongArchTargetStreamer::emitNop(SMLoc IDLoc, const MCSubtargetInfo *STI) {
+  emitRRI(LoongArch::ANDI, LoongArch::ZERO, LoongArch::ZERO, 0, IDLoc, STI);
+}
+
+LoongArchTargetAsmStreamer::LoongArchTargetAsmStreamer(MCStreamer &S,
+                                             formatted_raw_ostream &OS)
+    : LoongArchTargetStreamer(S), OS(OS) {}
+
+void LoongArchTargetAsmStreamer::emitDirectiveOptionPic0() {
+  OS << "\t.option\tpic0\n";
+}
+
+void LoongArchTargetAsmStreamer::emitDirectiveOptionPic2() {
+  OS << "\t.option\tpic2\n";
+}
+
+void LoongArchTargetAsmStreamer::emitDirectiveSetArch(StringRef Arch) {
+  OS << "\t.set arch=" << Arch << "\n";
+  LoongArchTargetStreamer::emitDirectiveSetArch(Arch);
+}
+
+void LoongArchTargetAsmStreamer::emitDirectiveSetLoongArch32() {
+  //OS << "\t.set\tloongarch32\n";
+  LoongArchTargetStreamer::emitDirectiveSetLoongArch32();
+}
+
+void LoongArchTargetAsmStreamer::emitDirectiveSetloongarch64() {
+  //OS << "\t.set\tloongarch64\n";
+  LoongArchTargetStreamer::emitDirectiveSetloongarch64();
+}
+
+void LoongArchTargetAsmStreamer::emitDirectiveSetSoftFloat() {
+  OS << "\t.set\tsoftfloat\n";
+  LoongArchTargetStreamer::emitDirectiveSetSoftFloat();
+}
+
+void LoongArchTargetAsmStreamer::emitDirectiveSetHardFloat() {
+  OS << "\t.set\thardfloat\n";
+  LoongArchTargetStreamer::emitDirectiveSetHardFloat();
+}
+
+void LoongArchTargetAsmStreamer::emitDirectiveModuleFP() {
+  LoongArchFPABIInfo::FpABIKind FpABI = FPABIInfo.getFpABI();
+  if (FpABI == LoongArchFPABIInfo::FpABIKind::SOFT)
+    OS << "\t.module\tsoftfloat\n";
+  else
+    OS << "\t.module\tfp=" << FPABIInfo.getFpABIString(FpABI) << "\n";
+}
+
+void LoongArchTargetAsmStreamer::emitDirectiveSetFp(
+    LoongArchFPABIInfo::FpABIKind Value) {
+  LoongArchTargetStreamer::emitDirectiveSetFp(Value);
+
+  OS << "\t.set\tfp=";
+  OS << FPABIInfo.getFpABIString(Value) << "\n";
+}
+
+void LoongArchTargetAsmStreamer::emitDirectiveModuleSoftFloat() {
+  OS << "\t.module\tsoftfloat\n";
+}
+
+void LoongArchTargetAsmStreamer::emitDirectiveModuleHardFloat() {
+  OS << "\t.module\thardfloat\n";
+}
+
+// This part is for ELF object output.
+LoongArchTargetELFStreamer::LoongArchTargetELFStreamer(MCStreamer &S,
+                                             const MCSubtargetInfo &STI)
+    : LoongArchTargetStreamer(S), STI(STI) {
+  MCAssembler &MCA = getStreamer().getAssembler();
+
+  // It's possible that MCObjectFileInfo isn't fully initialized at this point
+  // due to an initialization order problem where LLVMTargetMachine creates the
+  // target streamer before TargetLoweringObjectFile calls
+  // InitializeMCObjectFileInfo. There doesn't seem to be a single place that
+  // covers all cases so this statement covers most cases and direct object
+  // emission must call setPic() once MCObjectFileInfo has been initialized. The
+  // cases we don't handle here are covered by LoongArchAsmPrinter.
+  Pic = MCA.getContext().getObjectFileInfo()->isPositionIndependent();
+
+  // Set the header flags that we can in the constructor.
+  // FIXME: This is a fairly terrible hack. We set the rest
+  // of these in the destructor. The problem here is two-fold:
+  //
+  // a: Some of the eflags can be set/reset by directives.
+  // b: There aren't any usage paths that initialize the ABI
+  //    pointer until after we initialize either an assembler
+  //    or the target machine.
+  // We can fix this by making the target streamer construct
+  // the ABI, but this is fraught with wide ranging dependency
+  // issues as well.
+  unsigned EFlags = MCA.getELFHeaderEFlags();
+
+  // FIXME: Fix a dependency issue by instantiating the ABI object to some
+  // default based off the triple. The triple doesn't describe the target
+  // fully, but any external user of the API that uses the MCTargetStreamer
+  // would otherwise crash on assertion failure.
+
+  ABI = LoongArchABIInfo(
+      STI.getTargetTriple().getArch() == Triple::ArchType::loongarch32
+          ? LoongArchABIInfo::LP32()
+          : LoongArchABIInfo::LP64D());
+
+  EFlags |= ELF::EF_LARCH_ABI;
+  MCA.setELFHeaderEFlags(EFlags);
+}
+
+void LoongArchTargetELFStreamer::emitLabel(MCSymbol *S) {
+  auto *Symbol = cast<MCSymbolELF>(S);
+  getStreamer().getAssembler().registerSymbol(*Symbol);
+  uint8_t Type = Symbol->getType();
+  if (Type != ELF::STT_FUNC)
+    return;
+
+}
+
+void LoongArchTargetELFStreamer::finish() {
+  MCAssembler &MCA = getStreamer().getAssembler();
+  const MCObjectFileInfo &OFI = *MCA.getContext().getObjectFileInfo();
+
+  // .bss, .text and .data are always at least 16-byte aligned.
+  MCSection &TextSection = *OFI.getTextSection();
+  MCA.registerSection(TextSection);
+  MCSection &DataSection = *OFI.getDataSection();
+  MCA.registerSection(DataSection);
+  MCSection &BSSSection = *OFI.getBSSSection();
+  MCA.registerSection(BSSSection);
+
+  TextSection.setAlignment(Align(std::max(16u, TextSection.getAlignment())));
+  DataSection.setAlignment(Align(std::max(16u, DataSection.getAlignment())));
+  BSSSection.setAlignment(Align(std::max(16u, BSSSection.getAlignment())));
+
+  if (RoundSectionSizes) {
+    // Make sections sizes a multiple of the alignment. This is useful for
+    // verifying the output of IAS against the output of other assemblers but
+    // it's not necessary to produce a correct object and increases section
+    // size.
+    MCStreamer &OS = getStreamer();
+    for (MCSection &S : MCA) {
+      MCSectionELF &Section = static_cast<MCSectionELF &>(S);
+
+      unsigned Alignment = Section.getAlignment();
+      if (Alignment) {
+        OS.SwitchSection(&Section);
+        if (Section.UseCodeAlign())
+          OS.emitCodeAlignment(Alignment, Alignment);
+        else
+          OS.emitValueToAlignment(Alignment, 0, 1, Alignment);
+      }
+    }
+  }
+
+  // Update e_header flags. See the FIXME and comment above in
+  // the constructor for a full rundown on this.
+  unsigned EFlags = MCA.getELFHeaderEFlags();
+
+  // ABI
+  // LP64D does not require any ABI bits.
+  if (getABI().IsLP32())
+    EFlags |= ELF::EF_LARCH_ABI_LP32;
+  else if (getABI().IsLPX32())
+    EFlags |= ELF::EF_LARCH_ABI_XLP32;
+  else
+    EFlags |= ELF::EF_LARCH_ABI_LP64D;
+
+  MCA.setELFHeaderEFlags(EFlags);
+}
+
+MCELFStreamer &LoongArchTargetELFStreamer::getStreamer() {
+  return static_cast<MCELFStreamer &>(Streamer);
+}
+
+void LoongArchTargetELFStreamer::emitDirectiveOptionPic0() {
+  MCAssembler &MCA = getStreamer().getAssembler();
+  unsigned Flags = MCA.getELFHeaderEFlags();
+  // This option overrides other PIC options like -KPIC.
+  Pic = false;
+  ///XXX:Reloc no this flags
+  //Flags &= ~ELF::EF_LOONGARCH_PIC;
+  MCA.setELFHeaderEFlags(Flags);
+}
+
+void LoongArchTargetELFStreamer::emitDirectiveOptionPic2() {
+  MCAssembler &MCA = getStreamer().getAssembler();
+  unsigned Flags = MCA.getELFHeaderEFlags();
+  Pic = true;
+  // NOTE: We are following the GAS behaviour here which means the directive
+  // 'pic2' also sets the CPIC bit in the ELF header. This is different from
+  // what is stated in the SYSV ABI which consider the bits EF_LOONGARCH_PIC and
+  // EF_LOONGARCH_CPIC to be mutually exclusive.
+  ///XXX:Reloc no this flags
+  //Flags |= ELF::EF_LOONGARCH_PIC | ELF::EF_LOONGARCH_CPIC;
+  MCA.setELFHeaderEFlags(Flags);
+}
diff --git a/llvm/lib/Target/LoongArch/TargetInfo/CMakeLists.txt b/llvm/lib/Target/LoongArch/TargetInfo/CMakeLists.txt
new file mode 100644
index 000000000000..a0749909105b
--- /dev/null
+++ b/llvm/lib/Target/LoongArch/TargetInfo/CMakeLists.txt
@@ -0,0 +1,3 @@
+add_llvm_component_library(LLVMLoongArchInfo
+  LoongArchTargetInfo.cpp
+  )
diff --git a/llvm/lib/Target/LoongArch/TargetInfo/LLVMBuild.txt b/llvm/lib/Target/LoongArch/TargetInfo/LLVMBuild.txt
new file mode 100644
index 000000000000..6838e4066052
--- /dev/null
+++ b/llvm/lib/Target/LoongArch/TargetInfo/LLVMBuild.txt
@@ -0,0 +1,22 @@
+;===- ./lib/Target/LoongArch/TargetInfo/LLVMBuild.txt ---------------*- Conf -*--===;
+;
+; Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+; See https://llvm.org/LICENSE.txt for license information.
+; SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+;
+;===------------------------------------------------------------------------===;
+;
+; This is an LLVMBuild description file for the components in this subdirectory.
+;
+; For more information on the LLVMBuild system, please see:
+;
+;   http://llvm.org/docs/LLVMBuild.html
+;
+;===------------------------------------------------------------------------===;
+
+[component_0]
+type = Library
+name = LoongArchInfo
+parent = LoongArch
+required_libraries = Support
+add_to_library_groups = LoongArch
diff --git a/llvm/lib/Target/LoongArch/TargetInfo/LoongArchTargetInfo.cpp b/llvm/lib/Target/LoongArch/TargetInfo/LoongArchTargetInfo.cpp
new file mode 100644
index 000000000000..bf776db26175
--- /dev/null
+++ b/llvm/lib/Target/LoongArch/TargetInfo/LoongArchTargetInfo.cpp
@@ -0,0 +1,33 @@
+//===-- LoongArchTargetInfo.cpp - LoongArch Target Implementation -------------------===//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+
+#include "TargetInfo/LoongArchTargetInfo.h"
+#include "llvm/Support/TargetRegistry.h"
+using namespace llvm;
+
+Target &llvm::getTheLoongArch32Target() {
+  static Target TheLoongArch32Target;
+  return TheLoongArch32Target;
+}
+
+Target &llvm::getTheLoongArch64Target() {
+  static Target TheLoongArch64Target;
+  return TheLoongArch64Target;
+}
+
+extern "C" LLVM_EXTERNAL_VISIBILITY void LLVMInitializeLoongArchTargetInfo() {
+#if 0
+  //TODO: support it in futrue
+  RegisterTarget<Triple::loongarch32,
+                 /*HasJIT=*/false>
+      X(getTheLoongArch32Target(), "loongarch32", "LoongArch (32-bit)", "LoongArch");
+#endif
+  RegisterTarget<Triple::loongarch64,
+                 /*HasJIT=*/false>
+      A(getTheLoongArch64Target(), "loongarch64", "LoongArch (64-bit)", "LoongArch");
+}
diff --git a/llvm/lib/Target/LoongArch/TargetInfo/LoongArchTargetInfo.h b/llvm/lib/Target/LoongArch/TargetInfo/LoongArchTargetInfo.h
new file mode 100644
index 000000000000..7dce2497f7d9
--- /dev/null
+++ b/llvm/lib/Target/LoongArch/TargetInfo/LoongArchTargetInfo.h
@@ -0,0 +1,21 @@
+//===-- LoongArchTargetInfo.h - LoongArch Target Implementation -----------*- C++ -*-===//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef LLVM_LIB_TARGET_LOONGARCH_TARGETINFO_LOONGARCHTARGETINFO_H
+#define LLVM_LIB_TARGET_LOONGARCH_TARGETINFO_LOONGARCHTARGETINFO_H
+
+namespace llvm {
+
+class Target;
+
+Target &getTheLoongArch32Target();
+Target &getTheLoongArch64Target();
+
+} // namespace llvm
+
+#endif // LLVM_LIB_TARGET_LOONGARCH_TARGETINFO_LOONGARCHTARGETINFO_H
diff --git a/llvm/lib/Target/NVPTX/NVPTXAsmPrinter.cpp b/llvm/lib/Target/NVPTX/NVPTXAsmPrinter.cpp
index da1a398a68f0..38844ff4ddf9 100644
--- a/llvm/lib/Target/NVPTX/NVPTXAsmPrinter.cpp
+++ b/llvm/lib/Target/NVPTX/NVPTXAsmPrinter.cpp
@@ -1272,9 +1272,6 @@ void NVPTXAsmPrinter::emitPTXAddressSpace(unsigned int AddressSpace,
 std::string
 NVPTXAsmPrinter::getPTXFundamentalTypeStr(Type *Ty, bool useB4PTR) const {
   switch (Ty->getTypeID()) {
-  default:
-    llvm_unreachable("unexpected type");
-    break;
   case Type::IntegerTyID: {
     unsigned NumBits = cast<IntegerType>(Ty)->getBitWidth();
     if (NumBits == 1)
@@ -1305,9 +1302,10 @@ NVPTXAsmPrinter::getPTXFundamentalTypeStr(Type *Ty, bool useB4PTR) const {
       return "b32";
     else
       return "u32";
+  default:
+    break;
   }
   llvm_unreachable("unexpected type");
-  return nullptr;
 }
 
 void NVPTXAsmPrinter::emitPTXGlobalVariable(const GlobalVariable *GVar,
diff --git a/llvm/lib/Transforms/Instrumentation/MemorySanitizer.cpp b/llvm/lib/Transforms/Instrumentation/MemorySanitizer.cpp
index fcf7f470b3e1..c30c1938c381 100644
--- a/llvm/lib/Transforms/Instrumentation/MemorySanitizer.cpp
+++ b/llvm/lib/Transforms/Instrumentation/MemorySanitizer.cpp
@@ -383,6 +383,14 @@ static const MemoryMapParams Linux_X86_64_MemoryMapParams = {
 #endif
 };
 
+// loongarch64 Linux
+static const MemoryMapParams Linux_LOONGARCH64_MemoryMapParams = {
+  0,               // AndMask (not used)
+  0x008000000000,  // XorMask
+  0,               // ShadowBase (not used)
+  0x002000000000,  // OriginBase
+};
+
 // mips64 Linux
 static const MemoryMapParams Linux_MIPS64_MemoryMapParams = {
   0,               // AndMask (not used)
@@ -444,6 +452,11 @@ static const PlatformMemoryMapParams Linux_X86_MemoryMapParams = {
   &Linux_X86_64_MemoryMapParams,
 };
 
+static const PlatformMemoryMapParams Linux_LOONGARCH_MemoryMapParams = {
+  nullptr,
+  &Linux_LOONGARCH64_MemoryMapParams,
+};
+
 static const PlatformMemoryMapParams Linux_MIPS_MemoryMapParams = {
   nullptr,
   &Linux_MIPS64_MemoryMapParams,
@@ -501,6 +514,7 @@ public:
 private:
   friend struct MemorySanitizerVisitor;
   friend struct VarArgAMD64Helper;
+  friend struct VarArgLoongArch64Helper;
   friend struct VarArgMIPS64Helper;
   friend struct VarArgAArch64Helper;
   friend struct VarArgPowerPC64Helper;
@@ -937,6 +951,9 @@ void MemorySanitizer::initializeModule(Module &M) {
           case Triple::x86:
             MapParams = Linux_X86_MemoryMapParams.bits32;
             break;
+          case Triple::loongarch64:
+            MapParams = Linux_LOONGARCH_MemoryMapParams.bits64;
+            break;
           case Triple::mips64:
           case Triple::mips64el:
             MapParams = Linux_MIPS_MemoryMapParams.bits64;
@@ -4189,6 +4206,123 @@ struct VarArgAMD64Helper : public VarArgHelper {
   }
 };
 
+/// LoongArch64-specific implementation of VarArgHelper.
+struct VarArgLoongArch64Helper : public VarArgHelper {
+  Function &F;
+  MemorySanitizer &MS;
+  MemorySanitizerVisitor &MSV;
+  Value *VAArgTLSCopy = nullptr;
+  Value *VAArgSize = nullptr;
+
+  SmallVector<CallInst*, 16> VAStartInstrumentationList;
+
+  VarArgLoongArch64Helper(Function &F, MemorySanitizer &MS,
+                    MemorySanitizerVisitor &MSV) : F(F), MS(MS), MSV(MSV) {}
+
+  void visitCallBase(CallBase &CB, IRBuilder<> &IRB) override {
+    unsigned VAArgOffset = 0;
+    const DataLayout &DL = F.getParent()->getDataLayout();
+    for (auto ArgIt = CB.arg_begin() + CB.getFunctionType()->getNumParams(),
+              End = CB.arg_end();
+         ArgIt != End; ++ArgIt) {
+      Triple TargetTriple(F.getParent()->getTargetTriple());
+      Value *A = *ArgIt;
+      Value *Base;
+      uint64_t ArgSize = DL.getTypeAllocSize(A->getType());
+      if (TargetTriple.getArch() == Triple::loongarch64) {
+        // Adjusting the shadow for argument with size < 8 to match the placement
+        // of bits in big endian system
+        if (ArgSize < 8)
+          VAArgOffset += (8 - ArgSize);
+      }
+      Base = getShadowPtrForVAArgument(A->getType(), IRB, VAArgOffset, ArgSize);
+      VAArgOffset += ArgSize;
+      VAArgOffset = alignTo(VAArgOffset, 8);
+      if (!Base)
+        continue;
+      IRB.CreateAlignedStore(MSV.getShadow(A), Base, kShadowTLSAlignment);
+    }
+
+    Constant *TotalVAArgSize = ConstantInt::get(IRB.getInt64Ty(), VAArgOffset);
+    // Here using VAArgOverflowSizeTLS as VAArgSizeTLS to avoid creation of
+    // a new class member i.e. it is the total size of all VarArgs.
+    IRB.CreateStore(TotalVAArgSize, MS.VAArgOverflowSizeTLS);
+  }
+
+  /// Compute the shadow address for a given va_arg.
+  Value *getShadowPtrForVAArgument(Type *Ty, IRBuilder<> &IRB,
+                                   unsigned ArgOffset, unsigned ArgSize) {
+    // Make sure we don't overflow __msan_va_arg_tls.
+    if (ArgOffset + ArgSize > kParamTLSSize)
+      return nullptr;
+    Value *Base = IRB.CreatePointerCast(MS.VAArgTLS, MS.IntptrTy);
+    Base = IRB.CreateAdd(Base, ConstantInt::get(MS.IntptrTy, ArgOffset));
+    return IRB.CreateIntToPtr(Base, PointerType::get(MSV.getShadowTy(Ty), 0),
+                              "_msarg");
+  }
+
+  void visitVAStartInst(VAStartInst &I) override {
+    IRBuilder<> IRB(&I);
+    VAStartInstrumentationList.push_back(&I);
+    Value *VAListTag = I.getArgOperand(0);
+    Value *ShadowPtr, *OriginPtr;
+    const Align Alignment = Align(8);
+    std::tie(ShadowPtr, OriginPtr) = MSV.getShadowOriginPtr(
+        VAListTag, IRB, IRB.getInt8Ty(), Alignment, /*isStore*/ true);
+    IRB.CreateMemSet(ShadowPtr, Constant::getNullValue(IRB.getInt8Ty()),
+                     /* size */ 8, Alignment, false);
+  }
+
+  void visitVACopyInst(VACopyInst &I) override {
+    IRBuilder<> IRB(&I);
+    VAStartInstrumentationList.push_back(&I);
+    Value *VAListTag = I.getArgOperand(0);
+    Value *ShadowPtr, *OriginPtr;
+    const Align Alignment = Align(8);
+    std::tie(ShadowPtr, OriginPtr) = MSV.getShadowOriginPtr(
+        VAListTag, IRB, IRB.getInt8Ty(), Alignment, /*isStore*/ true);
+    IRB.CreateMemSet(ShadowPtr, Constant::getNullValue(IRB.getInt8Ty()),
+                     /* size */ 8, Alignment, false);
+  }
+
+  void finalizeInstrumentation() override {
+    assert(!VAArgSize && !VAArgTLSCopy &&
+           "finalizeInstrumentation called twice");
+    IRBuilder<> IRB(MSV.ActualFnStart->getFirstNonPHI());
+    VAArgSize = IRB.CreateLoad(IRB.getInt64Ty(), MS.VAArgOverflowSizeTLS);
+    Value *CopySize = IRB.CreateAdd(ConstantInt::get(MS.IntptrTy, 0),
+                                    VAArgSize);
+
+    if (!VAStartInstrumentationList.empty()) {
+      // If there is a va_start in this function, make a backup copy of
+      // va_arg_tls somewhere in the function entry block.
+      VAArgTLSCopy = IRB.CreateAlloca(Type::getInt8Ty(*MS.C), CopySize);
+      IRB.CreateMemCpy(VAArgTLSCopy, Align(8), MS.VAArgTLS, Align(8), CopySize);
+    }
+
+    // Instrument va_start.
+    // Copy va_list shadow from the backup copy of the TLS contents.
+    for (size_t i = 0, n = VAStartInstrumentationList.size(); i < n; i++) {
+      CallInst *OrigInst = VAStartInstrumentationList[i];
+      IRBuilder<> IRB(OrigInst->getNextNode());
+      Value *VAListTag = OrigInst->getArgOperand(0);
+      Type *RegSaveAreaPtrTy = Type::getInt64PtrTy(*MS.C);
+      Value *RegSaveAreaPtrPtr =
+          IRB.CreateIntToPtr(IRB.CreatePtrToInt(VAListTag, MS.IntptrTy),
+                             PointerType::get(RegSaveAreaPtrTy, 0));
+      Value *RegSaveAreaPtr =
+          IRB.CreateLoad(RegSaveAreaPtrTy, RegSaveAreaPtrPtr);
+      Value *RegSaveAreaShadowPtr, *RegSaveAreaOriginPtr;
+      const Align Alignment = Align(8);
+      std::tie(RegSaveAreaShadowPtr, RegSaveAreaOriginPtr) =
+          MSV.getShadowOriginPtr(RegSaveAreaPtr, IRB, IRB.getInt8Ty(),
+                                 Alignment, /*isStore*/ true);
+      IRB.CreateMemCpy(RegSaveAreaShadowPtr, Alignment, VAArgTLSCopy, Alignment,
+                       CopySize);
+    }
+  }
+};
+
 /// MIPS64-specific implementation of VarArgHelper.
 struct VarArgMIPS64Helper : public VarArgHelper {
   Function &F;
diff --git a/llvm/test/CodeGen/LoongArch/bstrins_d.ll b/llvm/test/CodeGen/LoongArch/bstrins_d.ll
new file mode 100644
index 000000000000..9b2846723a4a
--- /dev/null
+++ b/llvm/test/CodeGen/LoongArch/bstrins_d.ll
@@ -0,0 +1,40 @@
+; RUN: llc -march=loongarch64 -o - %s | FileCheck %s
+
+define void @bstrinsd_63_27(i64* nocapture %d) nounwind {
+; CHECK-LABEL: bstrinsd_63_27:
+; CHECK:    addi.d $r[[REG1:[0-9]+]], $zero, 123
+; CHECK:    bstrins.d $r[[REG2:[0-9]+]], $r[[REG1:[0-9]+]], 63, 27
+entry:
+  %tmp = load i64, i64* %d, align 8
+  %and5 = and i64 %tmp, 134217727
+  %or = or i64 %and5, 16508780544
+  store i64 %or, i64* %d, align 8
+  ret void
+}
+
+define void @bstrinsd_33_28(i64* nocapture %d) nounwind {
+; CHECK-LABEL: bstrinsd_33_28:
+; CHECK:    addi.d $r[[REG1:[0-9]+]], $zero, 4
+; CHECK:    bstrins.d $r[[REG2:[0-9]+]], $r[[REG1:[0-9]+]], 33, 28
+entry:
+  %tmp = load i64, i64* %d, align 8
+  %and5 = and i64 %tmp, -16911433729
+  %or = or i64 %and5, 1073741824
+  store i64 %or, i64* %d, align 8
+  ret void
+}
+
+define void @bstrinsd_49_34(i64* nocapture %d) nounwind {
+; CHECK-LABEL: bstrinsd_49_34:
+; CHECK:    srli.d $r[[REG1:[0-9]+]], $r[[REG2:[0-9]+]], 50
+; CHECK:    bstrins.d $r[[REG2:[0-9]+]], $r[[REG1:[0-9]+]], 49, 34
+entry:
+  %tmp0 = load i64, i64* %d, align 8
+  %lshr = lshr i64 %tmp0, 50
+  %tmp1 = load i64, i64* %d, align 8
+  %shl = shl nuw nsw i64 %lshr, 34
+  %and = and i64 %tmp1, -1125882726973441
+  %or = or i64 %and, %shl
+  store i64 %or, i64* %d, align 8
+  ret void
+}
diff --git a/llvm/test/CodeGen/LoongArch/bstrins_w.ll b/llvm/test/CodeGen/LoongArch/bstrins_w.ll
new file mode 100644
index 000000000000..3b62a760e81b
--- /dev/null
+++ b/llvm/test/CodeGen/LoongArch/bstrins_w.ll
@@ -0,0 +1,28 @@
+; RUN: llc -march=loongarch64 -o - %s | FileCheck %s
+
+define void @bstrins_w(i32 %s, i32* nocapture %d) nounwind {
+; CHECK-LABEL: bstrins_w:
+; CHECK:       bstrins.w $r[[REG2:[0-9]+]], $r[[REG1:[0-9]+]], 13, 5
+entry:
+  %and = shl i32 %s, 5
+  %shl = and i32 %and, 16352
+  %tmp3 = load i32, i32* %d, align 4
+  %and5 = and i32 %tmp3, -16353
+  %or = or i32 %and5, %shl
+  store i32 %or, i32* %d, align 4
+  ret void
+}
+
+define i32 @no_bstrinsw(i32* nocapture %d) {
+; CHECK-LABEL: no_bstrinsw:
+; CHECK:       addi.w $r[[REG2:[0-9]+]], $zero, -4
+; CHECK:       and $r[[REG1:[0-9]+]], $r[[REG1:[0-9]+]], $r[[REG2:[0-9]+]]
+; CHECK:       ori $r[[REG2:[0-9]+]], $r[[REG1:[0-9]+]], 8
+; CHECK-NOT:   bstrins.w {{[[:space:]].*}}
+entry:
+  %tmp = load volatile i32, i32* %d, align 4
+  %and = and i32 %tmp, -4
+  %or = or i32 %and, 8
+  store volatile i32 %or, i32* %d, align 4
+  ret i32 %and
+}
diff --git a/llvm/test/CodeGen/LoongArch/bstrpick_d.ll b/llvm/test/CodeGen/LoongArch/bstrpick_d.ll
new file mode 100644
index 000000000000..e1169cb21fc1
--- /dev/null
+++ b/llvm/test/CodeGen/LoongArch/bstrpick_d.ll
@@ -0,0 +1,64 @@
+; RUN: llc -march=loongarch64 -o - %s | FileCheck %s
+
+define i64 @bstrpickd_add_zext(i32 signext %n) {
+entry:
+  %add = add i32 %n, 1
+  %res = zext i32 %add to i64
+  ret i64 %res
+
+; CHECK-LABEL: bstrpickd_add_zext:
+; CHECK:       bstrpick.d $r[[REG:[0-9]+]], $r[[REG:[0-9]+]], 31, 0
+
+}
+
+define i64 @bstrpickd_and12(i64 zeroext %a) {
+entry:
+  %and = and i64 %a, 4095
+  ret i64 %and
+
+; CHECK-LABEL: bstrpickd_and12:
+; CHECK:       andi $r[[REG:[0-9]+]], $r[[REG:[0-9]+]], 4095
+
+}
+
+define i64 @bstrpickd_and13(i64 zeroext %a) {
+entry:
+  %and = and i64 %a, 8191
+  ret i64 %and
+
+; CHECK-LABEL: bstrpickd_and13:
+; CHECK:       bstrpick.d $r[[REG:[0-9]+]], $r[[REG:[0-9]+]], 12, 0
+
+}
+
+define i64 @bstrpickd_lsr_and8(i64 zeroext %a) {
+entry:
+  %shr = lshr i64 %a, 40
+  %and = and i64 %shr, 255
+  ret i64 %and
+
+; CHECK-LABEL: bstrpickd_lsr_and8:
+; CHECK:       bstrpick.d $r[[REG:[0-9]+]], $r[[REG:[0-9]+]], 47, 40
+
+}
+
+define i64 @bstrpickd_zext(i32 signext %a) {
+entry:
+  %conv = zext i32 %a to i64
+  ret i64 %conv
+
+; CHECK-LABEL: bstrpickd_zext:
+; CHECK:       bstrpick.d $r[[REG:[0-9]+]], $r[[REG:[0-9]+]], 31, 0
+
+}
+
+define i64 @bstrpickd_and_lsr(i64 zeroext %n) {
+entry:
+  %and = lshr i64 %n, 8
+  %shr = and i64 %and, 4095
+  ret i64 %shr
+
+; CHECK-LABEL: bstrpickd_and_lsr:
+; CHECK:       bstrpick.d $r[[REG:[0-9]+]], $r[[REG:[0-9]+]], 19, 8
+
+}
diff --git a/llvm/test/CodeGen/LoongArch/bstrpick_w.ll b/llvm/test/CodeGen/LoongArch/bstrpick_w.ll
new file mode 100644
index 000000000000..e60de4737806
--- /dev/null
+++ b/llvm/test/CodeGen/LoongArch/bstrpick_w.ll
@@ -0,0 +1,18 @@
+; RUN: llc -march=loongarch64 -o - %s | FileCheck %s
+
+define i32 @bstrpickw_and24(i32 signext %a) {
+; CHECK-LABEL: bstrpickw_and24:
+; CHECK:       bstrpick.w $r[[REG:[0-9]+]], $r[[REG:[0-9]+]], 23, 0
+entry:
+  %and = and i32 %a, 16777215
+  ret i32 %and
+}
+
+define i32 @bstrpickw_lshr_and(i32 %s, i32 %pos, i32 %sz) nounwind readnone {
+; CHECK-LABEL: bstrpickw_lshr_and:
+; CHECK:       bstrpick.w $r[[REG:[0-9]+]], $r[[REG:[0-9]+]], 13, 5
+entry:
+  %shr = lshr i32 %s, 5
+  %and = and i32 %shr, 511
+  ret i32 %and
+}
diff --git a/llvm/test/CodeGen/LoongArch/builtins-loongarch-base.ll b/llvm/test/CodeGen/LoongArch/builtins-loongarch-base.ll
new file mode 100644
index 000000000000..415530821a66
--- /dev/null
+++ b/llvm/test/CodeGen/LoongArch/builtins-loongarch-base.ll
@@ -0,0 +1,752 @@
+; Test the base intrinsics.
+; RUN: llc -march=loongarch64 -o - %s | FileCheck %s
+
+define void @cpucfg() {
+entry:
+  %u32_r = alloca i32, align 4
+  %u32_a = alloca i32, align 4
+  %0 = load i32, i32* %u32_a, align 4
+  %1 = call i32 @llvm.loongarch.cpucfg(i32 %0)
+  store i32 %1, i32* %u32_r, align 4
+  ret void
+}
+
+declare i32 @llvm.loongarch.cpucfg(i32)
+
+; CHECK-LABEL: cpucfg:
+; CHECK: ld.w	$r[[REG:[0-9]+]], $sp, 8
+; CHECK: cpucfg	$r[[REG:[0-9]+]], $r[[REG:[0-9]+]]
+; CHECK: st.w	$r[[REG:[0-9]+]], $sp, 12
+; CHECK: jr	$ra
+;
+
+define void @csrrd() {
+entry:
+  %u32_r = alloca i32, align 4
+  %0 = call i32 @llvm.loongarch.csrrd(i32 1)
+  store i32 %0, i32* %u32_r, align 4
+  ret void
+}
+
+declare i32 @llvm.loongarch.csrrd(i32)
+
+; CHECK-LABEL: csrrd:
+; CHECK: csrrd	$r[[REG:[0-9]+]], 1
+; CHECK: st.w	$r[[REG:[0-9]+]], $sp, 12
+; CHECK: jr	$ra
+;
+
+define void @dcsrrd() {
+entry:
+  %u64_r = alloca i64, align 8
+  %0 = call i64 @llvm.loongarch.dcsrrd(i64 1)
+  store i64 %0, i64* %u64_r, align 8
+  ret void
+}
+
+declare i64 @llvm.loongarch.dcsrrd(i64)
+
+; CHECK-LABEL: dcsrrd:
+; CHECK: csrrd	$r[[REG:[0-9]+]], 1
+; CHECK: st.d	$r[[REG:[0-9]+]], $sp, 8
+; CHECK: jr	$ra
+;
+
+define void @csrwr() {
+entry:
+  %u32_r = alloca i32, align 4
+  %u32_a = alloca i32, align 4
+  %0 = load i32, i32* %u32_a, align 4
+  %1 = call i32 @llvm.loongarch.csrwr(i32 %0, i32 1)
+  store i32 %1, i32* %u32_r, align 4
+  ret void
+}
+
+declare i32 @llvm.loongarch.csrwr(i32, i32)
+
+; CHECK-LABEL: csrwr:
+; CHECK: ld.w	$r[[REG:[0-9]+]], $sp, 8
+; CHECK: csrwr	$r[[REG:[0-9]+]], 1
+; CHECK: st.w	$r[[REG:[0-9]+]], $sp, 12
+; CHECK: jr	$ra
+;
+
+define void @dcsrwr() {
+entry:
+  %u64_r = alloca i64, align 8
+  %u64_a = alloca i64, align 8
+  %0 = load i64, i64* %u64_a, align 8
+  %1 = call i64 @llvm.loongarch.dcsrwr(i64 %0, i64 1)
+  store i64 %1, i64* %u64_r, align 8
+  ret void
+}
+
+declare i64 @llvm.loongarch.dcsrwr(i64, i64)
+
+; CHECK-LABEL: dcsrwr:
+; CHECK: ld.d	$r[[REG:[0-9]+]], $sp, 0
+; CHECK: csrwr	$r[[REG:[0-9]+]], 1
+; CHECK: st.d	$r[[REG:[0-9]+]], $sp, 8
+; CHECK: jr	$ra
+;
+
+define void @csrxchg() {
+entry:
+  %u32_r = alloca i32, align 4
+  %u32_a = alloca i32, align 4
+  %u32_b = alloca i32, align 4
+  %0 = load i32, i32* %u32_a, align 4
+  %1 = load i32, i32* %u32_b, align 4
+  %2 = call i32 @llvm.loongarch.csrxchg(i32 %0, i32 %1, i32 1)
+  store i32 %2, i32* %u32_r, align 4
+  ret void
+}
+
+declare i32 @llvm.loongarch.csrxchg(i32, i32, i32)
+
+; CHECK-LABEL: csrxchg:
+; CHECK: ld.w	$r[[REG1:[0-9]+]], $sp, 4
+; CHECK: ld.w	$r[[REG2:[0-9]+]], $sp, 8
+; CHECK: csrxchg	$r[[REG1:[0-9]+]], $r[[REG2:[0-9]+]], 1
+; CHECK: st.w	$r[[REG1:[0-9]+]], $sp, 12
+; CHECK: jr	$ra
+;
+
+define void @dcsrxchg() {
+entry:
+  %u64_r = alloca i64, align 8
+  %u64_a = alloca i64, align 8
+  %u64_b = alloca i64, align 8
+  %0 = load i64, i64* %u64_a, align 8
+  %1 = load i64, i64* %u64_b, align 8
+  %2 = call i64 @llvm.loongarch.dcsrxchg(i64 %0, i64 %1, i64 1)
+  store i64 %2, i64* %u64_r, align 8
+  ret void
+}
+
+declare i64 @llvm.loongarch.dcsrxchg(i64, i64, i64)
+
+; CHECK-LABEL: dcsrxchg:
+; CHECK: ld.d	$r[[REG1:[0-9]+]], $sp, 8
+; CHECK: ld.d	$r[[REG2:[0-9]+]], $sp, 16
+; CHECK: csrxchg	$r[[REG1:[0-9]+]], $r[[REG2:[0-9]+]], 1
+; CHECK: st.d	$r[[REG1:[0-9]+]], $sp, 24
+; CHECK: jr	$ra
+;
+
+define void @iocsrrd_b() {
+entry:
+  %u32_a = alloca i32, align 4
+  %u8_r = alloca i8, align 1
+  %0 = load i32, i32* %u32_a, align 4
+  %1 = call i32 @llvm.loongarch.iocsrrd.b(i32 %0)
+  %conv = trunc i32 %1 to i8
+  store i8 %conv, i8* %u8_r, align 1
+  ret void
+}
+
+declare i32 @llvm.loongarch.iocsrrd.b(i32)
+
+; CHECK-LABEL: iocsrrd_b:
+; CHECK: ld.w	$r[[REG:[0-9]+]], $sp, 12
+; CHECK: iocsrrd.b	$r[[REG:[0-9]+]], $r[[REG:[0-9]+]]
+; CHECK: st.b	$r[[REG:[0-9]+]], $sp, 8
+; CHECK: jr	$ra
+;
+
+define void @iocsrrd_h() {
+entry:
+  %u32_a = alloca i32, align 4
+  %u16_r = alloca i16, align 2
+  %0 = load i32, i32* %u32_a, align 4
+  %1 = call i32 @llvm.loongarch.iocsrrd.h(i32 %0)
+  %conv = trunc i32 %1 to i16
+  store i16 %conv, i16* %u16_r, align 2
+  ret void
+}
+
+declare i32 @llvm.loongarch.iocsrrd.h(i32)
+
+; CHECK-LABEL: iocsrrd_h:
+; CHECK: ld.w	$r[[REG:[0-9]+]], $sp, 12
+; CHECK: iocsrrd.h	$r[[REG:[0-9]+]], $r[[REG:[0-9]+]]
+; CHECK: st.h	$r[[REG:[0-9]+]], $sp, 8
+; CHECK: jr	$ra
+;
+
+define void @iocsrrd_w() {
+entry:
+  %u32_r = alloca i32, align 4
+  %u32_a = alloca i32, align 4
+  %0 = load i32, i32* %u32_a, align 4
+  %1 = call i32 @llvm.loongarch.iocsrrd.w(i32 %0)
+  store i32 %1, i32* %u32_r, align 4
+  ret void
+}
+
+declare i32 @llvm.loongarch.iocsrrd.w(i32)
+
+; CHECK-LABEL: iocsrrd_w:
+; CHECK: ld.w	$r[[REG:[0-9]+]], $sp, 8
+; CHECK: iocsrrd.w	$r[[REG:[0-9]+]], $r[[REG:[0-9]+]]
+; CHECK: st.w	$r[[REG:[0-9]+]], $sp, 12
+; CHECK: jr	$ra
+;
+
+define void @iocsrrd_d() {
+entry:
+  %u32_a = alloca i32, align 4
+  %u64_r = alloca i64, align 8
+  %0 = load i32, i32* %u32_a, align 4
+  %1 = call i64 @llvm.loongarch.iocsrrd.d(i32 %0)
+  store i64 %1, i64* %u64_r, align 8
+  ret void
+}
+
+declare i64 @llvm.loongarch.iocsrrd.d(i32)
+
+; CHECK-LABEL: iocsrrd_d:
+; CHECK: ld.w	$r[[REG:[0-9]+]], $sp, 12
+; CHECK: iocsrrd.d	$r[[REG:[0-9]+]], $r[[REG:[0-9]+]]
+; CHECK: st.d	$r[[REG:[0-9]+]], $sp, 0
+; CHECK: jr	$ra
+;
+
+define void @iocsrwr_b() {
+entry:
+  %u32_a = alloca i32, align 4
+  %u8_a = alloca i8, align 1
+  %0 = load i8, i8* %u8_a, align 1
+  %conv = zext i8 %0 to i32
+  %1 = load i32, i32* %u32_a, align 4
+  call void @llvm.loongarch.iocsrwr.b(i32 %conv, i32 %1)
+  ret void
+}
+
+declare void @llvm.loongarch.iocsrwr.b(i32, i32)
+
+; CHECK-LABEL: iocsrwr_b:
+; CHECK: ld.w	$r[[REG1:[0-9]+]], $sp, 12
+; CHECK: ld.bu	$r[[REG2:[0-9]+]], $sp, 8
+; CHECK: iocsrwr.b	$r[[REG2:[0-9]+]], $r[[REG1:[0-9]+]]
+; CHECK: jr	$ra
+;
+
+define void @iocsrwr_h() {
+entry:
+  %u32_a = alloca i32, align 4
+  %u16_a = alloca i16, align 2
+  %0 = load i16, i16* %u16_a, align 2
+  %conv = zext i16 %0 to i32
+  %1 = load i32, i32* %u32_a, align 4
+  call void @llvm.loongarch.iocsrwr.h(i32 %conv, i32 %1)
+  ret void
+}
+
+declare void @llvm.loongarch.iocsrwr.h(i32, i32)
+
+; CHECK-LABEL: iocsrwr_h:
+; CHECK: ld.w	$r[[REG1:[0-9]+]], $sp, 12
+; CHECK: ld.hu	$r[[REG2:[0-9]+]], $sp, 8
+; CHECK: iocsrwr.h	$r[[REG2:[0-9]+]], $r[[REG1:[0-9]+]]
+; CHECK: jr	$ra
+;
+
+define void @iocsrwr_w() {
+entry:
+  %u32_a = alloca i32, align 4
+  %u32_b = alloca i32, align 4
+  %0 = load i32, i32* %u32_a, align 4
+  %1 = load i32, i32* %u32_b, align 4
+  call void @llvm.loongarch.iocsrwr.w(i32 %0, i32 %1)
+  ret void
+}
+
+declare void @llvm.loongarch.iocsrwr.w(i32, i32)
+
+; CHECK-LABEL: iocsrwr_w:
+; CHECK: ld.w	$r[[REG1:[0-9]+]], $sp, 8
+; CHECK: ld.w	$r[[REG2:[0-9]+]], $sp, 12
+; CHECK: iocsrwr.w	$r[[REG2:[0-9]+]], $r[[REG1:[0-9]+]]
+; CHECK: jr	$ra
+;
+
+define void @iocsrwr_d() {
+entry:
+  %u32_a = alloca i32, align 4
+  %u64_a = alloca i64, align 8
+  %0 = load i64, i64* %u64_a, align 8
+  %1 = load i32, i32* %u32_a, align 4
+  call void @llvm.loongarch.iocsrwr.d(i64 %0, i32 %1)
+  ret void
+}
+
+declare void @llvm.loongarch.iocsrwr.d(i64, i32)
+
+; CHECK-LABEL: iocsrwr_d:
+; CHECK: ld.w	$r[[REG1:[0-9]+]], $sp, 12
+; CHECK: ld.d	$r[[REG2:[0-9]+]], $sp, 0
+; CHECK: iocsrwr.d	$r[[REG2:[0-9]+]], $r[[REG1:[0-9]+]]
+; CHECK: jr	$ra
+;
+
+define void @cacop() {
+entry:
+  %i32_a = alloca i32, align 4
+  %0 = load i32, i32* %i32_a, align 4
+  call void @llvm.loongarch.cacop(i32 1, i32 %0, i32 2)
+  ret void
+}
+
+declare void @llvm.loongarch.cacop(i32, i32, i32)
+
+; CHECK-LABEL: cacop:
+; CHECK: ld.w	$r[[REG:[0-9]+]], $sp, 12
+; CHECK: cacop	1, $r[[REG:[0-9]+]], 2
+; CHECK: jr	$ra
+;
+
+define void @dcacop() {
+entry:
+  %i64_a = alloca i64, align 8
+  %0 = load i64, i64* %i64_a, align 8
+  call void @llvm.loongarch.dcacop(i32 1, i64 %0, i64 2)
+  ret void
+}
+
+declare void @llvm.loongarch.dcacop(i32, i64, i64)
+
+; CHECK-LABEL: dcacop:
+; CHECK: ld.d	$r[[REG:[0-9]+]], $sp, 8
+; CHECK: cacop	1, $r[[REG:[0-9]+]], 2
+; CHECK: jr	$ra
+;
+
+define void @rdtime_d() {
+entry:
+  %value = alloca i64, align 8
+  %timeid = alloca i64, align 8
+  %0 = call { i64, i64 } asm sideeffect "rdtime.d\09$0,$1\0A\09", "=&r,=&r"() nounwind
+  %asmresult0 = extractvalue { i64, i64 } %0, 0
+  %asmresult1 = extractvalue { i64, i64 } %0, 1
+  store i64 %asmresult0, i64* %value, align 8
+  store i64 %asmresult1, i64* %timeid, align 8
+  ret void
+}
+
+; CHECK-LABEL: rdtime_d:
+; CHECK: rdtime.d	$r[[REG1:[0-9]+]], $r[[REG2:[0-9]+]]
+; CHECK: st.d	$r[[REG2:[0-9]+]], $sp, 8
+; CHECK: st.d	$r[[REG1:[0-9]+]], $sp, 0
+; CHECK: jr	$ra
+;
+
+define void @rdtimeh_w() {
+entry:
+  %value = alloca i32, align 4
+  %timeid = alloca i32, align 4
+  %0 = call { i32, i32 } asm sideeffect "rdtimeh.w\09$0,$1\0A\09", "=&r,=&r"() nounwind
+  %asmresult0 = extractvalue { i32, i32 } %0, 0
+  %asmresult1 = extractvalue { i32, i32 } %0, 1
+  store i32 %asmresult0, i32* %value, align 4
+  store i32 %asmresult1, i32* %timeid, align 4
+  ret void
+}
+
+; CHECK-LABEL: rdtimeh_w:
+; CHECK: rdtimeh.w	$r[[REG1:[0-9]+]], $r[[REG2:[0-9]+]]
+; CHECK: st.w	$r[[REG2:[0-9]+]], $sp, 12
+; CHECK: st.w	$r[[REG1:[0-9]+]], $sp, 8
+; CHECK: jr	$ra
+;
+
+define void @rdtimel_w() {
+entry:
+  %value = alloca i32, align 4
+  %timeid = alloca i32, align 4
+  %0 = call { i32, i32 } asm sideeffect "rdtimel.w\09$0,$1\0A\09", "=&r,=&r"() nounwind
+  %asmresult0 = extractvalue { i32, i32 } %0, 0
+  %asmresult1 = extractvalue { i32, i32 } %0, 1
+  store i32 %asmresult0, i32* %value, align 4
+  store i32 %asmresult1, i32* %timeid, align 4
+  ret void
+}
+
+; CHECK-LABEL: rdtimel_w:
+; CHECK: rdtimel.w	$r[[REG1:[0-9]+]], $r[[REG2:[0-9]+]]
+; CHECK: st.w	$r[[REG2:[0-9]+]], $sp, 12
+; CHECK: st.w	$r[[REG1:[0-9]+]], $sp, 8
+; CHECK: jr	$ra
+;
+
+define void @crc_w_b_w() {
+entry:
+  %i32_r = alloca i32, align 4
+  %i32_a = alloca i32, align 4
+  %i8_a = alloca i8, align 1
+  %0 = load i8, i8* %i8_a, align 1
+  %conv = sext i8 %0 to i32
+  %1 = load i32, i32* %i32_a, align 4
+  %2 = call i32 @llvm.loongarch.crc.w.b.w(i32 %conv, i32 %1)
+  store i32 %2, i32* %i32_r, align 4
+  ret void
+}
+
+declare i32 @llvm.loongarch.crc.w.b.w(i32, i32)
+
+; CHECK-LABEL: crc_w_b_w:
+; CHECK: ld.w	$r[[REG1:[0-9]+]], $sp, 8
+; CHECK: ld.b	$r[[REG2:[0-9]+]], $sp, 4
+; CHECK: crc.w.b.w	$r[[REG1:[0-9]+]], $r[[REG2:[0-9]+]], $r[[REG1:[0-9]+]]
+; CHECK: jr	$ra
+;
+
+define void @crc_w_h_w() {
+entry:
+  %i32_r = alloca i32, align 4
+  %i32_a = alloca i32, align 4
+  %i16_a = alloca i16, align 2
+  %0 = load i16, i16* %i16_a, align 2
+  %conv = sext i16 %0 to i32
+  %1 = load i32, i32* %i32_a, align 4
+  %2 = call i32 @llvm.loongarch.crc.w.h.w(i32 %conv, i32 %1)
+  store i32 %2, i32* %i32_r, align 4
+  ret void
+}
+
+declare i32 @llvm.loongarch.crc.w.h.w(i32, i32)
+
+; CHECK-LABEL: crc_w_h_w:
+; CHECK: ld.w	$r[[REG1:[0-9]+]], $sp, 8
+; CHECK: ld.h	$r[[REG2:[0-9]+]], $sp, 4
+; CHECK: crc.w.h.w	$r[[REG1:[0-9]+]], $r[[REG2:[0-9]+]], $r[[REG1:[0-9]+]]
+; CHECK: jr	$ra
+;
+
+define void @crc_w_w_w() {
+entry:
+  %i32_r = alloca i32, align 4
+  %i32_a = alloca i32, align 4
+  %i32_b = alloca i32, align 4
+  %0 = load i32, i32* %i32_a, align 4
+  %1 = load i32, i32* %i32_b, align 4
+  %2 = call i32 @llvm.loongarch.crc.w.w.w(i32 %0, i32 %1)
+  store i32 %2, i32* %i32_r, align 4
+  ret void
+}
+
+declare i32 @llvm.loongarch.crc.w.w.w(i32, i32)
+
+; CHECK-LABEL: crc_w_w_w:
+; CHECK: ld.w	$r[[REG1:[0-9]+]], $sp, 4
+; CHECK: ld.w	$r[[REG2:[0-9]+]], $sp, 8
+; CHECK: crc.w.w.w	$r[[REG1:[0-9]+]], $r[[REG2:[0-9]+]], $r[[REG1:[0-9]+]]
+; CHECK: jr	$ra
+;
+
+define void @crc_w_d_w() {
+entry:
+  %i32_r = alloca i32, align 4
+  %i32_a = alloca i32, align 4
+  %i64_a = alloca i64, align 8
+  %0 = load i64, i64* %i64_a, align 8
+  %1 = load i32, i32* %i32_a, align 4
+  %2 = call i32 @llvm.loongarch.crc.w.d.w(i64 %0, i32 %1)
+  store i32 %2, i32* %i32_r, align 4
+  ret void
+}
+
+declare i32 @llvm.loongarch.crc.w.d.w(i64, i32)
+
+; CHECK-LABEL: crc_w_d_w:
+; CHECK: ld.w	$r[[REG1:[0-9]+]], $sp, 8
+; CHECK: ld.d	$r[[REG2:[0-9]+]], $sp, 0
+; CHECK: crc.w.d.w	$r[[REG1:[0-9]+]], $r[[REG2:[0-9]+]], $r[[REG1:[0-9]+]]
+; CHECK: jr	$ra
+;
+
+define void @crcc_w_b_w() {
+entry:
+  %i32_r = alloca i32, align 4
+  %i32_a = alloca i32, align 4
+  %i8_a = alloca i8, align 1
+  %0 = load i8, i8* %i8_a, align 1
+  %conv = sext i8 %0 to i32
+  %1 = load i32, i32* %i32_a, align 4
+  %2 = call i32 @llvm.loongarch.crcc.w.b.w(i32 %conv, i32 %1)
+  store i32 %2, i32* %i32_r, align 4
+  ret void
+}
+
+declare i32 @llvm.loongarch.crcc.w.b.w(i32, i32)
+
+; CHECK-LABEL: crcc_w_b_w:
+; CHECK: ld.w	$r[[REG1:[0-9]+]], $sp, 8
+; CHECK: ld.b	$r[[REG2:[0-9]+]], $sp, 4
+; CHECK: crcc.w.b.w	$r[[REG1:[0-9]+]], $r[[REG2:[0-9]+]], $r[[REG1:[0-9]+]]
+; CHECK: jr	$ra
+;
+
+define void @crcc_w_h_w() {
+entry:
+  %i32_r = alloca i32, align 4
+  %i32_a = alloca i32, align 4
+  %i16_a = alloca i16, align 2
+  %0 = load i16, i16* %i16_a, align 2
+  %conv = sext i16 %0 to i32
+  %1 = load i32, i32* %i32_a, align 4
+  %2 = call i32 @llvm.loongarch.crcc.w.h.w(i32 %conv, i32 %1)
+  store i32 %2, i32* %i32_r, align 4
+  ret void
+}
+
+declare i32 @llvm.loongarch.crcc.w.h.w(i32, i32)
+
+; CHECK-LABEL: crcc_w_h_w:
+; CHECK: ld.w	$r[[REG1:[0-9]+]], $sp, 8
+; CHECK: ld.h	$r[[REG2:[0-9]+]], $sp, 4
+; CHECK: crcc.w.h.w	$r[[REG1:[0-9]+]], $r[[REG2:[0-9]+]], $r[[REG1:[0-9]+]]
+; CHECK: jr	$ra
+;
+
+define void @crcc_w_w_w() {
+entry:
+  %i32_r = alloca i32, align 4
+  %i32_a = alloca i32, align 4
+  %i32_b = alloca i32, align 4
+  %0 = load i32, i32* %i32_a, align 4
+  %1 = load i32, i32* %i32_b, align 4
+  %2 = call i32 @llvm.loongarch.crcc.w.w.w(i32 %0, i32 %1)
+  store i32 %2, i32* %i32_r, align 4
+  ret void
+}
+
+declare i32 @llvm.loongarch.crcc.w.w.w(i32, i32)
+
+; CHECK-LABEL: crcc_w_w_w:
+; CHECK: ld.w	$r[[REG1:[0-9]+]], $sp, 4
+; CHECK: ld.w	$r[[REG2:[0-9]+]], $sp, 8
+; CHECK: crcc.w.w.w	$r[[REG1:[0-9]+]], $r[[REG2:[0-9]+]], $r[[REG1:[0-9]+]]
+; CHECK: jr	$ra
+;
+
+define void @crcc_w_d_w() {
+entry:
+  %i32_r = alloca i32, align 4
+  %i32_a = alloca i32, align 4
+  %i64_a = alloca i64, align 8
+  %0 = load i64, i64* %i64_a, align 8
+  %1 = load i32, i32* %i32_a, align 4
+  %2 = call i32 @llvm.loongarch.crcc.w.d.w(i64 %0, i32 %1)
+  store i32 %2, i32* %i32_r, align 4
+  ret void
+}
+
+declare i32 @llvm.loongarch.crcc.w.d.w(i64, i32)
+
+; CHECK-LABEL: crcc_w_d_w:
+; CHECK: ld.w	$r[[REG1:[0-9]+]], $sp, 8
+; CHECK: ld.d	$r[[REG2:[0-9]+]], $sp, 0
+; CHECK: crcc.w.d.w	$r[[REG1:[0-9]+]], $r[[REG2:[0-9]+]], $r[[REG1:[0-9]+]]
+; CHECK: jr	$ra
+;
+
+define void @tlbclr() {
+entry:
+  call void @llvm.loongarch.tlbclr()
+  ret void
+}
+
+declare void @llvm.loongarch.tlbclr()
+
+; CHECK-LABEL: tlbclr:
+; CHECK: tlbclr
+; CHECK: jr	$ra
+;
+
+define void @tlbflush() {
+entry:
+  call void @llvm.loongarch.tlbflush()
+  ret void
+}
+
+declare void @llvm.loongarch.tlbflush()
+
+; CHECK-LABEL: tlbflush:
+; CHECK: tlbflush
+; CHECK: jr	$ra
+;
+
+define void @tlbfill() {
+entry:
+  call void @llvm.loongarch.tlbfill()
+  ret void
+}
+
+declare void @llvm.loongarch.tlbfill()
+
+; CHECK-LABEL: tlbfill:
+; CHECK: tlbfill
+; CHECK: jr	$ra
+;
+
+define void @tlbrd() {
+entry:
+  call void @llvm.loongarch.tlbrd()
+  ret void
+}
+
+declare void @llvm.loongarch.tlbrd()
+
+; CHECK-LABEL: tlbrd:
+; CHECK: tlbrd
+; CHECK: jr	$ra
+;
+
+define void @tlbwr() {
+entry:
+  call void @llvm.loongarch.tlbwr()
+  ret void
+}
+
+declare void @llvm.loongarch.tlbwr()
+
+; CHECK-LABEL: tlbwr:
+; CHECK: tlbwr
+; CHECK: jr	$ra
+;
+
+define void @tlbsrch() {
+entry:
+  call void @llvm.loongarch.tlbsrch()
+  ret void
+}
+
+declare void @llvm.loongarch.tlbsrch()
+
+; CHECK-LABEL: tlbsrch:
+; CHECK: tlbsrch
+; CHECK: jr	$ra
+;
+
+define void @syscall() {
+entry:
+  call void @llvm.loongarch.syscall(i64 1)
+  ret void
+}
+
+declare void @llvm.loongarch.syscall(i64)
+
+; CHECK-LABEL: syscall:
+; CHECK: syscall 1
+; CHECK: jr	$ra
+;
+
+define void @break_builtin() {
+entry:
+  call void @llvm.loongarch.break(i64 1)
+  ret void
+}
+
+declare void @llvm.loongarch.break(i64)
+
+; CHECK-LABEL: break_builtin:
+; CHECK: break 1
+; CHECK: jr	$ra
+;
+
+define void @asrtle_d() {
+entry:
+  %i64_a = alloca i64, align 8
+  %i64_b = alloca i64, align 8
+  %0 = load i64, i64* %i64_a, align 8
+  %1 = load i64, i64* %i64_b, align 8
+  call void @llvm.loongarch.asrtle.d(i64 %0, i64 %1)
+  ret void
+}
+
+declare void @llvm.loongarch.asrtle.d(i64, i64)
+
+; CHECK-LABEL: asrtle_d:
+; CHECK: ld.d	$r[[REG1:[0-9]+]], $sp, 0
+; CHECK: ld.d	$r[[REG2:[0-9]+]], $sp, 8
+; CHECK: asrtle.d	$r[[REG2:[0-9]+]], $r[[REG1:[0-9]+]]
+; CHECK: jr	$ra
+;
+
+define void @asrtgt_d() {
+entry:
+  %i64_a = alloca i64, align 8
+  %i64_b = alloca i64, align 8
+  %0 = load i64, i64* %i64_a, align 8
+  %1 = load i64, i64* %i64_b, align 8
+  call void @llvm.loongarch.asrtgt.d(i64 %0, i64 %1)
+  ret void
+}
+
+declare void @llvm.loongarch.asrtgt.d(i64, i64)
+
+; CHECK-LABEL: asrtgt_d:
+; CHECK: ld.d	$r[[REG1:[0-9]+]], $sp, 0
+; CHECK: ld.d	$r[[REG2:[0-9]+]], $sp, 8
+; CHECK: asrtgt.d	$r[[REG2:[0-9]+]], $r[[REG1:[0-9]+]]
+; CHECK: jr	$ra
+;
+
+define void @dbar() {
+entry:
+  call void @llvm.loongarch.dbar(i64 0)
+  ret void
+}
+
+declare void @llvm.loongarch.dbar(i64)
+
+; CHECK-LABEL: dbar:
+; CHECK: dbar 0
+; CHECK: jr	$ra
+;
+
+define void @ibar() {
+entry:
+  call void @llvm.loongarch.ibar(i64 0)
+  ret void
+}
+
+declare void @llvm.loongarch.ibar(i64)
+
+; CHECK-LABEL: ibar:
+; CHECK: ibar 0
+; CHECK: jr	$ra
+;
+
+define void @movfcsr2gr() {
+entry:
+  %u32_r = alloca i32, align 4
+  %rd = alloca i32, align 4
+  %0 = call i32 asm sideeffect "movfcsr2gr $0, $$fcsr0", "=&r"()
+  store i32 %0, i32* %rd, align 4
+  %1 = load i32, i32* %rd, align 4
+  store i32 %1, i32* %u32_r, align 4
+  ret void
+}
+
+; CHECK-LABEL: movfcsr2gr:
+; CHECK: movfcsr2gr	$r[[REG:[0-9]+]], $fcsr[[REG:[0-9]+]]
+; CHECK: st.w	$r[[REG:[0-9]+]], $sp, 8
+; CHECK: st.w	$r[[REG:[0-9]+]], $sp, 12
+; CHECK: jr	$ra
+;
+
+define void @movgr2fcsr() {
+entry:
+  %u32_a = alloca i32, align 4
+  %0 = load i32, i32* %u32_a, align 4
+  call void asm sideeffect "movgr2fcsr $$fcsr0, $0", "r"(i32 %0)
+  ret void
+}
+
+; CHECK-LABEL: movgr2fcsr:
+; CHECK: ld.w	$r[[REG:[0-9]+]], $sp, 12
+; CHECK: movgr2fcsr	$fcsr[[REG:[0-9]+]], $r[[REG:[0-9]+]]
+; CHECK: jr	$ra
+;
diff --git a/llvm/test/CodeGen/LoongArch/divrem.ll b/llvm/test/CodeGen/LoongArch/divrem.ll
new file mode 100644
index 000000000000..34293a83c25c
--- /dev/null
+++ b/llvm/test/CodeGen/LoongArch/divrem.ll
@@ -0,0 +1,68 @@
+; RUN: llc -march=loongarch64 -relocation-model=pic < %s | FileCheck %s -check-prefixes=CHECK,CHECK-TRAP
+
+; RUN: llc -march=loongarch64 -mnocheck-zero-division -relocation-model=pic < %s | FileCheck %s -check-prefixes=CHECK,NOCHECK
+
+; FileCheck Prefixes:
+;   CHECK-TRAP - trap
+;   NOCHECK - Division by zero will not be detected
+
+define i32 @sdiv1(i32 signext %a0, i32 signext %a1) nounwind readnone {
+entry:
+; CHECK-LABEL: sdiv1:
+
+; CHECK:         div.w $r4, $r4, $r5
+; CHECK-TRAP:    bne $r5, $zero, 8
+; CHECK-TRAP:    break 7
+
+; NOCHECK-NOT:   bne
+; NOCHECK-NOT:   break
+
+  %div = sdiv i32 %a0, %a1
+  ret i32 %div
+}
+
+define i32 @srem1(i32 signext %a0, i32 signext %a1) nounwind readnone {
+entry:
+; CHECK-LABEL: srem1:
+
+; CHECK:         mod.w $r4, $r4, $r5
+; CHECK-TRAP:    bne $r5, $zero, 8
+; CHECK-TRAP:    break 7
+
+; NOCHECK-NOT:   bne
+; NOCHECK-NOT:   break
+
+  %rem = srem i32 %a0, %a1
+  ret i32 %rem
+}
+
+define i32 @udiv1(i32 signext %a0, i32 signext %a1) nounwind readnone {
+entry:
+; CHECK-LABEL: udiv1:
+
+; CHECK:         div.wu $r4, $r4, $r5
+; CHECK-TRAP:    bne $r5, $zero, 8
+; CHECK-TRAP:    break 7
+
+; NOCHECK-NOT:   bne
+; NOCHECK-NOT:   break
+
+  %div = udiv i32 %a0, %a1
+  ret i32 %div
+}
+
+define i32 @urem1(i32 signext %a0, i32 signext %a1) nounwind readnone {
+entry:
+; CHECK-LABEL: urem1:
+
+
+; CHECK:         mod.wu $r4, $r4, $r5
+; CHECK-TRAP:    bne $r5, $zero, 8
+; CHECK-TRAP:    break 7
+
+; NOCHECK-NOT:   bne
+; NOCHECK-NOT:   break
+
+  %rem = urem i32 %a0, %a1
+  ret i32 %rem
+}
diff --git a/llvm/test/CodeGen/LoongArch/eliminateFI.ll b/llvm/test/CodeGen/LoongArch/eliminateFI.ll
new file mode 100644
index 000000000000..0272c95bdb9e
--- /dev/null
+++ b/llvm/test/CodeGen/LoongArch/eliminateFI.ll
@@ -0,0 +1,106 @@
+; Check whether LoongArchSERegisterInfo::eliminateFI works well
+; RUN: llc -march=loongarch64 -o - %s | FileCheck %s
+
+define signext i32 @ldptr_w_unaligned() {
+; CHECK-LABEL: ldptr_w_unaligned:
+; CHECK: # %bb.0: # %entry
+entry:
+  %array = alloca [6000 x i8], align 1
+  %arrayidx = getelementptr inbounds [6000 x i8], [6000 x i8]* %array, i64 0, i64 5001
+  %0 = bitcast i8* %arrayidx to i32*
+; the offset MUST be 0
+; CHECK: ldptr.w $r{{[0-9]+}}, $r{{[0-9]+}}, 0
+  %1 = load i32, i32* %0, align 1
+  ret i32 %1
+}
+
+define signext i32 @ldptr_w_aligned() {
+; CHECK-LABEL: ldptr_w_aligned:
+; CHECK: # %bb.0: # %entry
+entry:
+  %array = alloca [6000 x i8], align 1
+  %arrayidx = getelementptr inbounds [6000 x i8], [6000 x i8]* %array, i64 0, i64 5000
+  %0 = bitcast i8* %arrayidx to i32*
+; the offset may not be 0, but MUST be 4-bytes aligned
+; CHECK: ldptr.w $r{{[0-9]+}}, $r{{[0-9]+}}, {{[0-9]+}}
+  %1 = load i32, i32* %0, align 1
+  ret i32 %1
+}
+
+define signext i64 @ldptr_d_unaligned() {
+; CHECK-LABEL: ldptr_d_unaligned:
+; CHECK: # %bb.0: # %entry
+entry:
+  %array = alloca [6000 x i8], align 1
+  %arrayidx = getelementptr inbounds [6000 x i8], [6000 x i8]* %array, i64 0, i64 5001
+  %0 = bitcast i8* %arrayidx to i64*
+; the offset MUST be 0
+; CHECK: ldptr.d $r{{[0-9]+}}, $r{{[0-9]+}}, 0
+  %1 = load i64, i64* %0, align 1
+  ret i64 %1
+}
+
+define signext i64 @ldptr_d_aligned() {
+; CHECK-LABEL: ldptr_d_aligned:
+; CHECK: # %bb.0: # %entry
+entry:
+  %array = alloca [6000 x i8], align 1
+  %arrayidx = getelementptr inbounds [6000 x i8], [6000 x i8]* %array, i64 0, i64 5000
+  %0 = bitcast i8* %arrayidx to i64*
+; the offset may not be 0, but MUST be 4-bytes aligned
+; CHECK: ldptr.d $r{{[0-9]+}}, $r{{[0-9]+}}, {{[0-9]+}}
+  %1 = load i64, i64* %0, align 1
+  ret i64 %1
+}
+
+define void @stptr_w_unaligned(i32 signext %val) {
+; CHECK-LABEL: stptr_w_unaligned:
+; CHECK: # %bb.0: # %entry
+entry:
+  %array = alloca [6000 x i8], align 1
+  %arrayidx = getelementptr inbounds [6000 x i8], [6000 x i8]* %array, i64 0, i64 5001
+  %0 = bitcast i8* %arrayidx to i32*
+; the offset MUST be 0
+; CHECK: stptr.w $r{{[0-9]+}}, $r{{[0-9]+}}, 0
+  store i32 %val, i32* %0, align 1
+  ret void
+}
+
+define void @stptr_w_aligned(i32 signext %val) {
+; CHECK-LABEL: stptr_w_aligned:
+; CHECK: # %bb.0: # %entry
+entry:
+  %array = alloca [6000 x i8], align 1
+  %arrayidx = getelementptr inbounds [6000 x i8], [6000 x i8]* %array, i64 0, i64 5000
+  %0 = bitcast i8* %arrayidx to i32*
+; the offset may not be 0, but MUST be 4-bytes aligned
+; CHECK: stptr.w $r{{[0-9]+}}, $r{{[0-9]+}}, {{[0-9]+}}
+  store i32 %val, i32* %0, align 1
+  ret void
+}
+
+define void @stptr_d_unaligned(i64 %val) {
+; CHECK-LABEL: stptr_d_unaligned:
+; CHECK: # %bb.0: # %entry
+entry:
+  %array = alloca [6000 x i8], align 1
+  %arrayidx = getelementptr inbounds [6000 x i8], [6000 x i8]* %array, i64 0, i64 5001
+  %0 = bitcast i8* %arrayidx to i64*
+; the offset MUST be 0
+; CHECK: stptr.d $r{{[0-9]+}}, $r{{[0-9]+}}, 0
+  store i64 %val, i64* %0, align 1
+  ret void
+}
+
+define void @stptr_d_aligned(i64 %val) {
+; CHECK-LABEL: stptr_d_aligned:
+; CHECK: # %bb.0: # %entry
+entry:
+  %array = alloca [6000 x i8], align 1
+  %arrayidx = getelementptr inbounds [6000 x i8], [6000 x i8]* %array, i64 0, i64 5000
+  %0 = bitcast i8* %arrayidx to i64*
+; the offset may not be 0, but MUST be 4-bytes aligned
+; CHECK: stptr.d $r{{[0-9]+}}, $r{{[0-9]+}}, {{[0-9]+}}
+  store i64 %val, i64* %0, align 1
+  ret void
+}
diff --git a/llvm/test/CodeGen/LoongArch/inlineasm/preld.ll b/llvm/test/CodeGen/LoongArch/inlineasm/preld.ll
new file mode 100644
index 000000000000..8dbbed99f16f
--- /dev/null
+++ b/llvm/test/CodeGen/LoongArch/inlineasm/preld.ll
@@ -0,0 +1,8 @@
+; RUN: llc -march=loongarch64 -o - %s | FileCheck %s
+
+define void @preld(i32* %p) {
+entry:
+  ; CHECK: preld 10, $r4, 23
+  tail call void asm sideeffect "preld 10, $0, 23 \0A\09", "r"(i32* %p)
+  ret void
+}
diff --git a/llvm/test/CodeGen/LoongArch/ldptr.ll b/llvm/test/CodeGen/LoongArch/ldptr.ll
new file mode 100644
index 000000000000..8395b264fc46
--- /dev/null
+++ b/llvm/test/CodeGen/LoongArch/ldptr.ll
@@ -0,0 +1,70 @@
+; Check whether ld.w/ld.d/ldptr.w/ldptr.d/ldx.w/ldx.d instructions are properly generated
+; RUN: llc -march=loongarch64 -o - %s | FileCheck %s
+
+define signext i32 @ld_w(i32* %p) {
+; CHECK-LABEL: ld_w:
+; CHECK: # %bb.0: # %entry
+; CHECK-NEXT: ld.w  $r4, $r4, 2044
+; CHECK-NEXT: jr $ra
+entry:
+  %addr = getelementptr inbounds i32, i32* %p, i64 511
+  %val = load i32, i32* %addr, align 4
+  ret i32 %val
+}
+
+define signext i32 @ldptr_w(i32* %p) {
+; CHECK-LABEL: ldptr_w:
+; CHECK: # %bb.0: # %entry
+; CHECK-NEXT: ldptr.w  $r4, $r4, 2048
+; CHECK-NEXT: jr $ra
+entry:
+  %addr = getelementptr inbounds i32, i32* %p, i64 512
+  %val = load i32, i32* %addr, align 4
+  ret i32 %val
+}
+
+define signext i32 @ldx_w(i32* %p) {
+; CHECK-LABEL: ldx_w:
+; CHECK: # %bb.0: # %entry
+; CHECK-NEXT: lu12i.w $r[[REG:[0-9]+]], 8
+; CHECK-NEXT: ldx.w  $r4, $r4, $r[[REG:[0-9]+]]
+; CHECK-NEXT: jr $ra
+entry:
+  %addr = getelementptr inbounds i32, i32* %p, i64 8192
+  %val = load i32, i32* %addr, align 4
+  ret i32 %val
+}
+
+define i64 @ld_d(i64* %p) {
+; CHECK-LABEL: ld_d:
+; CHECK: # %bb.0: # %entry
+; CHECK-NEXT: ld.d  $r4, $r4, 2040
+; CHECK-NEXT: jr $ra
+entry:
+  %addr = getelementptr inbounds i64, i64* %p, i64 255
+  %val = load i64, i64* %addr, align 8
+  ret i64 %val
+}
+
+define i64 @ldptr_d(i64* %p) {
+; CHECK-LABEL: ldptr_d:
+; CHECK: # %bb.0: # %entry
+; CHECK-NEXT: ldptr.d  $r4, $r4, 2048
+; CHECK-NEXT: jr $ra
+entry:
+  %addr = getelementptr inbounds i64, i64* %p, i64 256
+  %val = load i64, i64* %addr, align 8
+  ret i64 %val
+}
+
+define i64 @ldx_d(i64* %p) {
+; CHECK-LABEL: ldx_d:
+; CHECK: # %bb.0: # %entry
+; CHECK-NEXT: lu12i.w $r[[REG:[0-9]+]], 8
+; CHECK-NEXT: ldx.d  $r4, $r4, $r[[REG:[0-9]+]]
+; CHECK-NEXT: jr $ra
+entry:
+  %addr = getelementptr inbounds i64, i64* %p, i64 4096
+  %val = load i64, i64* %addr, align 8
+  ret i64 %val
+}
diff --git a/llvm/test/CodeGen/LoongArch/lit.local.cfg b/llvm/test/CodeGen/LoongArch/lit.local.cfg
new file mode 100644
index 000000000000..6223fc691edc
--- /dev/null
+++ b/llvm/test/CodeGen/LoongArch/lit.local.cfg
@@ -0,0 +1,3 @@
+if not 'LoongArch' in config.root.targets:
+    config.unsupported = True
+
diff --git a/llvm/test/CodeGen/LoongArch/stptr.ll b/llvm/test/CodeGen/LoongArch/stptr.ll
new file mode 100644
index 000000000000..0a54e0f8fb76
--- /dev/null
+++ b/llvm/test/CodeGen/LoongArch/stptr.ll
@@ -0,0 +1,52 @@
+; Check whether st.w/st.d/stptr.w/stptr.d/stx.w/stx.d instructions are properly generated
+; RUN: llc -march=loongarch64 -o - %s | FileCheck %s
+
+define void @st_w(i32* %p, i32 signext %val) {
+; CHECK: st.w  $r5, $r4, 2044
+; CHECK: jr $ra
+  %addr = getelementptr inbounds i32, i32* %p, i64 511
+  store i32 %val, i32* %addr, align 4
+  ret void
+}
+
+define void @stptr_w(i32* %p, i32 signext %val) {
+; CHECK: stptr.w  $r5, $r4, 2048
+; CHECK: jr $ra
+  %addr = getelementptr inbounds i32, i32* %p, i64 512
+  store i32 %val, i32* %addr, align 4
+  ret void
+}
+
+define void @stx_w(i32* %p, i32 signext %val) {
+; CHECK: lu12i.w $r[[REG:[0-9]+]], 8
+; CHECK: stx.w  $r5, $r4, $r[[REG:[0-9]+]]
+; CHECK: jr $ra
+  %addr = getelementptr inbounds i32, i32* %p, i64 8192
+  store i32 %val, i32* %addr, align 4
+  ret void
+}
+
+define void @st_d(i64* %p, i64 %val) {
+; CHECK: st.d  $r5, $r4, 2040
+; CHECK: jr $ra
+  %addr = getelementptr inbounds i64, i64* %p, i64 255
+  store i64 %val, i64* %addr, align 8
+  ret void
+}
+
+define void @stptr_d(i64* %p, i64 %val) {
+; CHECK: stptr.d  $r5, $r4, 2048
+; CHECK: jr $ra
+  %addr = getelementptr inbounds i64, i64* %p, i64 256
+  store i64 %val, i64* %addr, align 8
+  ret void
+}
+
+define void @stx_d(i64* %p, i64 %val) {
+; CHECK: lu12i.w $r[[REG:[0-9]+]], 8
+; CHECK: stx.d  $r5, $r4, $r[[REG:[0-9]+]]
+; CHECK: jr $ra
+  %addr = getelementptr inbounds i64, i64* %p, i64 4096
+  store i64 %val, i64* %addr, align 8
+  ret void
+}
diff --git a/llvm/test/CodeGen/LoongArch/thread-pointer.ll b/llvm/test/CodeGen/LoongArch/thread-pointer.ll
new file mode 100644
index 000000000000..06a5886c4cab
--- /dev/null
+++ b/llvm/test/CodeGen/LoongArch/thread-pointer.ll
@@ -0,0 +1,9 @@
+; RUN: llc -march=loongarch64 < %s | FileCheck %s
+
+declare i8* @llvm.thread.pointer() nounwind readnone
+
+define i8* @thread_pointer() {
+; CHECK: move    $r4, $tp
+  %1 = tail call i8* @llvm.thread.pointer()
+  ret i8* %1
+}
diff --git a/llvm/test/MC/Disassembler/LoongArch/lit.local.cfg b/llvm/test/MC/Disassembler/LoongArch/lit.local.cfg
new file mode 100644
index 000000000000..6223fc691edc
--- /dev/null
+++ b/llvm/test/MC/Disassembler/LoongArch/lit.local.cfg
@@ -0,0 +1,3 @@
+if not 'LoongArch' in config.root.targets:
+    config.unsupported = True
+
diff --git a/llvm/test/MC/LoongArch/aligned-nops.s b/llvm/test/MC/LoongArch/aligned-nops.s
new file mode 100644
index 000000000000..2ef26ac4b5ed
--- /dev/null
+++ b/llvm/test/MC/LoongArch/aligned-nops.s
@@ -0,0 +1,25 @@
+# RUN: llvm-mc -filetype=obj -triple loongarch64 < %s \
+# RUN:     | llvm-objdump -d - | FileCheck -check-prefix=CHECK-INST %s
+
+# alpha and main are 8 byte alignment
+# but the alpha function's size is 4
+# So assembler will insert a nop to make sure 8 byte alignment.
+
+        .text
+       .p2align        3
+       .type   alpha,@function
+alpha:
+# BB#0:
+       addi.d  $sp, $sp, -16
+# CHECK-INST: nop
+.Lfunc_end0:
+       .size   alpha, .Lfunc_end0-alpha
+                                        # -- End function
+       .globl  main
+       .p2align        3
+       .type   main,@function
+main:                                   # @main
+# BB#0:
+.Lfunc_end1:
+       .size   main, .Lfunc_end1-main
+                                        # -- End function
diff --git a/llvm/test/MC/LoongArch/invalid.s b/llvm/test/MC/LoongArch/invalid.s
new file mode 100644
index 000000000000..eea01f6707d3
--- /dev/null
+++ b/llvm/test/MC/LoongArch/invalid.s
@@ -0,0 +1,4 @@
+# RUN: not llvm-mc %s -triple=loongarch64-unknown-linux-gnu 2>&1 | FileCheck %s
+.text
+csrxchg        $r6, $r0, 214                # CHECK: :[[@LINE]]:1: error: invalid operand ($zero) for instruction
+csrxchg        $r6, $r1, 214                # CHECK: :[[@LINE]]:1: error: invalid operand ($r1) for instruction
diff --git a/llvm/test/MC/LoongArch/unaligned-nops.s b/llvm/test/MC/LoongArch/unaligned-nops.s
new file mode 100644
index 000000000000..facb9f2769f8
--- /dev/null
+++ b/llvm/test/MC/LoongArch/unaligned-nops.s
@@ -0,0 +1,5 @@
+# RUN: not llvm-mc -filetype=obj  -triple=loongarch64 %s -o %t
+.byte 1
+# CHECK: LLVM ERROR: unable to write nop sequence of 3 bytes
+.p2align 2
+foo:
diff --git a/llvm/test/MC/LoongArch/valid_12imm.s b/llvm/test/MC/LoongArch/valid_12imm.s
new file mode 100644
index 000000000000..2d45db6e05de
--- /dev/null
+++ b/llvm/test/MC/LoongArch/valid_12imm.s
@@ -0,0 +1,9 @@
+# RUN: llvm-mc %s -triple=loongarch64-unknown-linux-gnu -show-encoding | FileCheck %s
+slti           $r27, $ra, 235                # CHECK: slti       $r27, $ra, 235                # encoding: [0x3b,0xac,0x03,0x02]
+sltui          $zero, $r8, 162               # CHECK: sltui      $zero, $r8, 162               # encoding: [0x00,0x89,0x42,0x02]
+addi.w         $r5, $r7, 246                 # CHECK: addi.w     $r5, $r7, 246                 # encoding: [0xe5,0xd8,0x83,0x02]
+addi.d         $r28, $r6, 75                 # CHECK: addi.d     $r28, $r6, 75                 # encoding: [0xdc,0x2c,0xc1,0x02]
+lu52i.d        $r13, $r4, 195                # CHECK: lu52i.d    $r13, $r4, 195                # encoding: [0x8d,0x0c,0x03,0x03]
+andi           $r25, $zero, 106              # CHECK: andi       $r25, $zero, 106              # encoding: [0x19,0xa8,0x41,0x03]
+ori            $r17, $r5, 47                 # CHECK: ori        $r17, $r5, 47                 # encoding: [0xb1,0xbc,0x80,0x03]
+xori           $r18, $r23, 99                # CHECK: xori       $r18, $r23, 99                # encoding: [0xf2,0x8e,0xc1,0x03]
diff --git a/llvm/test/MC/LoongArch/valid_4operands.s b/llvm/test/MC/LoongArch/valid_4operands.s
new file mode 100644
index 000000000000..712754bd319c
--- /dev/null
+++ b/llvm/test/MC/LoongArch/valid_4operands.s
@@ -0,0 +1,14 @@
+# RUN: llvm-mc %s -triple=loongarch64-unknown-linux-gnu -show-encoding | FileCheck %s
+fmadd.s        $f3, $f16, $f3, $f15          # CHECK: fmadd.s    $f3, $f16, $f3, $f15          # encoding: [0x03,0x8e,0x17,0x08]
+fmadd.d        $f21, $f24, $f28, $f24        # CHECK: fmadd.d    $f21, $f24, $f28, $f24        # encoding: [0x15,0x73,0x2c,0x08]
+fmsub.s        $f23, $f11, $f21, $f4         # CHECK: fmsub.s    $f23, $f11, $f21, $f4         # encoding: [0x77,0x55,0x52,0x08]
+fmsub.d        $f6, $f18, $f20, $f27         # CHECK: fmsub.d    $f6, $f18, $f20, $f27         # encoding: [0x46,0xd2,0x6d,0x08]
+fnmadd.s       $f29, $f1, $f24, $f20         # CHECK: fnmadd.s   $f29, $f1, $f24, $f20         # encoding: [0x3d,0x60,0x9a,0x08]
+fnmadd.d       $f25, $f13, $f19, $f30        # CHECK: fnmadd.d   $f25, $f13, $f19, $f30        # encoding: [0xb9,0x4d,0xaf,0x08]
+fnmsub.s       $f8, $f4, $f24, $f25          # CHECK: fnmsub.s   $f8, $f4, $f24, $f25          # encoding: [0x88,0xe0,0xdc,0x08]
+fnmsub.d       $f30, $f26, $f7, $f24         # CHECK: fnmsub.d   $f30, $f26, $f7, $f24         # encoding: [0x5e,0x1f,0xec,0x08]
+fcmp.ceq.s     $fcc7, $f17, $f29             # CHECK: fcmp.ceq.s $fcc7, $f17, $f29             # encoding: [0x27,0x76,0x12,0x0c]
+fcmp.ceq.d     $fcc4, $f12, $f9              # CHECK: fcmp.ceq.d $fcc4, $f12, $f9              # encoding: [0x84,0x25,0x22,0x0c]
+fcmp.cult.s    $fcc0, $f0, $f1               # CHECK: fcmp.cult.s $fcc0, $f0, $f1              # encoding: [0x00,0x04,0x15,0x0c]
+fcmp.cult.d    $fcc2, $f3, $f4               # CHECK: fcmp.cult.d $fcc2, $f3, $f4              # encoding: [0x62,0x10,0x25,0x0c]
+fsel           $f18, $f20, $f21, $fcc4       # CHECK: fsel       $f18, $f20, $f21, $fcc4       # encoding: [0x92,0x56,0x02,0x0d]
diff --git a/llvm/test/MC/LoongArch/valid_bigimm.s b/llvm/test/MC/LoongArch/valid_bigimm.s
new file mode 100644
index 000000000000..6b9228ee986e
--- /dev/null
+++ b/llvm/test/MC/LoongArch/valid_bigimm.s
@@ -0,0 +1,8 @@
+# RUN: llvm-mc %s -triple=loongarch64-unknown-linux-gnu -show-encoding | FileCheck %s
+addu16i.d      $r9, $r23, 23                 # CHECK: addu16i.d  $r9, $r23, 23                 # encoding: [0xe9,0x5e,0x00,0x10]
+lu12i.w        $r16, 49                      # CHECK: lu12i.w    $r16, 49                      # encoding: [0x30,0x06,0x00,0x14]
+lu32i.d        $sp, 196                      # CHECK: lu32i.d    $sp, 196                      # encoding: [0x83,0x18,0x00,0x16]
+pcaddi         $r9, 187                      # CHECK: pcaddi     $r9, 187                      # encoding: [0x69,0x17,0x00,0x18]
+pcalau12i      $r10, 89                      # CHECK: pcalau12i  $r10, 89                      # encoding: [0x2a,0x0b,0x00,0x1a]
+pcaddu12i      $zero, 37                     # CHECK: pcaddu12i  $zero, 37                     # encoding: [0xa0,0x04,0x00,0x1c]
+pcaddu18i      $r12, 26                      # CHECK: pcaddu18i  $r12, 26                      # encoding: [0x4c,0x03,0x00,0x1e]
diff --git a/llvm/test/MC/LoongArch/valid_branch.s b/llvm/test/MC/LoongArch/valid_branch.s
new file mode 100644
index 000000000000..0e0df3d94e0b
--- /dev/null
+++ b/llvm/test/MC/LoongArch/valid_branch.s
@@ -0,0 +1,13 @@
+# RUN: llvm-mc %s -triple=loongarch64-unknown-linux-gnu -show-encoding | FileCheck %s
+beqz           $r9, 96                       # CHECK: beqz       $r9, 96                       # encoding: [0x20,0x61,0x00,0x40]
+bnez           $sp, 212                      # CHECK: bnez       $sp, 212                      # encoding: [0x60,0xd4,0x00,0x44]
+bceqz          $fcc6, 12                     # CHECK: bceqz      $fcc6, 12                     # encoding: [0xc0,0x0c,0x00,0x48]
+bcnez          $fcc6, 72                     # CHECK: bcnez      $fcc6, 72                     # encoding: [0xc0,0x49,0x00,0x48]
+b              248                           # CHECK: b          248                           # encoding: [0x00,0xf8,0x00,0x50]
+bl             236                           # CHECK: bl         236                           # encoding: [0x00,0xec,0x00,0x54]
+beq            $r10, $r7, 176                # CHECK: beq        $r10, $r7, 176                # encoding: [0x47,0xb1,0x00,0x58]
+bne            $r25, $ra, 136                # CHECK: bne        $r25, $ra, 136                # encoding: [0x21,0x8b,0x00,0x5c]
+blt            $r15, $r30, 168               # CHECK: blt        $r15, $r30, 168               # encoding: [0xfe,0xa9,0x00,0x60]
+bge            $r12, $r15, 148               # CHECK: bge        $r12, $r15, 148               # encoding: [0x8f,0x95,0x00,0x64]
+bltu           $r17, $r5, 4                  # CHECK: bltu       $r17, $r5, 4                  # encoding: [0x25,0x06,0x00,0x68]
+bgeu           $r6, $r23, 140                # CHECK: bgeu       $r6, $r23, 140                # encoding: [0xd7,0x8c,0x00,0x6c]
diff --git a/llvm/test/MC/LoongArch/valid_float.s b/llvm/test/MC/LoongArch/valid_float.s
new file mode 100644
index 000000000000..c2563af091ba
--- /dev/null
+++ b/llvm/test/MC/LoongArch/valid_float.s
@@ -0,0 +1,75 @@
+# RUN: llvm-mc %s -triple=loongarch64-unknown-linux-gnu -show-encoding | FileCheck %s
+fadd.s         $f29, $f15, $f25              # CHECK: fadd.s          $f29, $f15, $f25              # encoding: [0xfd,0xe5,0x00,0x01]
+fadd.d         $f25, $f7, $f13               # CHECK: fadd.d          $f25, $f7, $f13               # encoding: [0xf9,0x34,0x01,0x01]
+fsub.s         $f14, $f6, $f31               # CHECK: fsub.s          $f14, $f6, $f31               # encoding: [0xce,0xfc,0x02,0x01]
+fsub.d         $f29, $f1, $f18               # CHECK: fsub.d          $f29, $f1, $f18               # encoding: [0x3d,0x48,0x03,0x01]
+fmul.s         $f0, $f7, $f17                # CHECK: fmul.s          $f0, $f7, $f17                # encoding: [0xe0,0xc4,0x04,0x01]
+fmul.d         $f4, $f30, $f7                # CHECK: fmul.d          $f4, $f30, $f7                # encoding: [0xc4,0x1f,0x05,0x01]
+fdiv.s         $f20, $f24, $f19              # CHECK: fdiv.s          $f20, $f24, $f19              # encoding: [0x14,0xcf,0x06,0x01]
+fdiv.d         $f3, $f25, $f28               # CHECK: fdiv.d          $f3, $f25, $f28               # encoding: [0x23,0x73,0x07,0x01]
+fmax.s         $f22, $f6, $f27               # CHECK: fmax.s          $f22, $f6, $f27               # encoding: [0xd6,0xec,0x08,0x01]
+fmax.d         $f11, $f26, $f13              # CHECK: fmax.d          $f11, $f26, $f13              # encoding: [0x4b,0x37,0x09,0x01]
+fmin.s         $f14, $f10, $f19              # CHECK: fmin.s          $f14, $f10, $f19              # encoding: [0x4e,0xcd,0x0a,0x01]
+fmin.d         $f1, $f13, $f27               # CHECK: fmin.d          $f1, $f13, $f27               # encoding: [0xa1,0x6d,0x0b,0x01]
+fmaxa.s        $f9, $f27, $f31               # CHECK: fmaxa.s         $f9, $f27, $f31               # encoding: [0x69,0xff,0x0c,0x01]
+fmaxa.d        $f24, $f13, $f4               # CHECK: fmaxa.d         $f24, $f13, $f4               # encoding: [0xb8,0x11,0x0d,0x01]
+fmina.s        $f15, $f18, $f1               # CHECK: fmina.s         $f15, $f18, $f1               # encoding: [0x4f,0x86,0x0e,0x01]
+fmina.d        $f18, $f10, $f0               # CHECK: fmina.d         $f18, $f10, $f0               # encoding: [0x52,0x01,0x0f,0x01]
+fscaleb.s      $f21, $f23, $f6               # CHECK: fscaleb.s       $f21, $f23, $f6               # encoding: [0xf5,0x9a,0x10,0x01]
+fscaleb.d      $f12, $f14, $f26              # CHECK: fscaleb.d       $f12, $f14, $f26              # encoding: [0xcc,0x69,0x11,0x01]
+fcopysign.s    $f13, $f24, $f23              # CHECK: fcopysign.s     $f13, $f24, $f23              # encoding: [0x0d,0xdf,0x12,0x01]
+fcopysign.d    $f16, $f26, $f6               # CHECK: fcopysign.d     $f16, $f26, $f6               # encoding: [0x50,0x1b,0x13,0x01]
+fabs.s         $f28, $f12                    # CHECK: fabs.s          $f28, $f12                    # encoding: [0x9c,0x05,0x14,0x01]
+fabs.d         $f23, $f3                     # CHECK: fabs.d          $f23, $f3                     # encoding: [0x77,0x08,0x14,0x01]
+fneg.s         $f21, $f24                    # CHECK: fneg.s          $f21, $f24                    # encoding: [0x15,0x17,0x14,0x01]
+fneg.d         $f11, $f26                    # CHECK: fneg.d          $f11, $f26                    # encoding: [0x4b,0x1b,0x14,0x01]
+flogb.s        $f31, $f23                    # CHECK: flogb.s         $f31, $f23                    # encoding: [0xff,0x26,0x14,0x01]
+flogb.d        $f21, $f29                    # CHECK: flogb.d         $f21, $f29                    # encoding: [0xb5,0x2b,0x14,0x01]
+fclass.s       $f20, $f9                     # CHECK: fclass.s        $f20, $f9                     # encoding: [0x34,0x35,0x14,0x01]
+fclass.d       $f19, $f2                     # CHECK: fclass.d        $f19, $f2                     # encoding: [0x53,0x38,0x14,0x01]
+fsqrt.s        $f27, $f18                    # CHECK: fsqrt.s         $f27, $f18                    # encoding: [0x5b,0x46,0x14,0x01]
+fsqrt.d        $f2, $f11                     # CHECK: fsqrt.d         $f2, $f11                     # encoding: [0x62,0x49,0x14,0x01]
+frecip.s       $f17, $f27                    # CHECK: frecip.s        $f17, $f27                    # encoding: [0x71,0x57,0x14,0x01]
+frecip.d       $f27, $f27                    # CHECK: frecip.d        $f27, $f27                    # encoding: [0x7b,0x5b,0x14,0x01]
+frsqrt.s       $f25, $f12                    # CHECK: frsqrt.s        $f25, $f12                    # encoding: [0x99,0x65,0x14,0x01]
+frsqrt.d       $f22, $f3                     # CHECK: frsqrt.d        $f22, $f3                     # encoding: [0x76,0x68,0x14,0x01]
+fmov.s         $f13, $f23                    # CHECK: fmov.s          $f13, $f23                    # encoding: [0xed,0x96,0x14,0x01]
+fmov.d         $f30, $f9                     # CHECK: fmov.d          $f30, $f9                     # encoding: [0x3e,0x99,0x14,0x01]
+movgr2fr.w     $f6, $tp                      # CHECK: movgr2fr.w      $f6, $tp                      # encoding: [0x46,0xa4,0x14,0x01]
+movgr2fr.d     $f30, $r11                    # CHECK: movgr2fr.d      $f30, $r11                    # encoding: [0x7e,0xa9,0x14,0x01]
+movgr2frh.w    $f23, $r26                    # CHECK: movgr2frh.w     $f23, $r26                    # encoding: [0x57,0xaf,0x14,0x01]
+movfr2gr.s     $r10, $f22                    # CHECK: movfr2gr.s      $r10, $f22                    # encoding: [0xca,0xb6,0x14,0x01]
+movfr2gr.d     $r26, $f17                    # CHECK: movfr2gr.d      $r26, $f17                    # encoding: [0x3a,0xba,0x14,0x01]
+movfrh2gr.s    $sp, $f26                     # CHECK: movfrh2gr.s     $sp, $f26                     # encoding: [0x43,0xbf,0x14,0x01]
+movfr2cf       $fcc4, $f11                   # CHECK: movfr2cf        $fcc4, $f11                   # encoding: [0x64,0xd1,0x14,0x01]
+movcf2fr       $f16, $fcc0                   # CHECK: movcf2fr        $f16, $fcc0                   # encoding: [0x10,0xd4,0x14,0x01]
+movgr2cf       $fcc5, $ra                    # CHECK: movgr2cf        $fcc5, $ra                    # encoding: [0x25,0xd8,0x14,0x01]
+movcf2gr       $r21, $fcc7                   # CHECK: movcf2gr        $r21, $fcc7                   # encoding: [0xf5,0xdc,0x14,0x01]
+fcvt.s.d       $f12, $f19                    # CHECK: fcvt.s.d        $f12, $f19                    # encoding: [0x6c,0x1a,0x19,0x01]
+fcvt.d.s       $f10, $f6                     # CHECK: fcvt.d.s        $f10, $f6                     # encoding: [0xca,0x24,0x19,0x01]
+ftintrm.w.s    $f16, $f16                    # CHECK: ftintrm.w.s     $f16, $f16                    # encoding: [0x10,0x06,0x1a,0x01]
+ftintrm.w.d    $f7, $f8                      # CHECK: ftintrm.w.d     $f7, $f8                      # encoding: [0x07,0x09,0x1a,0x01]
+ftintrm.l.s    $f24, $f10                    # CHECK: ftintrm.l.s     $f24, $f10                    # encoding: [0x58,0x25,0x1a,0x01]
+ftintrm.l.d    $f9, $f9                      # CHECK: ftintrm.l.d     $f9, $f9                      # encoding: [0x29,0x29,0x1a,0x01]
+ftintrp.w.s    $f14, $f31                    # CHECK: ftintrp.w.s     $f14, $f31                    # encoding: [0xee,0x47,0x1a,0x01]
+ftintrp.w.d    $f12, $f3                     # CHECK: ftintrp.w.d     $f12, $f3                     # encoding: [0x6c,0x48,0x1a,0x01]
+ftintrp.l.s    $f0, $f16                     # CHECK: ftintrp.l.s     $f0, $f16                     # encoding: [0x00,0x66,0x1a,0x01]
+ftintrp.l.d    $f4, $f29                     # CHECK: ftintrp.l.d     $f4, $f29                     # encoding: [0xa4,0x6b,0x1a,0x01]
+ftintrz.w.s    $f4, $f29                     # CHECK: ftintrz.w.s     $f4, $f29                     # encoding: [0xa4,0x87,0x1a,0x01]
+ftintrz.w.d    $f25, $f24                    # CHECK: ftintrz.w.d     $f25, $f24                    # encoding: [0x19,0x8b,0x1a,0x01]
+ftintrz.l.s    $f23, $f5                     # CHECK: ftintrz.l.s     $f23, $f5                     # encoding: [0xb7,0xa4,0x1a,0x01]
+ftintrz.l.d    $f3, $f10                     # CHECK: ftintrz.l.d     $f3, $f10                     # encoding: [0x43,0xa9,0x1a,0x01]
+ftintrne.w.s   $f4, $f17                     # CHECK: ftintrne.w.s    $f4, $f17                     # encoding: [0x24,0xc6,0x1a,0x01]
+ftintrne.w.d   $f31, $f12                    # CHECK: ftintrne.w.d    $f31, $f12                    # encoding: [0x9f,0xc9,0x1a,0x01]
+ftintrne.l.s   $f22, $f27                    # CHECK: ftintrne.l.s    $f22, $f27                    # encoding: [0x76,0xe7,0x1a,0x01]
+ftintrne.l.d   $f28, $f6                     # CHECK: ftintrne.l.d    $f28, $f6                     # encoding: [0xdc,0xe8,0x1a,0x01]
+ftint.w.s      $f21, $f13                    # CHECK: ftint.w.s       $f21, $f13                    # encoding: [0xb5,0x05,0x1b,0x01]
+ftint.w.d      $f3, $f14                     # CHECK: ftint.w.d       $f3, $f14                     # encoding: [0xc3,0x09,0x1b,0x01]
+ftint.l.s      $f31, $f24                    # CHECK: ftint.l.s       $f31, $f24                    # encoding: [0x1f,0x27,0x1b,0x01]
+ftint.l.d      $f16, $f24                    # CHECK: ftint.l.d       $f16, $f24                    # encoding: [0x10,0x2b,0x1b,0x01]
+ffint.s.w      $f30, $f5                     # CHECK: ffint.s.w       $f30, $f5                     # encoding: [0xbe,0x10,0x1d,0x01]
+ffint.s.l      $f6, $f5                      # CHECK: ffint.s.l       $f6, $f5                      # encoding: [0xa6,0x18,0x1d,0x01]
+ffint.d.w      $f24, $f18                    # CHECK: ffint.d.w       $f24, $f18                    # encoding: [0x58,0x22,0x1d,0x01]
+ffint.d.l      $f23, $f26                    # CHECK: ffint.d.l       $f23, $f26                    # encoding: [0x57,0x2b,0x1d,0x01]
+frint.s        $f5, $f17                     # CHECK: frint.s         $f5, $f17                     # encoding: [0x25,0x46,0x1e,0x01]
+frint.d        $f29, $f2                     # CHECK: frint.d         $f29, $f2                     # encoding: [0x5d,0x48,0x1e,0x01]
diff --git a/llvm/test/MC/LoongArch/valid_integer.s b/llvm/test/MC/LoongArch/valid_integer.s
new file mode 100644
index 000000000000..22b8dde9af37
--- /dev/null
+++ b/llvm/test/MC/LoongArch/valid_integer.s
@@ -0,0 +1,93 @@
+# RUN: llvm-mc %s -triple=loongarch64-unknown-linux-gnu -show-encoding | FileCheck %s
+clo.w          $ra, $sp                      # CHECK: clo.w      $ra, $sp                      # encoding: [0x61,0x10,0x00,0x00]
+clz.w          $r7, $r10                     # CHECK: clz.w      $r7, $r10                     # encoding: [0x47,0x15,0x00,0x00]
+cto.w          $tp, $r6                      # CHECK: cto.w      $tp, $r6                      # encoding: [0xc2,0x18,0x00,0x00]
+ctz.w          $r5, $r22                     # CHECK: ctz.w      $r5, $r22                     # encoding: [0xc5,0x1e,0x00,0x00]
+clo.d          $r29, $ra                     # CHECK: clo.d      $r29, $ra                     # encoding: [0x3d,0x20,0x00,0x00]
+clz.d          $r26, $r26                    # CHECK: clz.d      $r26, $r26                    # encoding: [0x5a,0x27,0x00,0x00]
+cto.d          $r18, $r20                    # CHECK: cto.d      $r18, $r20                    # encoding: [0x92,0x2a,0x00,0x00]
+ctz.d          $r17, $r10                    # CHECK: ctz.d      $r17, $r10                    # encoding: [0x51,0x2d,0x00,0x00]
+revb.2h        $r20, $r11                    # CHECK: revb.2h    $r20, $r11                    # encoding: [0x74,0x31,0x00,0x00]
+revb.4h        $r13, $r19                    # CHECK: revb.4h    $r13, $r19                    # encoding: [0x6d,0x36,0x00,0x00]
+revb.2w        $r28, $r27                    # CHECK: revb.2w    $r28, $r27                    # encoding: [0x7c,0x3b,0x00,0x00]
+revb.d         $zero, $r23                   # CHECK: revb.d     $zero, $r23                   # encoding: [0xe0,0x3e,0x00,0x00]
+revh.2w        $r28, $r10                    # CHECK: revh.2w    $r28, $r10                    # encoding: [0x5c,0x41,0x00,0x00]
+revh.d         $r9, $r7                      # CHECK: revh.d     $r9, $r7                      # encoding: [0xe9,0x44,0x00,0x00]
+bitrev.4b      $r21, $r27                    # CHECK: bitrev.4b  $r21, $r27                    # encoding: [0x75,0x4b,0x00,0x00]
+bitrev.8b      $r13, $r25                    # CHECK: bitrev.8b  $r13, $r25                    # encoding: [0x2d,0x4f,0x00,0x00]
+bitrev.w       $r25, $r5                     # CHECK: bitrev.w   $r25, $r5                     # encoding: [0xb9,0x50,0x00,0x00]
+bitrev.d       $r19, $r23                    # CHECK: bitrev.d   $r19, $r23                    # encoding: [0xf3,0x56,0x00,0x00]
+ext.w.h        $r23, $r23                    # CHECK: ext.w.h    $r23, $r23                    # encoding: [0xf7,0x5a,0x00,0x00]
+ext.w.b        $r20, $r18                    # CHECK: ext.w.b    $r20, $r18                    # encoding: [0x54,0x5e,0x00,0x00]
+rdtimel.w      $r24, $r4                     # CHECK: rdtimel.w  $r24, $r4                     # encoding: [0x98,0x60,0x00,0x00]
+rdtimeh.w      $r11, $r5                     # CHECK: rdtimeh.w  $r11, $r5                     # encoding: [0xab,0x64,0x00,0x00]
+rdtime.d       $tp, $ra                      # CHECK: rdtime.d   $tp, $ra                      # encoding: [0x22,0x68,0x00,0x00]
+cpucfg         $sp, $ra                      # CHECK: cpucfg     $sp, $ra                      # encoding: [0x23,0x6c,0x00,0x00]
+asrtle.d       $r21, $r19                    # CHECK: asrtle.d   $r21, $r19                    # encoding: [0xa0,0x4e,0x01,0x00]
+asrtgt.d       $ra, $r19                     # CHECK: asrtgt.d   $ra, $r19                     # encoding: [0x20,0xcc,0x01,0x00]
+alsl.w         $tp, $r17, $tp, 4             # CHECK: alsl.w     $tp, $r17, $tp, 4             # encoding: [0x22,0x8a,0x05,0x00]
+bytepick.w     $r29, $zero, $r16, 0          # CHECK: bytepick.w $r29, $zero, $r16, 0          # encoding: [0x1d,0x40,0x08,0x00]
+bytepick.d     $r15, $r17, $r20, 4           # CHECK: bytepick.d $r15, $r17, $r20, 4           # encoding: [0x2f,0x52,0x0e,0x00]
+add.w          $r9, $ra, $r31                # CHECK: add.w      $r9, $ra, $r31                # encoding: [0x29,0x7c,0x10,0x00]
+add.d          $tp, $r18, $r27               # CHECK: add.d      $tp, $r18, $r27               # encoding: [0x42,0xee,0x10,0x00]
+sub.w          $r21, $r25, $r19              # CHECK: sub.w      $r21, $r25, $r19              # encoding: [0x35,0x4f,0x11,0x00]
+sub.d          $r7, $r12, $r7                # CHECK: sub.d      $r7, $r12, $r7                # encoding: [0x87,0x9d,0x11,0x00]
+slt            $r29, $r26, $tp               # CHECK: slt        $r29, $r26, $tp               # encoding: [0x5d,0x0b,0x12,0x00]
+sltu           $r11, $r21, $r29              # CHECK: sltu       $r11, $r21, $r29              # encoding: [0xab,0xf6,0x12,0x00]
+maskeqz        $r20, $r11, $r18              # CHECK: maskeqz    $r20, $r11, $r18              # encoding: [0x74,0x49,0x13,0x00]
+masknez        $r20, $r13, $r26              # CHECK: masknez    $r20, $r13, $r26              # encoding: [0xb4,0xe9,0x13,0x00]
+nor            $r5, $r18, $r5                # CHECK: nor        $r5, $r18, $r5                # encoding: [0x45,0x16,0x14,0x00]
+and            $r19, $r31, $ra               # CHECK: and        $r19, $r31, $ra               # encoding: [0xf3,0x87,0x14,0x00]
+or             $r17, $r16, $r30              # CHECK: or         $r17, $r16, $r30              # encoding: [0x11,0x7a,0x15,0x00]
+xor            $r15, $r19, $r8               # CHECK: xor        $r15, $r19, $r8               # encoding: [0x6f,0xa2,0x15,0x00]
+orn            $tp, $sp, $r25                # CHECK: orn        $tp, $sp, $r25                # encoding: [0x62,0x64,0x16,0x00]
+andn           $r28, $r25, $r5               # CHECK: andn       $r28, $r25, $r5               # encoding: [0x3c,0x97,0x16,0x00]
+sll.w          $r24, $r27, $r23              # CHECK: sll.w      $r24, $r27, $r23              # encoding: [0x78,0x5f,0x17,0x00]
+srl.w          $r31, $r17, $r7               # CHECK: srl.w      $r31, $r17, $r7               # encoding: [0x3f,0x9e,0x17,0x00]
+sra.w          $r12, $r28, $r10              # CHECK: sra.w      $r12, $r28, $r10              # encoding: [0x8c,0x2b,0x18,0x00]
+sll.d          $r20, $r15, $sp               # CHECK: sll.d      $r20, $r15, $sp               # encoding: [0xf4,0x8d,0x18,0x00]
+srl.d          $r14, $r25, $zero             # CHECK: srl.d      $r14, $r25, $zero             # encoding: [0x2e,0x03,0x19,0x00]
+sra.d          $r7, $r22, $r31               # CHECK: sra.d      $r7, $r22, $r31               # encoding: [0xc7,0xfe,0x19,0x00]
+rotr.w         $ra, $r26, $r18               # CHECK: rotr.w     $ra, $r26, $r18               # encoding: [0x41,0x4b,0x1b,0x00]
+rotr.d         $r31, $sp, $ra                # CHECK: rotr.d     $r31, $sp, $ra                # encoding: [0x7f,0x84,0x1b,0x00]
+mul.w          $r4, $r18, $sp                # CHECK: mul.w      $r4, $r18, $sp                # encoding: [0x44,0x0e,0x1c,0x00]
+mulh.w         $r27, $r23, $zero             # CHECK: mulh.w     $r27, $r23, $zero             # encoding: [0xfb,0x82,0x1c,0x00]
+mulh.wu        $r10, $r17, $r24              # CHECK: mulh.wu    $r10, $r17, $r24              # encoding: [0x2a,0x62,0x1d,0x00]
+mul.d          $ra, $r14, $r24               # CHECK: mul.d      $ra, $r14, $r24               # encoding: [0xc1,0xe1,0x1d,0x00]
+mulh.d         $r28, $ra, $r27               # CHECK: mulh.d     $r28, $ra, $r27               # encoding: [0x3c,0x6c,0x1e,0x00]
+mulh.du        $r13, $r27, $r29              # CHECK: mulh.du    $r13, $r27, $r29              # encoding: [0x6d,0xf7,0x1e,0x00]
+mulw.d.w       $r27, $r6, $r17               # CHECK: mulw.d.w   $r27, $r6, $r17               # encoding: [0xdb,0x44,0x1f,0x00]
+mulw.d.wu      $r17, $r22, $r30              # CHECK: mulw.d.wu  $r17, $r22, $r30              # encoding: [0xd1,0xfa,0x1f,0x00]
+div.w          $r30, $r13, $r25              # CHECK: div.w      $r30, $r13, $r25              # encoding: [0xbe,0x65,0x20,0x00]
+mod.w          $ra, $r26, $r10               # CHECK: mod.w      $ra, $r26, $r10               # encoding: [0x41,0xab,0x20,0x00]
+div.wu         $r19, $r23, $zero             # CHECK: div.wu     $r19, $r23, $zero             # encoding: [0xf3,0x02,0x21,0x00]
+mod.wu         $r27, $r9, $r17               # CHECK: mod.wu     $r27, $r9, $r17               # encoding: [0x3b,0xc5,0x21,0x00]
+div.d          $r23, $r6, $r21               # CHECK: div.d      $r23, $r6, $r21               # encoding: [0xd7,0x54,0x22,0x00]
+mod.d          $r16, $sp, $r15               # CHECK: mod.d      $r16, $sp, $r15               # encoding: [0x70,0xbc,0x22,0x00]
+div.du         $r31, $r24, $r14              # CHECK: div.du     $r31, $r24, $r14              # encoding: [0x1f,0x3b,0x23,0x00]
+mod.du         $r25, $r23, $r24              # CHECK: mod.du     $r25, $r23, $r24              # encoding: [0xf9,0xe2,0x23,0x00]
+crc.w.b.w      $r24, $r7, $tp                # CHECK: crc.w.b.w  $r24, $r7, $tp                # encoding: [0xf8,0x08,0x24,0x00]
+crc.w.h.w      $r31, $r10, $r18              # CHECK: crc.w.h.w  $r31, $r10, $r18              # encoding: [0x5f,0xc9,0x24,0x00]
+crc.w.w.w      $r28, $r6, $r10               # CHECK: crc.w.w.w  $r28, $r6, $r10               # encoding: [0xdc,0x28,0x25,0x00]
+crc.w.d.w      $r28, $r11, $r31              # CHECK: crc.w.d.w  $r28, $r11, $r31              # encoding: [0x7c,0xfd,0x25,0x00]
+crcc.w.b.w     $r15, $r18, $sp               # CHECK: crcc.w.b.w $r15, $r18, $sp               # encoding: [0x4f,0x0e,0x26,0x00]
+crcc.w.h.w     $r21, $r29, $r18              # CHECK: crcc.w.h.w $r21, $r29, $r18              # encoding: [0xb5,0xcb,0x26,0x00]
+crcc.w.w.w     $r17, $r14, $r13              # CHECK: crcc.w.w.w $r17, $r14, $r13              # encoding: [0xd1,0x35,0x27,0x00]
+crcc.w.d.w     $r30, $r21, $r27              # CHECK: crcc.w.d.w $r30, $r21, $r27              # encoding: [0xbe,0xee,0x27,0x00]
+break          23                            # CHECK: break   23                               # encoding: [0x17,0x00,0x2a,0x00]
+syscall        2                             # CHECK: syscall 2                                # encoding: [0x02,0x00,0x2b,0x00]
+alsl.d         $r17, $r11, $r5, 3            # CHECK: alsl.d     $r17, $r11, $r5, 3            # encoding: [0x71,0x15,0x2d,0x00]
+slli.w         $r26, $r18, 0                 # CHECK: slli.w     $r26, $r18, 0                 # encoding: [0x5a,0x82,0x40,0x00]
+slli.d         $r10, $r31, 39                # CHECK: slli.d     $r10, $r31, 39                # encoding: [0xea,0x9f,0x41,0x00]
+srli.w         $r10, $r14, 30                # CHECK: srli.w     $r10, $r14, 30                # encoding: [0xca,0xf9,0x44,0x00]
+srli.d         $r31, $r22, 38                # CHECK: srli.d     $r31, $r22, 38                # encoding: [0xdf,0x9a,0x45,0x00]
+srai.w         $r8, $r17, 24                 # CHECK: srai.w     $r8, $r17, 24                 # encoding: [0x28,0xe2,0x48,0x00]
+srai.d         $r9, $r21, 27                 # CHECK: srai.d     $r9, $r21, 27                 # encoding: [0xa9,0x6e,0x49,0x00]
+rotri.w        $r23, $r20, 23                # CHECK: rotri.w    $r23, $r20, 23                # encoding: [0x97,0xde,0x4c,0x00]
+rotri.d        $r29, $zero, 7                # CHECK: rotri.d    $r29, $zero, 7                # encoding: [0x1d,0x1c,0x4d,0x00]
+bstrins.w      $r8, $r11, 7, 2               # CHECK: bstrins.w  $r8, $r11, 7, 2               # encoding: [0x68,0x09,0x67,0x00]
+bstrins.d      $r8, $r11, 7, 2               # CHECK: bstrins.d  $r8, $r11, 7, 2               # encoding: [0x68,0x09,0x87,0x00]
+bstrpick.w     $ra, $r9, 10, 4               # CHECK: bstrpick.w $ra, $r9, 10, 4               # encoding: [0x21,0x91,0x6a,0x00]
+bstrpick.d     $r31, $r27, 39, 22            # CHECK: bstrpick.d $r31, $r27, 39, 22            # encoding: [0x7f,0x5b,0xe7,0x00]
+cpucfg         $sp, $r8                      # CHECK: cpucfg     $sp, $r8                      # encoding: [0x03,0x6d,0x00,0x00]
+alsl.wu        $r19, $r8, $r25, 1            # CHECK: alsl.wu    $r19, $r8, $r25, 1            # encoding: [0x13,0x65,0x06,0x00]
diff --git a/llvm/test/MC/LoongArch/valid_memory.s b/llvm/test/MC/LoongArch/valid_memory.s
new file mode 100644
index 000000000000..4286e80966df
--- /dev/null
+++ b/llvm/test/MC/LoongArch/valid_memory.s
@@ -0,0 +1,102 @@
+# RUN: llvm-mc %s -triple=loongarch64-unknown-linux-gnu -show-encoding | FileCheck %s
+dbar           0                             # CHECK: dbar       0                             # encoding: [0x00,0x00,0x72,0x38]
+ibar           0                             # CHECK: ibar       0                             # encoding: [0x00,0x80,0x72,0x38]
+ll.w           $tp, $r27, 220                # CHECK: ll.w       $tp, $r27, 220                # encoding: [0x62,0xdf,0x00,0x20]
+sc.w           $r19, $r14, 56                # CHECK: sc.w       $r19, $r14, 56                # encoding: [0xd3,0x39,0x00,0x21]
+ll.d           $r25, $r27, 16                # CHECK: ll.d       $r25, $r27, 16                # encoding: [0x79,0x13,0x00,0x22]
+sc.d           $r17, $r17, 244               # CHECK: sc.d       $r17, $r17, 244               # encoding: [0x31,0xf6,0x00,0x23]
+ldptr.w        $r26, $r6, 60                 # CHECK: ldptr.w    $r26, $r6, 60                 # encoding: [0xda,0x3c,0x00,0x24]
+stptr.w        $r28, $r5, 216                # CHECK: stptr.w    $r28, $r5, 216                # encoding: [0xbc,0xd8,0x00,0x25]
+ldptr.d        $r5, $r29, 244                # CHECK: ldptr.d    $r5, $r29, 244                # encoding: [0xa5,0xf7,0x00,0x26]
+stptr.d        $r14, $r24, 196               # CHECK: stptr.d    $r14, $r24, 196               # encoding: [0x0e,0xc7,0x00,0x27]
+ld.b           $r24, $r8, 21                 # CHECK: ld.b       $r24, $r8, 21                 # encoding: [0x18,0x55,0x00,0x28]
+ld.h           $r7, $r18, 80                 # CHECK: ld.h       $r7, $r18, 80                 # encoding: [0x47,0x42,0x41,0x28]
+ld.w           $r18, $r26, 92                # CHECK: ld.w       $r18, $r26, 92                # encoding: [0x52,0x73,0x81,0x28]
+ld.d           $r18, $r20, 159               # CHECK: ld.d       $r18, $r20, 159               # encoding: [0x92,0x7e,0xc2,0x28]
+st.b           $sp, $r7, 95                  # CHECK: st.b       $sp, $r7, 95                  # encoding: [0xe3,0x7c,0x01,0x29]
+st.h           $r25, $r16, 122               # CHECK: st.h       $r25, $r16, 122               # encoding: [0x19,0xea,0x41,0x29]
+st.w           $r13, $r13, 175               # CHECK: st.w       $r13, $r13, 175               # encoding: [0xad,0xbd,0x82,0x29]
+st.d           $r30, $r30, 60                # CHECK: st.d       $r30, $r30, 60                # encoding: [0xde,0xf3,0xc0,0x29]
+ld.bu          $r13, $r13, 150               # CHECK: ld.bu      $r13, $r13, 150               # encoding: [0xad,0x59,0x02,0x2a]
+ld.hu          $r18, $r29, 198               # CHECK: ld.hu      $r18, $r29, 198               # encoding: [0xb2,0x1b,0x43,0x2a]
+ld.wu          $r14, $r19, 31                # CHECK: ld.wu      $r14, $r19, 31                # encoding: [0x6e,0x7e,0x80,0x2a]
+fld.s          $f23, $r15, 250               # CHECK: fld.s      $f23, $r15, 250               # encoding: [0xf7,0xe9,0x03,0x2b]
+fst.s          $f30, $r19, 230               # CHECK: fst.s      $f30, $r19, 230               # encoding: [0x7e,0x9a,0x43,0x2b]
+fld.d          $f22, $r17, 114               # CHECK: fld.d      $f22, $r17, 114               # encoding: [0x36,0xca,0x81,0x2b]
+fst.d          $f28, $r7, 198                # CHECK: fst.d      $f28, $r7, 198                # encoding: [0xfc,0x18,0xc3,0x2b]
+ldx.b          $r24, $ra, $tp                # CHECK: ldx.b      $r24, $ra, $tp                # encoding: [0x38,0x08,0x00,0x38]
+ldx.h          $r22, $r22, $r17              # CHECK: ldx.h      $r22, $r22, $r17              # encoding: [0xd6,0x46,0x04,0x38]
+ldx.w          $r25, $r11, $r23              # CHECK: ldx.w      $r25, $r11, $r23              # encoding: [0x79,0x5d,0x08,0x38]
+ldx.d          $r18, $r23, $r20              # CHECK: ldx.d      $r18, $r23, $r20              # encoding: [0xf2,0x52,0x0c,0x38]
+stx.b          $r19, $ra, $sp                # CHECK: stx.b      $r19, $ra, $sp                # encoding: [0x33,0x0c,0x10,0x38]
+stx.h          $zero, $r28, $r26             # CHECK: stx.h      $zero, $r28, $r26             # encoding: [0x80,0x6b,0x14,0x38]
+stx.w          $r7, $r4, $r31                # CHECK: stx.w      $r7, $r4, $r31                # encoding: [0x87,0x7c,0x18,0x38]
+stx.d          $r7, $r31, $r10               # CHECK: stx.d      $r7, $r31, $r10               # encoding: [0xe7,0x2b,0x1c,0x38]
+ldx.bu         $r11, $r9, $r9                # CHECK: ldx.bu     $r11, $r9, $r9                # encoding: [0x2b,0x25,0x20,0x38]
+ldx.hu         $r22, $r23, $r27              # CHECK: ldx.hu     $r22, $r23, $r27              # encoding: [0xf6,0x6e,0x24,0x38]
+ldx.wu         $r8, $r24, $r28               # CHECK: ldx.wu     $r8, $r24, $r28               # encoding: [0x08,0x73,0x28,0x38]
+fldx.s         $f1, $r15, $r19               # CHECK: fldx.s     $f1, $r15, $r19               # encoding: [0xe1,0x4d,0x30,0x38]
+fldx.d         $f27, $r13, $r31              # CHECK: fldx.d     $f27, $r13, $r31              # encoding: [0xbb,0x7d,0x34,0x38]
+fstx.s         $f26, $sp, $r22               # CHECK: fstx.s     $f26, $sp, $r22               # encoding: [0x7a,0x58,0x38,0x38]
+fstx.d         $f6, $r15, $r17               # CHECK: fstx.d     $f6, $r15, $r17               # encoding: [0xe6,0x45,0x3c,0x38]
+amswap_db.w       $r6, $r12, $r24, 0            # CHECK: amswap_db.w   $r6, $r12, $r24, 0            # encoding: [0x06,0x33,0x69,0x38]
+amswap_db.d       $tp, $r14, $r22, 0            # CHECK: amswap_db.d   $tp, $r14, $r22, 0            # encoding: [0xc2,0xba,0x69,0x38]
+amadd_db.w        $r8, $r12, $r21, 0            # CHECK: amadd_db.w    $r8, $r12, $r21, 0            # encoding: [0xa8,0x32,0x6a,0x38]
+amadd_db.d        $r5, $r17, $r29, 0            # CHECK: amadd_db.d    $r5, $r17, $r29, 0            # encoding: [0xa5,0xc7,0x6a,0x38]
+amand_db.w        $r4, $r19, $r22, 0            # CHECK: amand_db.w    $r4, $r19, $r22, 0            # encoding: [0xc4,0x4e,0x6b,0x38]
+amand_db.d        $r10, $r18, $r29, 0           # CHECK: amand_db.d    $r10, $r18, $r29, 0           # encoding: [0xaa,0xcb,0x6b,0x38]
+amor_db.w         $r6, $r16, $r23, 0            # CHECK: amor_db.w     $r6, $r16, $r23, 0            # encoding: [0xe6,0x42,0x6c,0x38]
+amor_db.d         $sp, $r16, $r24, 0            # CHECK: amor_db.d     $sp, $r16, $r24, 0            # encoding: [0x03,0xc3,0x6c,0x38]
+amxor_db.w        $tp, $r15, $r23, 0            # CHECK: amxor_db.w    $tp, $r15, $r23, 0            # encoding: [0xe2,0x3e,0x6d,0x38]
+amxor_db.d        $r8, $r20, $r28, 0            # CHECK: amxor_db.d    $r8, $r20, $r28, 0            # encoding: [0x88,0xd3,0x6d,0x38]
+ammax_db.w        $ra, $r11, $r23, 0            # CHECK: ammax_db.w    $ra, $r11, $r23, 0            # encoding: [0xe1,0x2e,0x6e,0x38]
+ammax_db.d        $r9, $r20, $r27, 0            # CHECK: ammax_db.d    $r9, $r20, $r27, 0            # encoding: [0x69,0xd3,0x6e,0x38]
+ammin_db.w        $r9, $r14, $r23, 0            # CHECK: ammin_db.w    $r9, $r14, $r23, 0            # encoding: [0xe9,0x3a,0x6f,0x38]
+ammin_db.d        $r9, $r13, $r22, 0            # CHECK: ammin_db.d    $r9, $r13, $r22, 0            # encoding: [0xc9,0xb6,0x6f,0x38]
+ammax_db.wu       $r9, $r11, $r22, 0            # CHECK: ammax_db.wu   $r9, $r11, $r22, 0            # encoding: [0xc9,0x2e,0x70,0x38]
+ammax_db.du       $r6, $r16, $r25, 0            # CHECK: ammax_db.du   $r6, $r16, $r25, 0            # encoding: [0x26,0xc3,0x70,0x38]
+ammin_db.wu       $r8, $r18, $r30, 0            # CHECK: ammin_db.wu   $r8, $r18, $r30, 0            # encoding: [0xc8,0x4b,0x71,0x38]
+ammin_db.du       $r7, $r16, $r25, 0            # CHECK: ammin_db.du   $r7, $r16, $r25, 0            # encoding: [0x27,0xc3,0x71,0x38]
+amswap.w       $r6, $r12, $r24, 0            # CHECK: amswap.w   $r6, $r12, $r24, 0            # encoding: [0x06,0x33,0x60,0x38]
+amswap.d       $tp, $r14, $r22, 0            # CHECK: amswap.d   $tp, $r14, $r22, 0            # encoding: [0xc2,0xba,0x60,0x38]
+amadd.w        $r8, $r12, $r21, 0            # CHECK: amadd.w    $r8, $r12, $r21, 0            # encoding: [0xa8,0x32,0x61,0x38]
+amadd.d        $r5, $r17, $r29, 0            # CHECK: amadd.d    $r5, $r17, $r29, 0            # encoding: [0xa5,0xc7,0x61,0x38]
+amand.w        $r4, $r19, $r22, 0            # CHECK: amand.w    $r4, $r19, $r22, 0            # encoding: [0xc4,0x4e,0x62,0x38]
+amand.d        $r10, $r18, $r29, 0           # CHECK: amand.d    $r10, $r18, $r29, 0           # encoding: [0xaa,0xcb,0x62,0x38]
+amor.w         $r6, $r16, $r23, 0            # CHECK: amor.w     $r6, $r16, $r23, 0            # encoding: [0xe6,0x42,0x63,0x38]
+amor.d         $sp, $r16, $r24, 0            # CHECK: amor.d     $sp, $r16, $r24, 0            # encoding: [0x03,0xc3,0x63,0x38]
+amxor.w        $tp, $r15, $r23, 0            # CHECK: amxor.w    $tp, $r15, $r23, 0            # encoding: [0xe2,0x3e,0x64,0x38]
+amxor.d        $r8, $r20, $r28, 0            # CHECK: amxor.d    $r8, $r20, $r28, 0            # encoding: [0x88,0xd3,0x64,0x38]
+ammax.w        $ra, $r11, $r23, 0            # CHECK: ammax.w    $ra, $r11, $r23, 0            # encoding: [0xe1,0x2e,0x65,0x38]
+ammax.d        $r9, $r20, $r27, 0            # CHECK: ammax.d    $r9, $r20, $r27, 0            # encoding: [0x69,0xd3,0x65,0x38]
+ammin.w        $r9, $r14, $r23, 0            # CHECK: ammin.w    $r9, $r14, $r23, 0            # encoding: [0xe9,0x3a,0x66,0x38]
+ammin.d        $r9, $r13, $r22, 0            # CHECK: ammin.d    $r9, $r13, $r22, 0            # encoding: [0xc9,0xb6,0x66,0x38]
+ammax.wu       $r9, $r11, $r22, 0            # CHECK: ammax.wu   $r9, $r11, $r22, 0            # encoding: [0xc9,0x2e,0x67,0x38]
+ammax.du       $r6, $r16, $r25, 0            # CHECK: ammax.du   $r6, $r16, $r25, 0            # encoding: [0x26,0xc3,0x67,0x38]
+ammin.wu       $r8, $r18, $r30, 0            # CHECK: ammin.wu   $r8, $r18, $r30, 0            # encoding: [0xc8,0x4b,0x68,0x38]
+ammin.du       $r7, $r16, $r25, 0            # CHECK: ammin.du   $r7, $r16, $r25, 0            # encoding: [0x27,0xc3,0x68,0x38]
+fldgt.s        $f3, $r27, $r13               # CHECK: fldgt.s    $f3, $r27, $r13               # encoding: [0x63,0x37,0x74,0x38]
+fldgt.d        $f26, $r5, $r31               # CHECK: fldgt.d    $f26, $r5, $r31               # encoding: [0xba,0xfc,0x74,0x38]
+fldle.s        $f24, $r29, $r17              # CHECK: fldle.s    $f24, $r29, $r17              # encoding: [0xb8,0x47,0x75,0x38]
+fldle.d        $f3, $r15, $r22               # CHECK: fldle.d    $f3, $r15, $r22               # encoding: [0xe3,0xd9,0x75,0x38]
+fstgt.s        $f31, $r13, $r30              # CHECK: fstgt.s    $f31, $r13, $r30              # encoding: [0xbf,0x79,0x76,0x38]
+fstgt.d        $f13, $r11, $r26              # CHECK: fstgt.d    $f13, $r11, $r26              # encoding: [0x6d,0xe9,0x76,0x38]
+fstle.s        $f13, $r13, $r7               # CHECK: fstle.s    $f13, $r13, $r7               # encoding: [0xad,0x1d,0x77,0x38]
+fstle.d        $f18, $r9, $r13               # CHECK: fstle.d    $f18, $r9, $r13               # encoding: [0x32,0xb5,0x77,0x38]
+preld          10, $zero, 23                 # CHECK: preld      10, $zero, 23                 # encoding: [0x0a,0x5c,0xc0,0x2a]
+ldgt.b         $r6, $r6, $r29                # CHECK: ldgt.b     $r6, $r6, $r29                # encoding: [0xc6,0x74,0x78,0x38]
+ldgt.h         $r5, $r31, $ra                # CHECK: ldgt.h     $r5, $r31, $ra                # encoding: [0xe5,0x87,0x78,0x38]
+ldgt.w         $r15, $r26, $r8               # CHECK: ldgt.w     $r15, $r26, $r8               # encoding: [0x4f,0x23,0x79,0x38]
+ldgt.d         $r23, $r25, $r31              # CHECK: ldgt.d     $r23, $r25, $r31              # encoding: [0x37,0xff,0x79,0x38]
+ldle.b         $r9, $r12, $r15               # CHECK: ldle.b     $r9, $r12, $r15               # encoding: [0x89,0x3d,0x7a,0x38]
+ldle.h         $r11, $r11, $r23              # CHECK: ldle.h     $r11, $r11, $r23              # encoding: [0x6b,0xdd,0x7a,0x38]
+ldle.w         $r24, $tp, $tp                # CHECK: ldle.w     $r24, $tp, $tp                # encoding: [0x58,0x08,0x7b,0x38]
+ldle.d         $r20, $r15, $r16              # CHECK: ldle.d     $r20, $r15, $r16              # encoding: [0xf4,0xc1,0x7b,0x38]
+stgt.b         $r27, $r19, $r20              # CHECK: stgt.b     $r27, $r19, $r20              # encoding: [0x7b,0x52,0x7c,0x38]
+stgt.h         $r16, $r4, $r6                # CHECK: stgt.h     $r16, $r4, $r6                # encoding: [0x90,0x98,0x7c,0x38]
+stgt.w         $r31, $r28, $r14              # CHECK: stgt.w     $r31, $r28, $r14              # encoding: [0x9f,0x3b,0x7d,0x38]
+stgt.d         $r30, $r21, $r24              # CHECK: stgt.d     $r30, $r21, $r24              # encoding: [0xbe,0xe2,0x7d,0x38]
+stle.b         $r10, $r4, $r16               # CHECK: stle.b     $r10, $r4, $r16               # encoding: [0x8a,0x40,0x7e,0x38]
+stle.h         $r17, $r17, $r21              # CHECK: stle.h     $r17, $r17, $r21              # encoding: [0x31,0xd6,0x7e,0x38]
+stle.w         $r23, $r28, $r29              # CHECK: stle.w     $r23, $r28, $r29              # encoding: [0x97,0x77,0x7f,0x38]
+stle.d         $r25, $r24, $r29              # CHECK: stle.d     $r25, $r24, $r29              # encoding: [0x19,0xf7,0x7f,0x38]
diff --git a/llvm/test/MC/LoongArch/valid_priv.s b/llvm/test/MC/LoongArch/valid_priv.s
new file mode 100644
index 000000000000..0792216a977d
--- /dev/null
+++ b/llvm/test/MC/LoongArch/valid_priv.s
@@ -0,0 +1,32 @@
+# RUN: llvm-mc %s -triple=loongarch64-unknown-linux-gnu -show-encoding | FileCheck %s
+csrrd          $r26, 30                      # CHECK: csrrd           $r26, 30                      # encoding: [0x1a,0x78,0x00,0x04]
+csrwr          $r24, 194                     # CHECK: csrwr           $r24, 194                     # encoding: [0x38,0x08,0x03,0x04]
+csrxchg        $r6, $r27, 214                # CHECK: csrxchg         $r6, $r27, 214                # encoding: [0x66,0x5b,0x03,0x04]
+cacop          0, $r10, 27                   # CHECK: cacop           0, $r10, 27                   # encoding: [0x40,0x6d,0x00,0x06]
+lddir          $r12, $r30, 92                # CHECK: lddir           $r12, $r30, 92                # encoding: [0xcc,0x73,0x41,0x06]
+ldpte          $r18, 200                     # CHECK: ldpte           $r18, 200                     # encoding: [0x40,0x22,0x47,0x06]
+iocsrrd.b      $r26, $r24                    # CHECK: iocsrrd.b       $r26, $r24                    # encoding: [0x1a,0x03,0x48,0x06]
+iocsrrd.h      $r5, $r27                     # CHECK: iocsrrd.h       $r5, $r27                     # encoding: [0x65,0x07,0x48,0x06]
+iocsrrd.w      $r10, $r20                    # CHECK: iocsrrd.w       $r10, $r20                    # encoding: [0x8a,0x0a,0x48,0x06]
+iocsrrd.d      $r17, $r25                    # CHECK: iocsrrd.d       $r17, $r25                    # encoding: [0x31,0x0f,0x48,0x06]
+iocsrwr.b      $r4, $r23                     # CHECK: iocsrwr.b       $r4, $r23                     # encoding: [0xe4,0x12,0x48,0x06]
+iocsrwr.h      $r11, $zero                   # CHECK: iocsrwr.h       $r11, $zero                   # encoding: [0x0b,0x14,0x48,0x06]
+iocsrwr.w      $r20, $r26                    # CHECK: iocsrwr.w       $r20, $r26                    # encoding: [0x54,0x1b,0x48,0x06]
+iocsrwr.d      $r20, $r7                     # CHECK: iocsrwr.d       $r20, $r7                     # encoding: [0xf4,0x1c,0x48,0x06]
+tlbclr                                       # CHECK: tlbclr                                        # encoding: [0x00,0x20,0x48,0x06]
+tlbflush                                     # CHECK: tlbflush                                      # encoding: [0x00,0x24,0x48,0x06]
+tlbsrch                                      # CHECK: tlbsrch                                       # encoding: [0x00,0x28,0x48,0x06]
+tlbrd                                        # CHECK: tlbrd                                         # encoding: [0x00,0x2c,0x48,0x06]
+tlbwr                                        # CHECK: tlbwr                                         # encoding: [0x00,0x30,0x48,0x06]
+tlbfill                                      # CHECK: tlbfill                                       # encoding: [0x00,0x34,0x48,0x06]
+ertn                                         # CHECK: ertn                                          # encoding: [0x00,0x38,0x48,0x06]
+idle           204                           # CHECK: idle            204                           # encoding: [0xcc,0x80,0x48,0x06]
+invtlb         16, $r29, $r25                # CHECK: invtlb          16, $r29, $r25                # encoding: [0xb0,0xe7,0x49,0x06]
+rdtimel.w      $r30, $r19                    # CHECK: rdtimel.w       $r30, $r19                    # encoding: [0x7e,0x62,0x00,0x00]
+rdtimeh.w      $r19, $r14                    # CHECK: rdtimeh.w       $r19, $r14                    # encoding: [0xd3,0x65,0x00,0x00]
+rdtime.d       $tp, $r15                     # CHECK: rdtime.d        $tp, $r15                     # encoding: [0xe2,0x69,0x00,0x00]
+asrtle.d       $r12, $r17                    # CHECK: asrtle.d        $r12, $r17                    # encoding: [0x80,0x45,0x01,0x00]
+asrtgt.d       $r20, $r20                    # CHECK: asrtgt.d        $r20, $r20                    # encoding: [0x80,0xd2,0x01,0x00]
+break          199                           # CHECK: break           199                           # encoding: [0xc7,0x00,0x2a,0x00]
+dbcl           201                           # CHECK: dbcl            201                           # encoding: [0xc9,0x80,0x2a,0x00]
+syscall        100                           # CHECK: syscall         100                           # encoding: [0x64,0x00,0x2b,0x00]
diff --git a/llvm/tools/llvm-readobj/ELFDumper.cpp b/llvm/tools/llvm-readobj/ELFDumper.cpp
index 15076f1f8933..91f6d85b66e0 100644
--- a/llvm/tools/llvm-readobj/ELFDumper.cpp
+++ b/llvm/tools/llvm-readobj/ELFDumper.cpp
@@ -1520,6 +1520,7 @@ static const EnumEntry<unsigned> ElfMachineType[] = {
   ENUM_ENT(EM_LANAI,         "EM_LANAI"),
   ENUM_ENT(EM_BPF,           "EM_BPF"),
   ENUM_ENT(EM_VE,            "NEC SX-Aurora Vector Engine"),
+  ENUM_ENT(EM_LOONGARCH,     "LoongArch"),
 };
 
 static const EnumEntry<unsigned> ElfSymbolBindings[] = {
@@ -1858,6 +1859,13 @@ static const EnumEntry<unsigned> ElfHeaderRISCVFlags[] = {
   ENUM_ENT(EF_RISCV_RVE, "RVE")
 };
 
+static const EnumEntry<unsigned> ElfHeaderLoongArchFlags[] = {
+  ENUM_ENT(EF_LARCH_ABI_LP64D, "LP64D")
+  // FIXME: Change these and add more flags in future when all ABIs definition were finalized.
+  // See current definitions:
+  // https://loongson.github.io/LoongArch-Documentation/LoongArch-ELF-ABI-EN.html#_e_flags_identifies_abi_type_and_version
+};
+
 static const EnumEntry<unsigned> ElfSymOtherFlags[] = {
   LLVM_READOBJ_ENUM_ENT(ELF, STV_INTERNAL),
   LLVM_READOBJ_ENUM_ENT(ELF, STV_HIDDEN),
@@ -3486,6 +3494,8 @@ template <class ELFT> void GNUStyle<ELFT>::printFileHeaders(const ELFO *Obj) {
                    unsigned(ELF::EF_MIPS_MACH));
   else if (e->e_machine == EM_RISCV)
     ElfFlags = printFlags(e->e_flags, makeArrayRef(ElfHeaderRISCVFlags));
+  else if (e->e_machine == EM_LOONGARCH)
+    ElfFlags = printFlags(e->e_flags, makeArrayRef(ElfHeaderLoongArchFlags));
   Str = "0x" + to_hexString(e->e_flags);
   if (!ElfFlags.empty())
     Str = Str + ", " + ElfFlags;
@@ -6137,6 +6147,8 @@ template <class ELFT> void LLVMStyle<ELFT>::printFileHeaders(const ELFO *Obj) {
                    unsigned(ELF::EF_AMDGPU_MACH));
     else if (E->e_machine == EM_RISCV)
       W.printFlags("Flags", E->e_flags, makeArrayRef(ElfHeaderRISCVFlags));
+    else if (E->e_machine == EM_LOONGARCH)
+      W.printFlags("Flags", E->e_flags, makeArrayRef(ElfHeaderLoongArchFlags));
     else
       W.printFlags("Flags", E->e_flags);
     W.printNumber("HeaderSize", E->e_ehsize);
diff --git a/llvm/utils/TableGen/DFAEmitter.cpp b/llvm/utils/TableGen/DFAEmitter.cpp
index 7391f6845a4b..e87765085289 100644
--- a/llvm/utils/TableGen/DFAEmitter.cpp
+++ b/llvm/utils/TableGen/DFAEmitter.cpp
@@ -174,7 +174,7 @@ namespace {
 struct Action {
   Record *R = nullptr;
   unsigned I = 0;
-  std::string S = nullptr;
+  std::string S;
 
   Action() = default;
   Action(Record *R, unsigned I, std::string S) : R(R), I(I), S(S) {}
diff --git a/llvm/utils/UpdateTestChecks/asm.py b/llvm/utils/UpdateTestChecks/asm.py
index 998d8b1f5773..54a03c3be1cc 100644
--- a/llvm/utils/UpdateTestChecks/asm.py
+++ b/llvm/utils/UpdateTestChecks/asm.py
@@ -63,6 +63,11 @@ ASM_FUNCTION_MSP430_RE = re.compile(
     r'^_?(?P<func>[^:]+):[ \t]*;+[ \t]*@(?P=func)\n[^:]*?'
     r'(?P<body>.*?)\n'
     r'(\$|\.L)func_end[0-9]+:\n',             # $func_end0:
+
+ASM_FUNCTION_LOONGARCH_RE = re.compile(
+    r'^_?(?P<func>[^:]+):[ \t]*#+[ \t]*@(?P=func)\n[^:]*?' # f: (name of func)
+    r'(?P<body>^##?[ \t]+[^:]+:.*?)\s*'                    # (body of the function)
+    r'.Lfunc_end[0-9]+:\n',                                # .Lfunc_end[0-9]:
     flags=(re.M | re.S))
 
 ASM_FUNCTION_PPC_RE = re.compile(
@@ -257,6 +262,16 @@ def scrub_asm_msp430(asm, args):
   asm = common.SCRUB_TRAILING_WHITESPACE_RE.sub(r'', asm)
   return asm
 
+def scrub_asm_loongarch(asm, args):
+  # Scrub runs of whitespace out of the assembly, but leave the leading
+  # whitespace in place.
+  asm = common.SCRUB_WHITESPACE_RE.sub(r' ', asm)
+  # Expand the tabs used for indentation.
+  asm = string.expandtabs(asm, 2)
+  # Strip trailing whitespace.
+  asm = common.SCRUB_TRAILING_WHITESPACE_RE.sub(r'', asm)
+  return asm
+
 def scrub_asm_riscv(asm, args):
   # Scrub runs of whitespace out of the assembly, but leave the leading
   # whitespace in place.
@@ -344,6 +359,7 @@ def build_function_body_dictionary_for_triple(args, raw_tool_output, triple, pre
       'msp430': (scrub_asm_msp430, ASM_FUNCTION_MSP430_RE),
       'ppc32': (scrub_asm_powerpc, ASM_FUNCTION_PPC_RE),
       'powerpc': (scrub_asm_powerpc, ASM_FUNCTION_PPC_RE),
+      'loongarch64': (scrub_asm_loongarch, ASM_FUNCTION_LOONGARCH_RE),
       'riscv32': (scrub_asm_riscv, ASM_FUNCTION_RISCV_RE),
       'riscv64': (scrub_asm_riscv, ASM_FUNCTION_RISCV_RE),
       'lanai': (scrub_asm_lanai, ASM_FUNCTION_LANAI_RE),
diff --git a/llvm/utils/benchmark/src/benchmark_register.h b/llvm/utils/benchmark/src/benchmark_register.h
index 0705e219f2fa..6001fb8e0e48 100644
--- a/llvm/utils/benchmark/src/benchmark_register.h
+++ b/llvm/utils/benchmark/src/benchmark_register.h
@@ -2,6 +2,7 @@
 #define BENCHMARK_REGISTER_H
 
 #include <vector>
+#include <limits>
 
 #include "check.h"
 
diff --git a/llvm/utils/benchmark/src/cycleclock.h b/llvm/utils/benchmark/src/cycleclock.h
index 1b0f09359c9b..48602e8f420c 100644
--- a/llvm/utils/benchmark/src/cycleclock.h
+++ b/llvm/utils/benchmark/src/cycleclock.h
@@ -167,6 +167,12 @@ inline BENCHMARK_ALWAYS_INLINE int64_t Now() {
   struct timeval tv;
   gettimeofday(&tv, nullptr);
   return static_cast<int64_t>(tv.tv_sec) * 1000000 + tv.tv_usec;
+#elif defined(__loongarch__)
+  // loongarch apparently only allows rdtsc for superusers, so we fall
+  // back to gettimeofday.  It's possible clock_gettime would be better.
+  struct timeval tv;
+  gettimeofday(&tv, nullptr);
+  return static_cast<int64_t>(tv.tv_sec) * 1000000 + tv.tv_usec;
 #elif defined(__s390__) // Covers both s390 and s390x.
   // Return the CPU clock.
   uint64_t tsc;
diff --git a/llvm/utils/gn/secondary/clang/lib/Basic/BUILD.gn b/llvm/utils/gn/secondary/clang/lib/Basic/BUILD.gn
index 3c573395993d..94d81c6b9b9e 100644
--- a/llvm/utils/gn/secondary/clang/lib/Basic/BUILD.gn
+++ b/llvm/utils/gn/secondary/clang/lib/Basic/BUILD.gn
@@ -80,6 +80,7 @@ static_library("Basic") {
     "Targets/Hexagon.cpp",
     "Targets/Lanai.cpp",
     "Targets/Le64.cpp",
+    "Targets/LoongArch.cpp",
     "Targets/MSP430.cpp",
     "Targets/Mips.cpp",
     "Targets/NVPTX.cpp",
diff --git a/llvm/utils/gn/secondary/clang/lib/Driver/BUILD.gn b/llvm/utils/gn/secondary/clang/lib/Driver/BUILD.gn
index 04d483a12eed..a9383a4fc471 100644
--- a/llvm/utils/gn/secondary/clang/lib/Driver/BUILD.gn
+++ b/llvm/utils/gn/secondary/clang/lib/Driver/BUILD.gn
@@ -47,6 +47,7 @@ static_library("Driver") {
     "ToolChains/Ananas.cpp",
     "ToolChains/Arch/AArch64.cpp",
     "ToolChains/Arch/ARM.cpp",
+    "ToolChains/Arch/LoongArch.cpp",
     "ToolChains/Arch/Mips.cpp",
     "ToolChains/Arch/PPC.cpp",
     "ToolChains/Arch/RISCV.cpp",
diff --git a/llvm/utils/gn/secondary/llvm/include/llvm/IR/BUILD.gn b/llvm/utils/gn/secondary/llvm/include/llvm/IR/BUILD.gn
index 9dad1b97f9f1..b68cf5c7ffa9 100644
--- a/llvm/utils/gn/secondary/llvm/include/llvm/IR/BUILD.gn
+++ b/llvm/utils/gn/secondary/llvm/include/llvm/IR/BUILD.gn
@@ -67,6 +67,16 @@ tablegen("IntrinsicsHexagon") {
   td_file = "Intrinsics.td"
 }
 
+tablegen("IntrinsicsLoongArch") {
+  visibility = [ ":public_tablegen" ]
+  output_name = "IntrinsicsLoongArch.h"
+  args = [
+    "-gen-intrinsic-enums",
+    "-intrinsic-prefix=loongarch",
+  ]
+  td_file = "Intrinsics.td"
+}
+
 tablegen("IntrinsicsMips") {
   visibility = [ ":public_tablegen" ]
   output_name = "IntrinsicsMips.h"
@@ -176,6 +186,7 @@ group("public_tablegen") {
     ":IntrinsicsARM",
     ":IntrinsicsBPF",
     ":IntrinsicsHexagon",
+    ":IntrinsicsLoongArch",
     ":IntrinsicsMips",
     ":IntrinsicsNVPTX",
     ":IntrinsicsPowerPC",
diff --git a/llvm/utils/gn/secondary/llvm/lib/Target/LoongArch/AsmParser/BUILD.gn b/llvm/utils/gn/secondary/llvm/lib/Target/LoongArch/AsmParser/BUILD.gn
new file mode 100644
index 000000000000..cc3bb49a6ac7
--- /dev/null
+++ b/llvm/utils/gn/secondary/llvm/lib/Target/LoongArch/AsmParser/BUILD.gn
@@ -0,0 +1,24 @@
+import("//llvm/utils/TableGen/tablegen.gni")
+
+tablegen("LoongArchGenAsmMatcher") {
+  visibility = [ ":AsmParser" ]
+  args = [ "-gen-asm-matcher" ]
+  td_file = "../LoongArch.td"
+}
+
+static_library("AsmParser") {
+  output_name = "LLVMLoongArchAsmParser"
+  deps = [
+    ":LoongArchGenAsmMatcher",
+    "//llvm/lib/MC",
+    "//llvm/lib/MC/MCParser",
+    "//llvm/lib/Support",
+    "//llvm/lib/Target/LoongArch/MCTargetDesc",
+    "//llvm/lib/Target/LoongArch/TargetInfo",
+  ]
+  include_dirs = [ ".." ]
+  sources = [
+    # Make `gn format` not collapse this, for sync_source_lists_from_cmake.py.
+    "LoongArchAsmParser.cpp",
+  ]
+}
diff --git a/llvm/utils/gn/secondary/llvm/lib/Target/LoongArch/BUILD.gn b/llvm/utils/gn/secondary/llvm/lib/Target/LoongArch/BUILD.gn
new file mode 100644
index 000000000000..e89db5200e89
--- /dev/null
+++ b/llvm/utils/gn/secondary/llvm/lib/Target/LoongArch/BUILD.gn
@@ -0,0 +1,102 @@
+import("//llvm/utils/TableGen/tablegen.gni")
+
+tablegen("LoongArchGenCallingConv") {
+  visibility = [ ":LLVMLoongArchCodeGen" ]
+  args = [ "-gen-callingconv" ]
+  td_file = "LoongArch.td"
+}
+
+tablegen("LoongArchGenDAGISel") {
+  visibility = [ ":LLVMLoongArchCodeGen" ]
+  args = [ "-gen-dag-isel" ]
+  td_file = "LoongArch.td"
+}
+
+tablegen("LoongArchGenFastISel") {
+  visibility = [ ":LLVMLoongArchCodeGen" ]
+  args = [ "-gen-fast-isel" ]
+  td_file = "LoongArch.td"
+}
+
+tablegen("LoongArchGenGlobalISel") {
+  visibility = [ ":LLVMLoongArchCodeGen" ]
+  args = [ "-gen-global-isel" ]
+  td_file = "LoongArch.td"
+}
+
+tablegen("LoongArchGenMCPseudoLowering") {
+  visibility = [ ":LLVMLoongArchCodeGen" ]
+  args = [ "-gen-pseudo-lowering" ]
+  td_file = "LoongArch.td"
+}
+
+tablegen("LoongArchGenRegisterBank") {
+  visibility = [ ":LLVMLoongArchCodeGen" ]
+  args = [ "-gen-register-bank" ]
+  td_file = "LoongArch.td"
+}
+
+static_library("LLVMLoongArchCodeGen") {
+  deps = [
+    ":LoongArchGenCallingConv",
+    ":LoongArchGenDAGISel",
+    ":LoongArchGenFastISel",
+    ":LoongArchGenGlobalISel",
+    ":LoongArchGenMCPseudoLowering",
+    ":LoongArchGenRegisterBank",
+    "MCTargetDesc",
+    "TargetInfo",
+    "//llvm/include/llvm/Config:llvm-config",
+    "//llvm/lib/Analysis",
+    "//llvm/lib/CodeGen",
+    "//llvm/lib/CodeGen/AsmPrinter",
+    "//llvm/lib/CodeGen/GlobalISel",
+    "//llvm/lib/CodeGen/SelectionDAG",
+    "//llvm/lib/IR",
+    "//llvm/lib/MC",
+    "//llvm/lib/Support",
+    "//llvm/lib/Target",
+  ]
+  include_dirs = [ "." ]
+  sources = [
+    "LoongArchAnalyzeImmediate.cpp",
+    "LoongArchAsmPrinter.cpp",
+    "LoongArchCCState.cpp",
+    "LoongArchCallLowering.cpp",
+    "LoongArchConstantIslandPass.cpp",
+    "LoongArchDelaySlotFiller.cpp",
+    "LoongArchExpandPseudo.cpp",
+    "LoongArchFrameLowering.cpp",
+    "LoongArchISelDAGToDAG.cpp",
+    "LoongArchISelLowering.cpp",
+    "LoongArchInstrInfo.cpp",
+    "LoongArchInstructionSelector.cpp",
+    "LoongArchLegalizerInfo.cpp",
+    "LoongArchMCInstLower.cpp",
+    "LoongArchMachineFunction.cpp",
+    "LoongArchModuleISelDAGToDAG.cpp",
+    "LoongArchOptimizePICCall.cpp",
+    "LoongArchPreLegalizerCombiner.cpp",
+    "LoongArchRegisterBankInfo.cpp",
+    "LoongArchRegisterInfo.cpp",
+    "LoongArchSubtarget.cpp",
+    "LoongArchTargetMachine.cpp",
+    "LoongArchTargetObjectFile.cpp",
+  ]
+}
+
+# This is a bit different from most build files: Due to this group
+# having the directory's name, "//llvm/lib/Target/LoongArch" will refer to this
+# target, which pulls in the code in this directory *and all subdirectories*.
+# For most other directories, "//llvm/lib/Foo" only pulls in the code directly
+# in "llvm/lib/Foo". The forwarding targets in //llvm/lib/Target expect this
+# different behavior.
+group("LoongArch") {
+  deps = [
+    ":LLVMLoongArchCodeGen",
+    "AsmParser",
+    "Disassembler",
+    "MCTargetDesc",
+    "TargetInfo",
+  ]
+}
diff --git a/llvm/utils/gn/secondary/llvm/lib/Target/LoongArch/Disassembler/BUILD.gn b/llvm/utils/gn/secondary/llvm/lib/Target/LoongArch/Disassembler/BUILD.gn
new file mode 100644
index 000000000000..0a9b4cf59441
--- /dev/null
+++ b/llvm/utils/gn/secondary/llvm/lib/Target/LoongArch/Disassembler/BUILD.gn
@@ -0,0 +1,23 @@
+import("//llvm/utils/TableGen/tablegen.gni")
+
+tablegen("LoongArchGenDisassemblerTables") {
+  visibility = [ ":Disassembler" ]
+  args = [ "-gen-disassembler" ]
+  td_file = "../LoongArch.td"
+}
+
+static_library("Disassembler") {
+  output_name = "LLVMLoongArchDisassembler"
+  deps = [
+    ":LoongArchGenDisassemblerTables",
+    "//llvm/lib/MC/MCDisassembler",
+    "//llvm/lib/Support",
+    "//llvm/lib/Target/LoongArch/MCTargetDesc",
+    "//llvm/lib/Target/LoongArch/TargetInfo",
+  ]
+  include_dirs = [ ".." ]
+  sources = [
+    # Make `gn format` not collapse this, for sync_source_lists_from_cmake.py.
+    "LoongArchDisassembler.cpp",
+  ]
+}
diff --git a/llvm/utils/gn/secondary/llvm/lib/Target/LoongArch/MCTargetDesc/BUILD.gn b/llvm/utils/gn/secondary/llvm/lib/Target/LoongArch/MCTargetDesc/BUILD.gn
new file mode 100644
index 000000000000..f0b96c965fe4
--- /dev/null
+++ b/llvm/utils/gn/secondary/llvm/lib/Target/LoongArch/MCTargetDesc/BUILD.gn
@@ -0,0 +1,74 @@
+import("//llvm/utils/TableGen/tablegen.gni")
+
+tablegen("LoongArchGenAsmWriter") {
+  visibility = [ ":MCTargetDesc" ]
+  args = [ "-gen-asm-writer" ]
+  td_file = "../LoongArch.td"
+}
+
+tablegen("LoongArchGenInstrInfo") {
+  visibility = [ ":tablegen" ]
+  args = [ "-gen-instr-info" ]
+  td_file = "../LoongArch.td"
+}
+
+tablegen("LoongArchGenMCCodeEmitter") {
+  visibility = [ ":MCTargetDesc" ]
+  args = [ "-gen-emitter" ]
+  td_file = "../LoongArch.td"
+}
+
+tablegen("LoongArchGenRegisterInfo") {
+  visibility = [ ":tablegen" ]
+  args = [ "-gen-register-info" ]
+  td_file = "../LoongArch.td"
+}
+
+tablegen("LoongArchGenSubtargetInfo") {
+  visibility = [ ":tablegen" ]
+  args = [ "-gen-subtarget" ]
+  td_file = "../LoongArch.td"
+}
+
+# This should contain tablegen targets generating .inc files included
+# by other targets. .inc files only used by .cpp files in this directory
+# should be in deps on the static_library instead.
+group("tablegen") {
+  visibility = [
+    ":MCTargetDesc",
+    "../TargetInfo",
+  ]
+  public_deps = [
+    ":LoongArchGenInstrInfo",
+    ":LoongArchGenRegisterInfo",
+    ":LoongArchGenSubtargetInfo",
+  ]
+}
+
+static_library("MCTargetDesc") {
+  output_name = "LLVMLoongArchDesc"
+  public_deps = [ ":tablegen" ]
+  deps = [
+    ":LoongArchGenAsmWriter",
+    ":LoongArchGenMCCodeEmitter",
+    "//llvm/lib/MC",
+    "//llvm/lib/Support",
+    "//llvm/lib/Target/LoongArch/TargetInfo",
+  ]
+  include_dirs = [ ".." ]
+  sources = [
+    "LoongArchABIFlagsSection.cpp",
+    "LoongArchABIInfo.cpp",
+    "LoongArchAsmBackend.cpp",
+    "LoongArchELFObjectWriter.cpp",
+    "LoongArchELFStreamer.cpp",
+    "LoongArchInstPrinter.cpp",
+    "LoongArchMCAsmInfo.cpp",
+    "LoongArchMCCodeEmitter.cpp",
+    "LoongArchMCExpr.cpp",
+    "LoongArchMCTargetDesc.cpp",
+    "LoongArchNaClELFStreamer.cpp",
+    "LoongArchOptionRecord.cpp",
+    "LoongArchTargetStreamer.cpp",
+  ]
+}
diff --git a/llvm/utils/gn/secondary/llvm/lib/Target/LoongArch/TargetInfo/BUILD.gn b/llvm/utils/gn/secondary/llvm/lib/Target/LoongArch/TargetInfo/BUILD.gn
new file mode 100644
index 000000000000..a476bdd5fd16
--- /dev/null
+++ b/llvm/utils/gn/secondary/llvm/lib/Target/LoongArch/TargetInfo/BUILD.gn
@@ -0,0 +1,9 @@
+static_library("TargetInfo") {
+  output_name = "LLVMLoongArchInfo"
+  deps = [ "//llvm/lib/Support" ]
+  include_dirs = [ ".." ]
+  sources = [
+    # Make `gn format` not collapse this, for sync_source_lists_from_cmake.py.
+    "LoongArchTargetInfo.cpp",
+  ]
+}
diff --git a/llvm/utils/gn/secondary/llvm/lib/Target/targets.gni b/llvm/utils/gn/secondary/llvm/lib/Target/targets.gni
index 102040c2fa82..ccf336c00e8d 100644
--- a/llvm/utils/gn/secondary/llvm/lib/Target/targets.gni
+++ b/llvm/utils/gn/secondary/llvm/lib/Target/targets.gni
@@ -14,6 +14,7 @@ llvm_all_targets = [
   "BPF",
   "Hexagon",
   "Lanai",
+  "LoongArch",
   "Mips",
   "NVPTX",
   "PowerPC",
@@ -47,6 +48,7 @@ llvm_build_AArch64 = false
 llvm_build_AMDGPU = false
 llvm_build_ARM = false
 llvm_build_BPF = false
+llvm_build_LoongArch = false
 llvm_build_Mips = false
 llvm_build_PowerPC = false
 llvm_build_WebAssembly = false
@@ -60,6 +62,8 @@ foreach(target, llvm_targets_to_build) {
     llvm_build_ARM = true
   } else if (target == "BPF") {
     llvm_build_BPF = true
+  } else if (target == "LoongArch") {
+    llvm_build_BPF = true
   } else if (target == "Mips") {
     llvm_build_Mips = true
   } else if (target == "PowerPC") {
diff --git a/llvm/utils/gn/secondary/llvm/tools/llvm-config/BUILD.gn b/llvm/utils/gn/secondary/llvm/tools/llvm-config/BUILD.gn
index 70c06909c8eb..ac50ccaf2242 100644
--- a/llvm/utils/gn/secondary/llvm/tools/llvm-config/BUILD.gn
+++ b/llvm/utils/gn/secondary/llvm/tools/llvm-config/BUILD.gn
@@ -228,6 +228,11 @@ action("LibraryDependencies.inc") {
     "//llvm/lib/Target/Lanai/LLVMBuild.txt",
     "//llvm/lib/Target/Lanai/MCTargetDesc/LLVMBuild.txt",
     "//llvm/lib/Target/Lanai/TargetInfo/LLVMBuild.txt",
+    "//llvm/lib/Target/LoongArch/AsmParser/LLVMBuild.txt",
+    "//llvm/lib/Target/LoongArch/Disassembler/LLVMBuild.txt",
+    "//llvm/lib/Target/LoongArch/LLVMBuild.txt",
+    "//llvm/lib/Target/LoongArch/MCTargetDesc/LLVMBuild.txt",
+    "//llvm/lib/Target/LoongArch/TargetInfo/LLVMBuild.txt",
     "//llvm/lib/Target/MSP430/AsmParser/LLVMBuild.txt",
     "//llvm/lib/Target/MSP430/Disassembler/LLVMBuild.txt",
     "//llvm/lib/Target/MSP430/LLVMBuild.txt",
